
#include <Python.h>
#include <exception>

#include "THP.h"
#include "torch/csrc/utils/auto_gpu.h"
#include "torch/csrc/nn/type_checks.h"

#include <TH/TH.h>
#include <THC/THC.h>


TH_API void THNN_CudaHalfAbs_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfAbs_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfAbs_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfAbs_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaAbs_updateOutput(void*, THCudaTensor*, THCudaTensor*);

PyObject * CudaAbs_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaAbs_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaAbs_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleAbs_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleAbs_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleAbs_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleAbs_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfAbs_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfAbs_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfAbs_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfAbs_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaAbs_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaAbs_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaAbs_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaAbs_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleAbs_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleAbs_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleAbs_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleAbs_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfAbsCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfAbsCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfAbsCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfAbsCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaAbsCriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaAbsCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaAbsCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaAbsCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleAbsCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleAbsCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleAbsCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleAbsCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfAbsCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfAbsCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfAbsCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfAbsCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaAbsCriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaAbsCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaAbsCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaAbsCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleAbsCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleAbsCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleAbsCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleAbsCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfBatchNormalization_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, double, double);

PyObject * CudaHalfBatchNormalization_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_bias_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaHalfTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaHalfTensor* arg_saveMean_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaHalfTensor* arg_saveStd_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      bool arg_train = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      double arg_momentum = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfBatchNormalization_updateOutput(arg_state, arg_input_, arg_output_, arg_weight_, arg_bias_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_momentum, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfBatchNormalization_updateOutput", 1, "(int state, torch.cuda.HalfTensor input_, torch.cuda.HalfTensor output_, [torch.cuda.HalfTensor weight_ or None], [torch.cuda.HalfTensor bias_ or None], [torch.cuda.HalfTensor runningMean_ or None], [torch.cuda.HalfTensor runningVar_ or None], torch.cuda.HalfTensor saveMean_, torch.cuda.HalfTensor saveStd_, bool train, float momentum, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaBatchNormalization_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, double, double);

PyObject * CudaBatchNormalization_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_bias_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaTensor* arg_saveMean_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaTensor* arg_saveStd_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      bool arg_train = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      double arg_momentum = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaBatchNormalization_updateOutput(arg_state, arg_input_, arg_output_, arg_weight_, arg_bias_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_momentum, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaBatchNormalization_updateOutput", 1, "(int state, torch.cuda.FloatTensor input_, torch.cuda.FloatTensor output_, [torch.cuda.FloatTensor weight_ or None], [torch.cuda.FloatTensor bias_ or None], [torch.cuda.FloatTensor runningMean_ or None], [torch.cuda.FloatTensor runningVar_ or None], torch.cuda.FloatTensor saveMean_, torch.cuda.FloatTensor saveStd_, bool train, float momentum, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleBatchNormalization_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, double, double);

PyObject * CudaDoubleBatchNormalization_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_bias_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaDoubleTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaDoubleTensor* arg_saveMean_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaDoubleTensor* arg_saveStd_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      bool arg_train = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      double arg_momentum = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleBatchNormalization_updateOutput(arg_state, arg_input_, arg_output_, arg_weight_, arg_bias_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_momentum, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleBatchNormalization_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input_, torch.cuda.DoubleTensor output_, [torch.cuda.DoubleTensor weight_ or None], [torch.cuda.DoubleTensor bias_ or None], [torch.cuda.DoubleTensor runningMean_ or None], [torch.cuda.DoubleTensor runningVar_ or None], torch.cuda.DoubleTensor saveMean_, torch.cuda.DoubleTensor saveStd_, bool train, float momentum, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfBatchNormalization_backward(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, double, double);

PyObject * CudaHalfBatchNormalization_backward(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8)) || PyTuple_GET_ITEM(args, 8) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 9)) || PyTuple_GET_ITEM(args, 9) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput_ = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_gradWeight_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_gradBias_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaHalfTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaHalfTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      THCudaHalfTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 8) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8)));
      THCudaHalfTensor* arg_saveMean_ = (PyTuple_GET_ITEM(args, 9) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 9)));
      THCudaHalfTensor* arg_saveStd_ = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      bool arg_train = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfBatchNormalization_backward(arg_state, arg_input_, arg_gradOutput_, arg_gradInput_, arg_gradWeight_, arg_gradBias_, arg_weight_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_scale, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfBatchNormalization_backward", 1, "(int state, torch.cuda.HalfTensor input_, torch.cuda.HalfTensor gradOutput_, [torch.cuda.HalfTensor gradInput_ or None], [torch.cuda.HalfTensor gradWeight_ or None], [torch.cuda.HalfTensor gradBias_ or None], [torch.cuda.HalfTensor weight_ or None], [torch.cuda.HalfTensor runningMean_ or None], [torch.cuda.HalfTensor runningVar_ or None], [torch.cuda.HalfTensor saveMean_ or None], [torch.cuda.HalfTensor saveStd_ or None], bool train, float scale, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaBatchNormalization_backward(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, double, double);

PyObject * CudaBatchNormalization_backward(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8)) || PyTuple_GET_ITEM(args, 8) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 9)) || PyTuple_GET_ITEM(args, 9) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput_ = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_gradWeight_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_gradBias_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      THCudaTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 8) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8)));
      THCudaTensor* arg_saveMean_ = (PyTuple_GET_ITEM(args, 9) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 9)));
      THCudaTensor* arg_saveStd_ = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      bool arg_train = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaBatchNormalization_backward(arg_state, arg_input_, arg_gradOutput_, arg_gradInput_, arg_gradWeight_, arg_gradBias_, arg_weight_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_scale, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaBatchNormalization_backward", 1, "(int state, torch.cuda.FloatTensor input_, torch.cuda.FloatTensor gradOutput_, [torch.cuda.FloatTensor gradInput_ or None], [torch.cuda.FloatTensor gradWeight_ or None], [torch.cuda.FloatTensor gradBias_ or None], [torch.cuda.FloatTensor weight_ or None], [torch.cuda.FloatTensor runningMean_ or None], [torch.cuda.FloatTensor runningVar_ or None], [torch.cuda.FloatTensor saveMean_ or None], [torch.cuda.FloatTensor saveStd_ or None], bool train, float scale, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleBatchNormalization_backward(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, double, double);

PyObject * CudaDoubleBatchNormalization_backward(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) || PyTuple_GET_ITEM(args, 8) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 9)) || PyTuple_GET_ITEM(args, 9) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput_ = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput_ = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_gradWeight_ = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_gradBias_ = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaDoubleTensor* arg_weight_ = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaDoubleTensor* arg_runningMean_ = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      THCudaDoubleTensor* arg_runningVar_ = (PyTuple_GET_ITEM(args, 8) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8)));
      THCudaDoubleTensor* arg_saveMean_ = (PyTuple_GET_ITEM(args, 9) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 9)));
      THCudaDoubleTensor* arg_saveStd_ = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      bool arg_train = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleBatchNormalization_backward(arg_state, arg_input_, arg_gradOutput_, arg_gradInput_, arg_gradWeight_, arg_gradBias_, arg_weight_, arg_runningMean_, arg_runningVar_, arg_saveMean_, arg_saveStd_, arg_train, arg_scale, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleBatchNormalization_backward", 1, "(int state, torch.cuda.DoubleTensor input_, torch.cuda.DoubleTensor gradOutput_, [torch.cuda.DoubleTensor gradInput_ or None], [torch.cuda.DoubleTensor gradWeight_ or None], [torch.cuda.DoubleTensor gradBias_ or None], [torch.cuda.DoubleTensor weight_ or None], [torch.cuda.DoubleTensor runningMean_ or None], [torch.cuda.DoubleTensor runningVar_ or None], [torch.cuda.DoubleTensor saveMean_ or None], [torch.cuda.DoubleTensor saveStd_ or None], bool train, float scale, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfBCECriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, bool);

PyObject * CudaHalfBCECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfBCECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfBCECriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, [torch.cuda.HalfTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaBCECriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, THCudaTensor*, bool);

PyObject * CudaBCECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaBCECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaBCECriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleBCECriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, bool);

PyObject * CudaDoubleBCECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleBCECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleBCECriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfBCECriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, bool);

PyObject * CudaHalfBCECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfBCECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfBCECriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, [torch.cuda.HalfTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaBCECriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, THCudaTensor*, bool);

PyObject * CudaBCECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaBCECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaBCECriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, [torch.cuda.FloatTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleBCECriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, bool);

PyObject * CudaDoubleBCECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleBCECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleBCECriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfClassNLLCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, THCudaHalfTensor*, int64_t, bool);

PyObject * CudaHalfClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaHalfTensor* arg_total_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor output, bool sizeAverage, [torch.cuda.HalfTensor weights or None], torch.cuda.HalfTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaClassNLLCriterion_updateOutput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, bool, THCudaTensor*, THCudaTensor*, int64_t, bool);

PyObject * CudaClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaTensor* arg_total_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleClassNLLCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, THCudaDoubleTensor*, int64_t, bool);

PyObject * CudaDoubleClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaDoubleTensor* arg_total_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], torch.cuda.DoubleTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfClassNLLCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, THCudaHalfTensor*, int64_t, bool);

PyObject * CudaHalfClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaHalfTensor* arg_total_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, [torch.cuda.HalfTensor weights or None], torch.cuda.HalfTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaClassNLLCriterion_updateGradInput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, bool, THCudaTensor*, THCudaTensor*, int64_t, bool);

PyObject * CudaClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaTensor* arg_total_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleClassNLLCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, THCudaDoubleTensor*, int64_t, bool);

PyObject * CudaDoubleClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaDoubleTensor* arg_total_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], torch.cuda.DoubleTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfDistKLDivCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfDistKLDivCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfDistKLDivCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfDistKLDivCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDistKLDivCriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaDistKLDivCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDistKLDivCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDistKLDivCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleDistKLDivCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleDistKLDivCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleDistKLDivCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleDistKLDivCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfDistKLDivCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfDistKLDivCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfDistKLDivCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfDistKLDivCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDistKLDivCriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaDistKLDivCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDistKLDivCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDistKLDivCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleDistKLDivCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleDistKLDivCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleDistKLDivCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleDistKLDivCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfELU_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, float, bool);

PyObject * CudaHalfELU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfELU_updateOutput(arg_state, arg_input, arg_output, arg_alpha, arg_scale, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfELU_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float alpha, float scale, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaELU_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, float, bool);

PyObject * CudaELU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaELU_updateOutput(arg_state, arg_input, arg_output, arg_alpha, arg_scale, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaELU_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float alpha, float scale, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleELU_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool);

PyObject * CudaDoubleELU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleELU_updateOutput(arg_state, arg_input, arg_output, arg_alpha, arg_scale, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleELU_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float alpha, float scale, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfELU_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfELU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfELU_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output, arg_alpha, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfELU_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor output, float alpha, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaELU_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaELU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaELU_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output, arg_alpha, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaELU_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor output, float alpha, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleELU_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleELU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleELU_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output, arg_alpha, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleELU_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor output, float alpha, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfFeatureLPPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, int, int, bool);

PyObject * CudaHalfFeatureLPPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_inputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_outputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_power = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfFeatureLPPooling_updateOutput(arg_state, arg_inputTH, arg_outputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfFeatureLPPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor inputTH, torch.cuda.HalfTensor outputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaFeatureLPPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, int, int, bool);

PyObject * CudaFeatureLPPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_inputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_outputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_power = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaFeatureLPPooling_updateOutput(arg_state, arg_inputTH, arg_outputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaFeatureLPPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor inputTH, torch.cuda.FloatTensor outputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleFeatureLPPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, int, int, bool);

PyObject * CudaDoubleFeatureLPPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_inputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_outputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleFeatureLPPooling_updateOutput(arg_state, arg_inputTH, arg_outputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleFeatureLPPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor inputTH, torch.cuda.DoubleTensor outputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfFeatureLPPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, int, int, bool);

PyObject * CudaHalfFeatureLPPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_inputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_outputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInputTH = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      float arg_power = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfFeatureLPPooling_updateGradInput(arg_state, arg_gradOutputTH, arg_inputTH, arg_outputTH, arg_gradInputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfFeatureLPPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutputTH, torch.cuda.HalfTensor inputTH, torch.cuda.HalfTensor outputTH, torch.cuda.HalfTensor gradInputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaFeatureLPPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, int, int, bool);

PyObject * CudaFeatureLPPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_inputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_outputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInputTH = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      float arg_power = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaFeatureLPPooling_updateGradInput(arg_state, arg_gradOutputTH, arg_inputTH, arg_outputTH, arg_gradInputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaFeatureLPPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutputTH, torch.cuda.FloatTensor inputTH, torch.cuda.FloatTensor outputTH, torch.cuda.FloatTensor gradInputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleFeatureLPPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, int, int, bool);

PyObject * CudaDoubleFeatureLPPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_inputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_outputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInputTH = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleFeatureLPPooling_updateGradInput(arg_state, arg_gradOutputTH, arg_inputTH, arg_outputTH, arg_gradInputTH, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleFeatureLPPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutputTH, torch.cuda.DoubleTensor inputTH, torch.cuda.DoubleTensor outputTH, torch.cuda.DoubleTensor gradInputTH, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfHardTanh_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, float, bool);

PyObject * CudaHalfHardTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_min_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_max_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfHardTanh_updateOutput(arg_state, arg_input, arg_output, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfHardTanh_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHardTanh_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, float, bool);

PyObject * CudaHardTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_min_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_max_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHardTanh_updateOutput(arg_state, arg_input, arg_output, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHardTanh_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleHardTanh_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool);

PyObject * CudaDoubleHardTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleHardTanh_updateOutput(arg_state, arg_input, arg_output, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleHardTanh_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfHardTanh_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float, bool);

PyObject * CudaHalfHardTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_min_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_max_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfHardTanh_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfHardTanh_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHardTanh_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float, bool);

PyObject * CudaHardTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_min_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_max_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHardTanh_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHardTanh_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleHardTanh_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool);

PyObject * CudaDoubleHardTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleHardTanh_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleHardTanh_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfGatedLinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfGatedLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfGatedLinear_updateOutput(arg_state, arg_input, arg_output, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfGatedLinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaGatedLinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaGatedLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaGatedLinear_updateOutput(arg_state, arg_input, arg_output, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaGatedLinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleGatedLinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleGatedLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleGatedLinear_updateOutput(arg_state, arg_input, arg_output, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleGatedLinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfGatedLinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfGatedLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfGatedLinear_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfGatedLinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaGatedLinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaGatedLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaGatedLinear_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaGatedLinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleGatedLinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleGatedLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleGatedLinear_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleGatedLinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIm2Col_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfIm2Col_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIm2Col_updateOutput(arg_state, arg_input, arg_output, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIm2Col_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIm2Col_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaIm2Col_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIm2Col_updateOutput(arg_state, arg_input, arg_output, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIm2Col_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIm2Col_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleIm2Col_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIm2Col_updateOutput(arg_state, arg_input, arg_output, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIm2Col_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIm2Col_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfIm2Col_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIm2Col_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_inputHeight, arg_inputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIm2Col_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int inputHeight, int inputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIm2Col_updateGradInput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaIm2Col_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIm2Col_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_inputHeight, arg_inputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIm2Col_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int inputHeight, int inputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIm2Col_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleIm2Col_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIm2Col_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_inputHeight, arg_inputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIm2Col_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int inputHeight, int inputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfCol2Im_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfCol2Im_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfCol2Im_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfCol2Im_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputHeight, int outputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaCol2Im_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaCol2Im_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaCol2Im_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaCol2Im_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputHeight, int outputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleCol2Im_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleCol2Im_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleCol2Im_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleCol2Im_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputHeight, int outputWidth, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfCol2Im_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfCol2Im_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfCol2Im_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfCol2Im_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaCol2Im_updateGradInput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaCol2Im_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaCol2Im_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaCol2Im_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleCol2Im_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleCol2Im_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_sH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_sW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleCol2Im_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_kH, arg_kW, arg_dH, arg_dW, arg_padH, arg_padW, arg_sH, arg_sW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleCol2Im_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int kH, int kW, int dH, int dW, int padH, int padW, int sH, int sW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLeakyReLU_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, bool);

PyObject * CudaHalfLeakyReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_negval = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLeakyReLU_updateOutput(arg_state, arg_input, arg_output, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLeakyReLU_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLeakyReLU_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, bool);

PyObject * CudaLeakyReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_negval = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLeakyReLU_updateOutput(arg_state, arg_input, arg_output, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLeakyReLU_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLeakyReLU_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, bool);

PyObject * CudaDoubleLeakyReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLeakyReLU_updateOutput(arg_state, arg_input, arg_output, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLeakyReLU_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLeakyReLU_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, bool);

PyObject * CudaHalfLeakyReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_negval = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLeakyReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLeakyReLU_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLeakyReLU_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, bool);

PyObject * CudaLeakyReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_negval = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLeakyReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLeakyReLU_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLeakyReLU_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, bool);

PyObject * CudaDoubleLeakyReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLeakyReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLeakyReLU_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfGRUFused_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfGRUFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_hidden = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_hx = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_hy = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_storage = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfGRUFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_hx, arg_hy, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfGRUFused_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor hidden, [torch.cuda.HalfTensor bias1 or None], [torch.cuda.HalfTensor bias2 or None], torch.cuda.HalfTensor hx, torch.cuda.HalfTensor hy, torch.cuda.HalfTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaGRUFused_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaGRUFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_hidden = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_hx = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_hy = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_storage = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaGRUFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_hx, arg_hy, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaGRUFused_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor hidden, [torch.cuda.FloatTensor bias1 or None], [torch.cuda.FloatTensor bias2 or None], torch.cuda.FloatTensor hx, torch.cuda.FloatTensor hy, torch.cuda.FloatTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleGRUFused_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleGRUFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_hidden = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_hx = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_hy = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_storage = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleGRUFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_hx, arg_hy, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleGRUFused_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor hidden, [torch.cuda.DoubleTensor bias1 or None], [torch.cuda.DoubleTensor bias2 or None], torch.cuda.DoubleTensor hx, torch.cuda.DoubleTensor hy, torch.cuda.DoubleTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfGRUFused_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfGRUFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradInInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInHidden = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInputHx = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_storage = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfGRUFused_updateGradInput(arg_state, arg_gradInInput, arg_gradInHidden, arg_gradOutput, arg_gradInputHx, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfGRUFused_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradInInput, torch.cuda.HalfTensor gradInHidden, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInputHx, torch.cuda.HalfTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaGRUFused_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaGRUFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradInInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInHidden = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInputHx = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_storage = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaGRUFused_updateGradInput(arg_state, arg_gradInInput, arg_gradInHidden, arg_gradOutput, arg_gradInputHx, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaGRUFused_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradInInput, torch.cuda.FloatTensor gradInHidden, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInputHx, torch.cuda.FloatTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleGRUFused_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleGRUFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradInInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInHidden = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInputHx = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_storage = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleGRUFused_updateGradInput(arg_state, arg_gradInInput, arg_gradInHidden, arg_gradOutput, arg_gradInputHx, arg_storage);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleGRUFused_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradInInput, torch.cuda.DoubleTensor gradInHidden, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInputHx, torch.cuda.DoubleTensor storage)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLSTMFused_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfLSTMFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_hidden = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_cx = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_hy = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_cy = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLSTMFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_cx, arg_hy, arg_cy);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLSTMFused_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor hidden, [torch.cuda.HalfTensor bias1 or None], [torch.cuda.HalfTensor bias2 or None], torch.cuda.HalfTensor cx, torch.cuda.HalfTensor hy, torch.cuda.HalfTensor cy)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLSTMFused_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaLSTMFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_hidden = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_cx = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_hy = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_cy = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLSTMFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_cx, arg_hy, arg_cy);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLSTMFused_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor hidden, [torch.cuda.FloatTensor bias1 or None], [torch.cuda.FloatTensor bias2 or None], torch.cuda.FloatTensor cx, torch.cuda.FloatTensor hy, torch.cuda.FloatTensor cy)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLSTMFused_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleLSTMFused_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_hidden = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_bias1 = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_bias2 = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_cx = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_hy = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_cy = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLSTMFused_updateOutput(arg_state, arg_input, arg_hidden, arg_bias1, arg_bias2, arg_cx, arg_hy, arg_cy);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLSTMFused_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor hidden, [torch.cuda.DoubleTensor bias1 or None], [torch.cuda.DoubleTensor bias2 or None], torch.cuda.DoubleTensor cx, torch.cuda.DoubleTensor hy, torch.cuda.DoubleTensor cy)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLSTMFused_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfLSTMFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_storage = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInGates = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_prevC = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_cy = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_gradOutputCell = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_gradInputCx = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLSTMFused_updateGradInput(arg_state, arg_storage, arg_gradInGates, arg_prevC, arg_cy, arg_gradOutput, arg_gradOutputCell, arg_gradInputCx);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLSTMFused_updateGradInput", 1, "(int state, torch.cuda.HalfTensor storage, torch.cuda.HalfTensor gradInGates, torch.cuda.HalfTensor prevC, torch.cuda.HalfTensor cy, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradOutputCell, torch.cuda.HalfTensor gradInputCx)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLSTMFused_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaLSTMFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_storage = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInGates = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_prevC = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_cy = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_gradOutputCell = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_gradInputCx = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLSTMFused_updateGradInput(arg_state, arg_storage, arg_gradInGates, arg_prevC, arg_cy, arg_gradOutput, arg_gradOutputCell, arg_gradInputCx);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLSTMFused_updateGradInput", 1, "(int state, torch.cuda.FloatTensor storage, torch.cuda.FloatTensor gradInGates, torch.cuda.FloatTensor prevC, torch.cuda.FloatTensor cy, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradOutputCell, torch.cuda.FloatTensor gradInputCx)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLSTMFused_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleLSTMFused_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_storage = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInGates = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_prevC = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_cy = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_gradOutputCell = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_gradInputCx = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLSTMFused_updateGradInput(arg_state, arg_storage, arg_gradInGates, arg_prevC, arg_cy, arg_gradOutput, arg_gradOutputCell, arg_gradInputCx);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLSTMFused_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor storage, torch.cuda.DoubleTensor gradInGates, torch.cuda.DoubleTensor prevC, torch.cuda.DoubleTensor cy, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradOutputCell, torch.cuda.DoubleTensor gradInputCx)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLogSigmoid_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfLogSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_buffer = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLogSigmoid_updateOutput(arg_state, arg_input, arg_output, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLogSigmoid_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLogSigmoid_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaLogSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_buffer = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLogSigmoid_updateOutput(arg_state, arg_input, arg_output, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLogSigmoid_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLogSigmoid_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleLogSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_buffer = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLogSigmoid_updateOutput(arg_state, arg_input, arg_output, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLogSigmoid_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLogSigmoid_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfLogSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_buffer = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLogSigmoid_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLogSigmoid_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLogSigmoid_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaLogSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_buffer = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLogSigmoid_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLogSigmoid_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLogSigmoid_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleLogSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_buffer = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLogSigmoid_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLogSigmoid_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLookupTable_accGradParameters(void*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, float);

PyObject * CudaHalfLookupTable_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_sorted = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaLongTensor* arg_indices = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      int arg_paddingValue = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLookupTable_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_count, arg_sorted, arg_indices, arg_scaleGradByFreq, arg_paddingValue, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLookupTable_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.LongTensor count, [torch.cuda.LongTensor sorted or None], [torch.cuda.LongTensor indices or None], bool scaleGradByFreq, int paddingValue, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLookupTable_accGradParameters(void*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, float);

PyObject * CudaLookupTable_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_sorted = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaLongTensor* arg_indices = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      int arg_paddingValue = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLookupTable_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_count, arg_sorted, arg_indices, arg_scaleGradByFreq, arg_paddingValue, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLookupTable_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.LongTensor count, [torch.cuda.LongTensor sorted or None], [torch.cuda.LongTensor indices or None], bool scaleGradByFreq, int paddingValue, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLookupTable_accGradParameters(void*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, double);

PyObject * CudaDoubleLookupTable_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_sorted = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaLongTensor* arg_indices = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      int arg_paddingValue = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLookupTable_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_count, arg_sorted, arg_indices, arg_scaleGradByFreq, arg_paddingValue, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLookupTable_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.LongTensor count, [torch.cuda.LongTensor sorted or None], [torch.cuda.LongTensor indices or None], bool scaleGradByFreq, int paddingValue, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLookupTable_renorm(void*, THCudaLongTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfLookupTable_renorm(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_idx = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_maxNorm = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_normType = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLookupTable_renorm(arg_state, arg_idx, arg_weight, arg_maxNorm, arg_normType);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLookupTable_renorm", 1, "(int state, torch.cuda.LongTensor idx, torch.cuda.HalfTensor weight, float maxNorm, float normType)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLookupTable_renorm(void*, THCudaLongTensor*, THCudaTensor*, float, float);

PyObject * CudaLookupTable_renorm(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_idx = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_maxNorm = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_normType = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLookupTable_renorm(arg_state, arg_idx, arg_weight, arg_maxNorm, arg_normType);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLookupTable_renorm", 1, "(int state, torch.cuda.LongTensor idx, torch.cuda.FloatTensor weight, float maxNorm, float normType)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLookupTable_renorm(void*, THCudaLongTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleLookupTable_renorm(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_idx = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_maxNorm = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_normType = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLookupTable_renorm(arg_state, arg_idx, arg_weight, arg_maxNorm, arg_normType);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLookupTable_renorm", 1, "(int state, torch.cuda.LongTensor idx, torch.cuda.DoubleTensor weight, float maxNorm, float normType)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLookupTableBag_updateOutput(void*, THCudaLongTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, THCudaLongTensor*);

PyObject * CudaHalfLookupTableBag_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None)) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_offsets = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLookupTableBag_updateOutput(arg_state, arg_input, arg_offsets, arg_weight, arg_output, arg_offset2bag, arg_mode, arg_seq_length);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLookupTableBag_updateOutput", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.LongTensor offsets, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor output, torch.cuda.LongTensor offset2bag, int mode, [torch.cuda.LongTensor seq_length or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLookupTableBag_updateOutput(void*, THCudaLongTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, THCudaLongTensor*);

PyObject * CudaLookupTableBag_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None)) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_offsets = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLookupTableBag_updateOutput(arg_state, arg_input, arg_offsets, arg_weight, arg_output, arg_offset2bag, arg_mode, arg_seq_length);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLookupTableBag_updateOutput", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.LongTensor offsets, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor output, torch.cuda.LongTensor offset2bag, int mode, [torch.cuda.LongTensor seq_length or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLookupTableBag_updateOutput(void*, THCudaLongTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, THCudaLongTensor*);

PyObject * CudaDoubleLookupTableBag_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None)) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_offsets = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLookupTableBag_updateOutput(arg_state, arg_input, arg_offsets, arg_weight, arg_output, arg_offset2bag, arg_mode, arg_seq_length);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLookupTableBag_updateOutput", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.LongTensor offsets, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor output, torch.cuda.LongTensor offset2bag, int mode, [torch.cuda.LongTensor seq_length or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfLookupTableBag_accGradParameters(void*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, THCudaLongTensor*, float);

PyObject * CudaHalfLookupTableBag_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_sortedIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_origIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      float arg_scale_ = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfLookupTableBag_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_offset2bag, arg_count, arg_sortedIndices, arg_origIndices, arg_scaleGradByFreq, arg_mode, arg_seq_length, arg_scale_);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfLookupTableBag_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.LongTensor offset2bag, torch.cuda.LongTensor count, torch.cuda.LongTensor sortedIndices, torch.cuda.LongTensor origIndices, bool scaleGradByFreq, int mode, [torch.cuda.LongTensor seq_length or None], float scale_)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaLookupTableBag_accGradParameters(void*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, THCudaLongTensor*, float);

PyObject * CudaLookupTableBag_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_sortedIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_origIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      float arg_scale_ = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaLookupTableBag_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_offset2bag, arg_count, arg_sortedIndices, arg_origIndices, arg_scaleGradByFreq, arg_mode, arg_seq_length, arg_scale_);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaLookupTableBag_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.LongTensor offset2bag, torch.cuda.LongTensor count, torch.cuda.LongTensor sortedIndices, torch.cuda.LongTensor origIndices, bool scaleGradByFreq, int mode, [torch.cuda.LongTensor seq_length or None], float scale_)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleLookupTableBag_accGradParameters(void*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaLongTensor*, bool, int, THCudaLongTensor*, double);

PyObject * CudaDoubleLookupTableBag_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          (THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10)) || PyTuple_GET_ITEM(args, 10) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_input = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_offset2bag = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_count = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_sortedIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_origIndices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      bool arg_scaleGradByFreq = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      int arg_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_seq_length = (PyTuple_GET_ITEM(args, 10) == Py_None ? NULL : THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10)));
      double arg_scale_ = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleLookupTableBag_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_offset2bag, arg_count, arg_sortedIndices, arg_origIndices, arg_scaleGradByFreq, arg_mode, arg_seq_length, arg_scale_);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleLookupTableBag_accGradParameters", 1, "(int state, torch.cuda.LongTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.LongTensor offset2bag, torch.cuda.LongTensor count, torch.cuda.LongTensor sortedIndices, torch.cuda.LongTensor origIndices, bool scaleGradByFreq, int mode, [torch.cuda.LongTensor seq_length or None], float scale_)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfL1Cost_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfL1Cost_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfL1Cost_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfL1Cost_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaL1Cost_updateOutput(void*, THCudaTensor*, THCudaTensor*);

PyObject * CudaL1Cost_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaL1Cost_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaL1Cost_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleL1Cost_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleL1Cost_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleL1Cost_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleL1Cost_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfL1Cost_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfL1Cost_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) || PyTuple_GET_ITEM(args, 2) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = (PyTuple_GET_ITEM(args, 2) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2)));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfL1Cost_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfL1Cost_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, [torch.cuda.HalfTensor gradOutput or None], torch.cuda.HalfTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaL1Cost_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaL1Cost_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) || PyTuple_GET_ITEM(args, 2) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = (PyTuple_GET_ITEM(args, 2) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2)));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaL1Cost_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaL1Cost_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, [torch.cuda.FloatTensor gradOutput or None], torch.cuda.FloatTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleL1Cost_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleL1Cost_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) || PyTuple_GET_ITEM(args, 2) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = (PyTuple_GET_ITEM(args, 2) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2)));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleL1Cost_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleL1Cost_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, [torch.cuda.DoubleTensor gradOutput or None], torch.cuda.DoubleTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMarginCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, float);

PyObject * CudaHalfMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMarginCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMarginCriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, float);

PyObject * CudaMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMarginCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMarginCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, double);

PyObject * CudaDoubleMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMarginCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMarginCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, float);

PyObject * CudaHalfMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradInput, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradInput, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMarginCriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, float);

PyObject * CudaMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradInput, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradInput, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMarginCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, double);

PyObject * CudaDoubleMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradInput, arg_sizeAverage, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradInput, bool sizeAverage, float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMSECriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfMSECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMSECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMSECriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMSECriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaMSECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMSECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMSECriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMSECriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleMSECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMSECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMSECriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMSECriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfMSECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMSECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMSECriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMSECriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaMSECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMSECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMSECriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMSECriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleMSECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMSECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMSECriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfMultiLabelMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_istarget = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMultiLabelMarginCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor output, torch.cuda.HalfTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMultiLabelMarginCriterion_updateOutput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaMultiLabelMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_istarget = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMultiLabelMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMultiLabelMarginCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, torch.cuda.FloatTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleMultiLabelMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_istarget = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMultiLabelMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMultiLabelMarginCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfMultiLabelMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_istarget = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMultiLabelMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMultiLabelMarginCriterion_updateGradInput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaMultiLabelMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_istarget = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMultiLabelMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMultiLabelMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleMultiLabelMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_istarget = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      bool arg_sizeaverage = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMultiLabelMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_istarget, arg_sizeaverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMultiLabelMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor istarget, bool sizeaverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMultiMarginCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, bool, int, THCudaHalfTensor*, float, bool);

PyObject * CudaHalfMultiMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMultiMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMultiMarginCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor output, bool sizeAverage, int p, [torch.cuda.HalfTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMultiMarginCriterion_updateOutput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, bool, int, THCudaTensor*, float, bool);

PyObject * CudaMultiMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMultiMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMultiMarginCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, int p, [torch.cuda.FloatTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMultiMarginCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, bool, int, THCudaDoubleTensor*, double, bool);

PyObject * CudaDoubleMultiMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMultiMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMultiMarginCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, int p, [torch.cuda.DoubleTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfMultiMarginCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, int, THCudaHalfTensor*, float, bool);

PyObject * CudaHalfMultiMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfMultiMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfMultiMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, int p, [torch.cuda.HalfTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaMultiMarginCriterion_updateGradInput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, bool, int, THCudaTensor*, float, bool);

PyObject * CudaMultiMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      float arg_margin = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaMultiMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaMultiMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, int p, [torch.cuda.FloatTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleMultiMarginCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, int, THCudaDoubleTensor*, double, bool);

PyObject * CudaDoubleMultiMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleMultiMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_p, arg_weights, arg_margin, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleMultiMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, int p, [torch.cuda.DoubleTensor weights or None], float margin, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfPReLU_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfPReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfPReLU_updateOutput(arg_state, arg_input, arg_output, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfPReLU_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaPReLU_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaPReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaPReLU_updateOutput(arg_state, arg_input, arg_output, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaPReLU_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoublePReLU_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoublePReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoublePReLU_updateOutput(arg_state, arg_input, arg_output, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoublePReLU_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfPReLU_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfPReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfPReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfPReLU_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaPReLU_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaPReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaPReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaPReLU_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoublePReLU_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoublePReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoublePReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoublePReLU_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfPReLU_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float);

PyObject * CudaHalfPReLU_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfPReLU_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_gradWeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfPReLU_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor gradWeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaPReLU_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float);

PyObject * CudaPReLU_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaPReLU_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_gradWeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaPReLU_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor gradWeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoublePReLU_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double);

PyObject * CudaDoublePReLU_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoublePReLU_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_gradWeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoublePReLU_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor gradWeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSmoothL1Criterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfSmoothL1Criterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSmoothL1Criterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSmoothL1Criterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSmoothL1Criterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaSmoothL1Criterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSmoothL1Criterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSmoothL1Criterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSmoothL1Criterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleSmoothL1Criterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSmoothL1Criterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSmoothL1Criterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSmoothL1Criterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfSmoothL1Criterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSmoothL1Criterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSmoothL1Criterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSmoothL1Criterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaSmoothL1Criterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSmoothL1Criterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSmoothL1Criterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSmoothL1Criterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleSmoothL1Criterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSmoothL1Criterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSmoothL1Criterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSparseLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSparseLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSparseLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfSparseLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaSparseLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleSparseLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_legacyUpdateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSparseLinear_legacyUpdateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_legacyUpdateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_legacyUpdateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_legacyUpdateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSparseLinear_legacyUpdateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_legacyUpdateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_legacyUpdateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_legacyUpdateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSparseLinear_legacyUpdateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_legacyUpdateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_legacyUpdateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_legacyAccGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfSparseLinear_legacyAccGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_legacyAccGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_legacyAccGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_legacyAccGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaSparseLinear_legacyAccGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_legacyAccGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_legacyAccGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_legacyAccGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleSparseLinear_legacyAccGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_legacyAccGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_legacyAccGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_zeroGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSparseLinear_zeroGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_lastInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_zeroGradParameters(arg_state, arg_gradWeight, arg_gradBias, arg_lastInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_zeroGradParameters", 1, "(int state, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor lastInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_zeroGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSparseLinear_zeroGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_lastInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_zeroGradParameters(arg_state, arg_gradWeight, arg_gradBias, arg_lastInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_zeroGradParameters", 1, "(int state, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor lastInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_zeroGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSparseLinear_zeroGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_lastInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_zeroGradParameters(arg_state, arg_gradWeight, arg_gradBias, arg_lastInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_zeroGradParameters", 1, "(int state, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor lastInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSparseLinear_updateParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float);

PyObject * CudaHalfSparseLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_lastInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      float arg_learningRate = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSparseLinear_updateParameters(arg_state, arg_weight, arg_bias, arg_gradWeight, arg_gradBias, arg_lastInput, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSparseLinear_updateParameters", 1, "(int state, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor lastInput, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSparseLinear_updateParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float);

PyObject * CudaSparseLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_lastInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      float arg_learningRate = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSparseLinear_updateParameters(arg_state, arg_weight, arg_bias, arg_gradWeight, arg_gradBias, arg_lastInput, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSparseLinear_updateParameters", 1, "(int state, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor lastInput, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSparseLinear_updateParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double);

PyObject * CudaDoubleSparseLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_lastInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      double arg_learningRate = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSparseLinear_updateParameters(arg_state, arg_weight, arg_bias, arg_gradWeight, arg_gradBias, arg_lastInput, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSparseLinear_updateParameters", 1, "(int state, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor lastInput, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIndexLinear_updateOutput(void*, THCudaLongTensor*, int64_t, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfIndexLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_values = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaHalfTensor* arg_normalizedValues = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      int arg_train = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIndexLinear_updateOutput(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_output, arg_weight, arg_bias, arg_normalizedValues, arg_train);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIndexLinear_updateOutput", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.HalfTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, torch.cuda.HalfTensor normalizedValues, int train)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIndexLinear_updateOutput(void*, THCudaLongTensor*, int64_t, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaIndexLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_values = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaTensor* arg_normalizedValues = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      int arg_train = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIndexLinear_updateOutput(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_output, arg_weight, arg_bias, arg_normalizedValues, arg_train);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIndexLinear_updateOutput", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.FloatTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, torch.cuda.FloatTensor normalizedValues, int train)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIndexLinear_updateOutput(void*, THCudaLongTensor*, int64_t, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleIndexLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_values = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaDoubleTensor* arg_normalizedValues = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      int arg_train = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIndexLinear_updateOutput(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_output, arg_weight, arg_bias, arg_normalizedValues, arg_train);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIndexLinear_updateOutput", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.DoubleTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, torch.cuda.DoubleTensor normalizedValues, int train)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIndexLinear_accGradParameters(void*, THCudaLongTensor*, int64_t, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfIndexLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 10)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_values = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      THCudaHalfTensor* arg_valuesBuffer = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 11));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIndexLinear_accGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_valuesBuffer, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIndexLinear_accGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.HalfTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, torch.cuda.HalfTensor valuesBuffer, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIndexLinear_accGradParameters(void*, THCudaLongTensor*, int64_t, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaIndexLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 10)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_values = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      THCudaTensor* arg_valuesBuffer = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 11));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIndexLinear_accGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_valuesBuffer, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIndexLinear_accGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.FloatTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, torch.cuda.FloatTensor valuesBuffer, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIndexLinear_accGradParameters(void*, THCudaLongTensor*, int64_t, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleIndexLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 10)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_values = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      THCudaDoubleTensor* arg_valuesBuffer = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 11));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIndexLinear_accGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_valuesBuffer, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIndexLinear_accGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.DoubleTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, torch.cuda.DoubleTensor valuesBuffer, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIndexLinear_accUpdateGradParameters(void*, THCudaLongTensor*, int64_t, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfIndexLinear_accUpdateGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_values = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIndexLinear_accUpdateGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIndexLinear_accUpdateGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.HalfTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIndexLinear_accUpdateGradParameters(void*, THCudaLongTensor*, int64_t, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaIndexLinear_accUpdateGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_values = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIndexLinear_accUpdateGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIndexLinear_accUpdateGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.FloatTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIndexLinear_accUpdateGradParameters(void*, THCudaLongTensor*, int64_t, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleIndexLinear_accUpdateGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaLongTensor* arg_keys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_values = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_sizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIndexLinear_accUpdateGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIndexLinear_accUpdateGradParameters", 1, "(int state, torch.cuda.LongTensor keys, int keysOffset, torch.cuda.DoubleTensor values, torch.cuda.LongTensor sizes, torch.cuda.LongTensor cumSumSizes, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfIndexLinear_updateParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, THCudaLongTensor*, int64_t, float, float);

PyObject * CudaHalfIndexLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_runningKeys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      float arg_learningRate = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfIndexLinear_updateParameters(arg_state, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_runningKeys, arg_cumSumSizes, arg_keysOffset, arg_weightDecay, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfIndexLinear_updateParameters", 1, "(int state, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, torch.cuda.LongTensor runningKeys, torch.cuda.LongTensor cumSumSizes, int keysOffset, float weightDecay, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaIndexLinear_updateParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, THCudaLongTensor*, int64_t, float, float);

PyObject * CudaIndexLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_runningKeys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      float arg_weightDecay = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      float arg_learningRate = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaIndexLinear_updateParameters(arg_state, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_runningKeys, arg_cumSumSizes, arg_keysOffset, arg_weightDecay, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaIndexLinear_updateParameters", 1, "(int state, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, torch.cuda.LongTensor runningKeys, torch.cuda.LongTensor cumSumSizes, int keysOffset, float weightDecay, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleIndexLinear_updateParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaLongTensor*, int64_t, double, double);

PyObject * CudaDoubleIndexLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaLongTensor* arg_runningKeys = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaLongTensor* arg_cumSumSizes = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      double arg_learningRate = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleIndexLinear_updateParameters(arg_state, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_runningKeys, arg_cumSumSizes, arg_keysOffset, arg_weightDecay, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleIndexLinear_updateParameters", 1, "(int state, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, torch.cuda.LongTensor runningKeys, torch.cuda.LongTensor cumSumSizes, int keysOffset, float weightDecay, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAdaptiveMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int);

PyObject * CudaHalfSpatialAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAdaptiveMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int);

PyObject * CudaSpatialAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAdaptiveMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int);

PyObject * CudaDoubleSpatialAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAdaptiveMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*);

PyObject * CudaHalfSpatialAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAdaptiveMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*);

PyObject * CudaSpatialAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*);

PyObject * CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAdaptiveAveragePooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfSpatialAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAdaptiveAveragePooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaSpatialAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAdaptiveAveragePooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleSpatialAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAdaptiveAveragePooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSpatialAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAdaptiveAveragePooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSpatialAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAveragePooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaHalfSpatialAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAveragePooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAveragePooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaSpatialAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAveragePooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAveragePooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaDoubleSpatialAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAveragePooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialAveragePooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaHalfSpatialAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialAveragePooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialAveragePooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaSpatialAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialAveragePooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialAveragePooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, bool, bool);

PyObject * CudaDoubleSpatialAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialAveragePooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, THCudaHalfTensor*, int64_t, bool);

PyObject * CudaHalfSpatialClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaHalfTensor* arg_total_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor output, bool sizeAverage, [torch.cuda.HalfTensor weights or None], torch.cuda.HalfTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialClassNLLCriterion_updateOutput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, bool, THCudaTensor*, THCudaTensor*, int64_t, bool);

PyObject * CudaSpatialClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaTensor* arg_total_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, THCudaDoubleTensor*, int64_t, bool);

PyObject * CudaDoubleSpatialClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THCudaDoubleTensor* arg_total_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialClassNLLCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], torch.cuda.DoubleTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaLongTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, THCudaHalfTensor*, THCudaHalfTensor*, int64_t, bool);

PyObject * CudaHalfSpatialClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaHalfTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaHalfTensor* arg_total_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.LongTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, [torch.cuda.HalfTensor weights or None], torch.cuda.HalfTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialClassNLLCriterion_updateGradInput(void*, THCudaTensor*, THCudaLongTensor*, THCudaTensor*, THCudaTensor*, bool, THCudaTensor*, THCudaTensor*, int64_t, bool);

PyObject * CudaSpatialClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaTensor* arg_total_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaLongTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, THCudaDoubleTensor*, THCudaDoubleTensor*, int64_t, bool);

PyObject * CudaDoubleSpatialClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaLongTensor* arg_target = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      THCudaDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THCudaDoubleTensor* arg_total_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_reduce = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_weights, arg_total_weight, arg_ignore_index, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialClassNLLCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.LongTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, [torch.cuda.DoubleTensor weights or None], torch.cuda.DoubleTensor total_weight, int ignore_index, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionLocal_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaHalfSpatialConvolutionLocal_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionLocal_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionLocal_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionLocal_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaSpatialConvolutionLocal_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionLocal_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionLocal_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionLocal_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaDoubleSpatialConvolutionLocal_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionLocal_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionLocal_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionLocal_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaHalfSpatialConvolutionLocal_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionLocal_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionLocal_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionLocal_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaSpatialConvolutionLocal_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionLocal_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionLocal_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionLocal_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t);

PyObject * CudaDoubleSpatialConvolutionLocal_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionLocal_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionLocal_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionLocal_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t, float);

PyObject * CudaHalfSpatialConvolutionLocal_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionLocal_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionLocal_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionLocal_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t, float);

PyObject * CudaSpatialConvolutionLocal_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionLocal_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionLocal_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionLocal_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int64_t, int64_t, int64_t, int64_t, double);

PyObject * CudaDoubleSpatialConvolutionLocal_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int64_t arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int64_t arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int64_t arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int64_t arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionLocal_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionLocal_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, int inputWidth, int inputHeight, int outputWidth, int outputHeight, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionMM_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfSpatialConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionMM_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionMM_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaSpatialConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionMM_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionMM_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionMM_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionMM_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfSpatialConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionMM_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionMM_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaSpatialConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionMM_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionMM_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialConvolutionMM_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, float);

PyObject * CudaHalfSpatialConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialConvolutionMM_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialConvolutionMM_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, float);

PyObject * CudaSpatialConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialConvolutionMM_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, double);

PyObject * CudaDoubleSpatialConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialConvolutionMM_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialDepthwiseConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDepthwiseConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDepthwiseConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialDepthwiseConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDepthwiseConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDepthwiseConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialDepthwiseConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDepthwiseConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDepthwiseConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialDepthwiseConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDepthwiseConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDepthwiseConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialDepthwiseConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDepthwiseConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDepthwiseConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialDepthwiseConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDepthwiseConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDepthwiseConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialDepthwiseConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDepthwiseConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDepthwiseConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialDepthwiseConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDepthwiseConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDepthwiseConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialDepthwiseConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDepthwiseConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDepthwiseConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialCrossMapLRN_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, float, float, float);

PyObject * CudaHalfSpatialCrossMapLRN_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_scale = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      float arg_k = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialCrossMapLRN_updateOutput(arg_state, arg_input, arg_output, arg_scale, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialCrossMapLRN_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor scale, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialCrossMapLRN_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, float, float, float);

PyObject * CudaSpatialCrossMapLRN_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_scale = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      float arg_k = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialCrossMapLRN_updateOutput(arg_state, arg_input, arg_output, arg_scale, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialCrossMapLRN_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor scale, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialCrossMapLRN_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, double, double, double);

PyObject * CudaDoubleSpatialCrossMapLRN_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_scale = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      double arg_k = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialCrossMapLRN_updateOutput(arg_state, arg_input, arg_output, arg_scale, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialCrossMapLRN_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor scale, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialCrossMapLRN_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, float, float, float);

PyObject * CudaHalfSpatialCrossMapLRN_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_scale = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      float arg_k = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialCrossMapLRN_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale, arg_output, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialCrossMapLRN_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor scale, torch.cuda.HalfTensor output, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialCrossMapLRN_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, float, float, float);

PyObject * CudaSpatialCrossMapLRN_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_scale = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      float arg_alpha = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      float arg_k = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialCrossMapLRN_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale, arg_output, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialCrossMapLRN_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor scale, torch.cuda.FloatTensor output, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialCrossMapLRN_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, double, double, double);

PyObject * CudaDoubleSpatialCrossMapLRN_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_scale = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_size = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      double arg_k = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialCrossMapLRN_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale, arg_output, arg_size, arg_alpha, arg_beta, arg_k);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialCrossMapLRN_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor scale, torch.cuda.DoubleTensor output, int size, float alpha, float beta, float k)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDilatedConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDilatedConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDilatedConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDilatedConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDilatedConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfSpatialDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDilatedConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, float);

PyObject * CudaSpatialDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDilatedConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleSpatialDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullDilatedConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullDilatedConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullDilatedConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullDilatedConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfSpatialFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullDilatedConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaSpatialFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullDilatedConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleSpatialFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfSpatialDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDilatedMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaSpatialDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDilatedMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleSpatialDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfSpatialDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialDilatedMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaSpatialDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialDilatedMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleSpatialDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_dilationW, arg_dilationH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, int dilationW, int dilationH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFractionalMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, THCudaLongTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSpatialFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaHalfTensor* arg_randomSamples = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.HalfTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFractionalMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, THCudaLongTensor*, THCudaTensor*);

PyObject * CudaSpatialFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaTensor* arg_randomSamples = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.FloatTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFractionalMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, THCudaLongTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSpatialFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THCudaDoubleTensor* arg_randomSamples = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.DoubleTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFractionalMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, THCudaLongTensor*);

PyObject * CudaHalfSpatialFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFractionalMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, THCudaLongTensor*);

PyObject * CudaSpatialFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFractionalMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, THCudaLongTensor*);

PyObject * CudaDoubleSpatialFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputW, arg_outputH, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int outputW, int outputH, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaHalfSpatialFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaSpatialFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleSpatialFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor columns, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialFullConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfSpatialFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialFullConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialFullConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, float);

PyObject * CudaSpatialFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialFullConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialFullConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleSpatialFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 15))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialFullConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kW, int kH, int dW, int dH, int padW, int padH, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaHalfSpatialMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaSpatialMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaDoubleSpatialMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaHalfSpatialMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaSpatialMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, bool);

PyObject * CudaDoubleSpatialMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_ceil_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int kW, int kH, int dW, int dH, int padW, int padH, bool ceil_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialMaxUnpooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int);

PyObject * CudaHalfSpatialMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialMaxUnpooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int);

PyObject * CudaSpatialMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialMaxUnpooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int);

PyObject * CudaDoubleSpatialMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int);

PyObject * CudaHalfSpatialMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialMaxUnpooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int);

PyObject * CudaSpatialMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialMaxUnpooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int);

PyObject * CudaDoubleSpatialMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_owidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_oheight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_owidth, arg_oheight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int owidth, int oheight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialReflectionPadding_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialReflectionPadding_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialReflectionPadding_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialReflectionPadding_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialReflectionPadding_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialReflectionPadding_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialReflectionPadding_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialReflectionPadding_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialReflectionPadding_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialReplicationPadding_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialReplicationPadding_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialReplicationPadding_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialReplicationPadding_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialReplicationPadding_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialReplicationPadding_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialReplicationPadding_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialReplicationPadding_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialReplicationPadding_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padB = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR, arg_padT, arg_padB);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int padL, int padR, int padT, int padB)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialSubSampling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialSubSampling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialSubSampling_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialSubSampling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialSubSampling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialSubSampling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialSubSampling_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialSubSampling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialSubSampling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialSubSampling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialSubSampling_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialSubSampling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialSubSampling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfSpatialSubSampling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialSubSampling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialSubSampling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialSubSampling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaSpatialSubSampling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialSubSampling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialSubSampling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialSubSampling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleSpatialSubSampling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialSubSampling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_kH, arg_dW, arg_dH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialSubSampling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, int kW, int kH, int dW, int dH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialSubSampling_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, float);

PyObject * CudaHalfSpatialSubSampling_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialSubSampling_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_kH, arg_dW, arg_dH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialSubSampling_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, int kW, int kH, int dW, int dH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialSubSampling_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, float);

PyObject * CudaSpatialSubSampling_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialSubSampling_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_kH, arg_dW, arg_dH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialSubSampling_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, int kW, int kH, int dW, int dH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialSubSampling_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, double);

PyObject * CudaDoubleSpatialSubSampling_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialSubSampling_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_kH, arg_dW, arg_dH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialSubSampling_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, int kW, int kH, int dW, int dH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, bool);

PyObject * CudaHalfSpatialUpSamplingBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialUpSamplingBilinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialUpSamplingBilinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, bool);

PyObject * CudaSpatialUpSamplingBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialUpSamplingBilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialUpSamplingBilinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialUpSamplingBilinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, bool);

PyObject * CudaDoubleSpatialUpSamplingBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialUpSamplingBilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialUpSamplingBilinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, bool);

PyObject * CudaHalfSpatialUpSamplingBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputHeight, arg_inputWidth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialUpSamplingBilinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int nbatch, int nchannels, int inputHeight, int inputWidth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialUpSamplingBilinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, bool);

PyObject * CudaSpatialUpSamplingBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialUpSamplingBilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputHeight, arg_inputWidth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialUpSamplingBilinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int nbatch, int nchannels, int inputHeight, int inputWidth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialUpSamplingBilinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, bool);

PyObject * CudaDoubleSpatialUpSamplingBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 9) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialUpSamplingBilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputHeight, arg_inputWidth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialUpSamplingBilinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int nbatch, int nchannels, int inputHeight, int inputWidth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfSpatialUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialUpSamplingNearest_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaSpatialUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialUpSamplingNearest_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleSpatialUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfSpatialUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialUpSamplingNearest_updateOutput(void*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaSpatialUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialUpSamplingNearest_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleSpatialUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialGridSamplerBilinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfSpatialGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_grid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor grid, torch.cuda.HalfTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialGridSamplerBilinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaSpatialGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_grid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor grid, torch.cuda.FloatTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialGridSamplerBilinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleSpatialGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_grid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor grid, torch.cuda.DoubleTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSpatialGridSamplerBilinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfSpatialGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_grid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradGrid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSpatialGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSpatialGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor grid, torch.cuda.HalfTensor gradGrid, torch.cuda.HalfTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSpatialGridSamplerBilinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaSpatialGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_grid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradGrid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSpatialGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSpatialGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor grid, torch.cuda.FloatTensor gradGrid, torch.cuda.FloatTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSpatialGridSamplerBilinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleSpatialGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_grid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradGrid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSpatialGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSpatialGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor grid, torch.cuda.DoubleTensor gradGrid, torch.cuda.DoubleTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricGridSamplerBilinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfVolumetricGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_grid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor grid, torch.cuda.HalfTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricGridSamplerBilinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaVolumetricGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_grid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor grid, torch.cuda.FloatTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricGridSamplerBilinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleVolumetricGridSamplerBilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_grid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricGridSamplerBilinear_updateOutput(arg_state, arg_input, arg_grid, arg_output, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricGridSamplerBilinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor grid, torch.cuda.DoubleTensor output, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricGridSamplerBilinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfVolumetricGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_grid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradGrid = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor grid, torch.cuda.HalfTensor gradGrid, torch.cuda.HalfTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricGridSamplerBilinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaVolumetricGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_grid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradGrid = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor grid, torch.cuda.FloatTensor gradGrid, torch.cuda.FloatTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricGridSamplerBilinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleVolumetricGridSamplerBilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_grid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradGrid = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_padding_mode = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricGridSamplerBilinear_updateGradInput(arg_state, arg_input, arg_gradInput, arg_grid, arg_gradGrid, arg_gradOutput, arg_padding_mode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricGridSamplerBilinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor grid, torch.cuda.DoubleTensor gradGrid, torch.cuda.DoubleTensor gradOutput, int padding_mode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfRReLU_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, double, double, bool, bool, void*);

PyObject * CudaHalfRReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_noise = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_train = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      void* arg_generator = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfRReLU_updateOutput(arg_state, arg_input, arg_output, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace, arg_generator);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfRReLU_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor noise, float lower, float upper, bool train, bool inplace, int generator)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaRReLU_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, double, double, bool, bool, void*);

PyObject * CudaRReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_noise = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_train = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      void* arg_generator = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaRReLU_updateOutput(arg_state, arg_input, arg_output, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace, arg_generator);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaRReLU_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor noise, float lower, float upper, bool train, bool inplace, int generator)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleRReLU_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool, bool, void*);

PyObject * CudaDoubleRReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_noise = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_train = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      void* arg_generator = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleRReLU_updateOutput(arg_state, arg_input, arg_output, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace, arg_generator);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleRReLU_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor noise, float lower, float upper, bool train, bool inplace, int generator)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfRReLU_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, double, double, bool, bool);

PyObject * CudaHalfRReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_noise = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      bool arg_train = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfRReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfRReLU_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor noise, float lower, float upper, bool train, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaRReLU_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, double, double, bool, bool);

PyObject * CudaRReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_noise = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      bool arg_train = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaRReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaRReLU_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor noise, float lower, float upper, bool train, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleRReLU_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool, bool);

PyObject * CudaDoubleRReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_noise = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      bool arg_train = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleRReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleRReLU_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor noise, float lower, float upper, bool train, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSigmoid_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSigmoid_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSigmoid_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSigmoid_updateOutput(void*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSigmoid_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSigmoid_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSigmoid_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSigmoid_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSigmoid_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSigmoid_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSigmoid_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSigmoid_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSigmoid_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSigmoid_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSigmoid_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSigmoid_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSigmoid_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSigmoid_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftMarginCriterion_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfSoftMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftMarginCriterion_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftMarginCriterion_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaSoftMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftMarginCriterion_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftMarginCriterion_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleSoftMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftMarginCriterion_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor output, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftMarginCriterion_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, bool, bool);

PyObject * CudaHalfSoftMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_target = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor target, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftMarginCriterion_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, bool, bool);

PyObject * CudaSoftMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_target = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor target, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftMarginCriterion_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, bool, bool);

PyObject * CudaDoubleSoftMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_target = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      bool arg_sizeAverage = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      bool arg_reduce = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_sizeAverage, arg_reduce);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftMarginCriterion_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor target, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, bool sizeAverage, bool reduce)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftPlus_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfSoftPlus_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftPlus_updateOutput(arg_state, arg_input, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftPlus_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftPlus_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaSoftPlus_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftPlus_updateOutput(arg_state, arg_input, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftPlus_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftPlus_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleSoftPlus_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftPlus_updateOutput(arg_state, arg_input, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftPlus_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftPlus_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float);

PyObject * CudaHalfSoftPlus_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftPlus_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftPlus_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftPlus_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float);

PyObject * CudaSoftPlus_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      float arg_beta = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftPlus_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftPlus_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftPlus_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double);

PyObject * CudaDoubleSoftPlus_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftPlus_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftPlus_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftShrink_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float);

PyObject * CudaHalfSoftShrink_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_lambda = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftShrink_updateOutput(arg_state, arg_input, arg_output, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftShrink_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftShrink_updateOutput(void*, THCudaTensor*, THCudaTensor*, float);

PyObject * CudaSoftShrink_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_lambda = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftShrink_updateOutput(arg_state, arg_input, arg_output, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftShrink_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftShrink_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double);

PyObject * CudaDoubleSoftShrink_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftShrink_updateOutput(arg_state, arg_input, arg_output, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftShrink_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSoftShrink_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float);

PyObject * CudaHalfSoftShrink_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_lambda = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSoftShrink_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSoftShrink_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSoftShrink_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float);

PyObject * CudaSoftShrink_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_lambda = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSoftShrink_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSoftShrink_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSoftShrink_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double);

PyObject * CudaDoubleSoftShrink_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSoftShrink_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSoftShrink_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSquare_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSquare_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSquare_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSquare_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSquare_updateOutput(void*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSquare_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSquare_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSquare_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSquare_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSquare_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSquare_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSquare_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSquare_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSquare_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSquare_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSquare_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSquare_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSquare_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSquare_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSquare_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSquare_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSquare_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSquare_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSquare_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSqrt_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float);

PyObject * CudaHalfSqrt_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_eps = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSqrt_updateOutput(arg_state, arg_input, arg_output, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSqrt_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSqrt_updateOutput(void*, THCudaTensor*, THCudaTensor*, float);

PyObject * CudaSqrt_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_eps = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSqrt_updateOutput(arg_state, arg_input, arg_output, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSqrt_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSqrt_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double);

PyObject * CudaDoubleSqrt_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_eps = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSqrt_updateOutput(arg_state, arg_input, arg_output, arg_eps);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSqrt_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float eps)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfSqrt_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfSqrt_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfSqrt_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfSqrt_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaSqrt_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaSqrt_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaSqrt_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaSqrt_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleSqrt_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleSqrt_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleSqrt_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleSqrt_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTanh_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTanh_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTanh_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTanh_updateOutput(void*, THCudaTensor*, THCudaTensor*);

PyObject * CudaTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTanh_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTanh_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTanh_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTanh_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTanh_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTanh_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTanh_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTanh_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTanh_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTanh_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTanh_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTanh_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTanh_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTanh_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int);

PyObject * CudaHalfTemporalConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_dW, arg_inputFrameSize, arg_outputFrameSize);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor bias, int kW, int dW, int inputFrameSize, int outputFrameSize)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int);

PyObject * CudaTemporalConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_dW, arg_inputFrameSize, arg_outputFrameSize);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor bias, int kW, int dW, int inputFrameSize, int outputFrameSize)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int);

PyObject * CudaDoubleTemporalConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputFrameSize = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_kW, arg_dW, arg_inputFrameSize, arg_outputFrameSize);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor bias, int kW, int dW, int inputFrameSize, int outputFrameSize)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfTemporalConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaTemporalConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleTemporalConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, float);

PyObject * CudaHalfTemporalConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_dW, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, int kW, int dW, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, float);

PyObject * CudaTemporalConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_dW, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, int kW, int dW, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, double);

PyObject * CudaDoubleTemporalConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_kW, arg_dW, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, int kW, int dW, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int);

PyObject * CudaHalfTemporalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int);

PyObject * CudaTemporalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int);

PyObject * CudaDoubleTemporalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int);

PyObject * CudaHalfTemporalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int);

PyObject * CudaTemporalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int);

PyObject * CudaDoubleTemporalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kW, arg_dW);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int kW, int dW)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalRowConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, bool);

PyObject * CudaHalfTemporalRowConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalRowConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalRowConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalRowConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, bool);

PyObject * CudaTemporalRowConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalRowConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalRowConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalRowConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, bool);

PyObject * CudaDoubleTemporalRowConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalRowConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalRowConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalRowConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, bool);

PyObject * CudaHalfTemporalRowConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalRowConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalRowConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalRowConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, bool);

PyObject * CudaTemporalRowConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalRowConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalRowConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalRowConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, bool);

PyObject * CudaDoubleTemporalRowConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalRowConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalRowConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalRowConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, bool, float);

PyObject * CudaHalfTemporalRowConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalRowConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalRowConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, torch.cuda.HalfTensor gradBias, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kW, int dW, int padW, bool featFirst, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalRowConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, bool, float);

PyObject * CudaTemporalRowConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalRowConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalRowConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, torch.cuda.FloatTensor gradBias, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalRowConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, bool, double);

PyObject * CudaDoubleTemporalRowConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalRowConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalRowConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, torch.cuda.DoubleTensor gradBias, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalReflectionPadding_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfTemporalReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalReflectionPadding_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalReflectionPadding_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaTemporalReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalReflectionPadding_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalReflectionPadding_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleTemporalReflectionPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalReflectionPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalReflectionPadding_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalReflectionPadding_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfTemporalReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalReflectionPadding_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaTemporalReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalReflectionPadding_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleTemporalReflectionPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalReflectionPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalReflectionPadding_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalReplicationPadding_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfTemporalReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalReplicationPadding_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalReplicationPadding_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaTemporalReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalReplicationPadding_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalReplicationPadding_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleTemporalReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalReplicationPadding_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalReplicationPadding_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int);

PyObject * CudaHalfTemporalReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalReplicationPadding_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int);

PyObject * CudaTemporalReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalReplicationPadding_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int);

PyObject * CudaDoubleTemporalReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_padL = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_padR = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_padL, arg_padR);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int padL, int padR)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, bool);

PyObject * CudaHalfTemporalUpSamplingLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(arg_state, arg_input, arg_output, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalUpSamplingLinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalUpSamplingLinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, bool);

PyObject * CudaTemporalUpSamplingLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalUpSamplingLinear_updateOutput(arg_state, arg_input, arg_output, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalUpSamplingLinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalUpSamplingLinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, bool);

PyObject * CudaDoubleTemporalUpSamplingLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalUpSamplingLinear_updateOutput(arg_state, arg_input, arg_output, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalUpSamplingLinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, bool);

PyObject * CudaHalfTemporalUpSamplingLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputWidth, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalUpSamplingLinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int nbatch, int nchannels, int inputWidth, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalUpSamplingLinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, bool);

PyObject * CudaTemporalUpSamplingLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalUpSamplingLinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputWidth, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalUpSamplingLinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int nbatch, int nchannels, int inputWidth, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalUpSamplingLinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, bool);

PyObject * CudaDoubleTemporalUpSamplingLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalUpSamplingLinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputWidth, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalUpSamplingLinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int nbatch, int nchannels, int inputWidth, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfTemporalUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalUpSamplingNearest_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaTemporalUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalUpSamplingNearest_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleTemporalUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfTemporalUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfTemporalUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaTemporalUpSamplingNearest_updateOutput(void*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaTemporalUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaTemporalUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaTemporalUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleTemporalUpSamplingNearest_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleTemporalUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleTemporalUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleTemporalUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfThreshold_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, float, float, bool);

PyObject * CudaHalfThreshold_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfThreshold_updateOutput(arg_state, arg_input, arg_output, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfThreshold_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaThreshold_updateOutput(void*, THCudaTensor*, THCudaTensor*, float, float, bool);

PyObject * CudaThreshold_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      float arg_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaThreshold_updateOutput(arg_state, arg_input, arg_output, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaThreshold_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleThreshold_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool);

PyObject * CudaDoubleThreshold_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleThreshold_updateOutput(arg_state, arg_input, arg_output, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleThreshold_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfThreshold_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, float, float, bool);

PyObject * CudaHalfThreshold_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfThreshold_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfThreshold_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaThreshold_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, float, float, bool);

PyObject * CudaThreshold_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      float arg_threshold = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      float arg_val = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaThreshold_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaThreshold_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleThreshold_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, double, double, bool);

PyObject * CudaDoubleThreshold_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleThreshold_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_threshold, arg_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleThreshold_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, float threshold, float val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAveragePooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaHalfVolumetricAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAveragePooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAveragePooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaVolumetricAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAveragePooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAveragePooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaDoubleVolumetricAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 12) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAveragePooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaHalfVolumetricAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAveragePooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAveragePooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaVolumetricAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAveragePooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool);

PyObject * CudaDoubleVolumetricAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceil_mode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      bool arg_count_include_pad = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceil_mode, arg_count_include_pad);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAveragePooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceil_mode, bool count_include_pad)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaVolumetricConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor finput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaVolumetricConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor finput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor finput, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, float);

PyObject * CudaHalfVolumetricConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, float);

PyObject * CudaVolumetricConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, double);

PyObject * CudaDoubleVolumetricConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int dT, int dW, int dH, int padT, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricDilatedConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricDilatedConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor columns, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricDilatedConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor columns, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricDilatedConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_columns, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor columns, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfVolumetricDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_columns = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_ones = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradWeight, [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor columns, torch.cuda.HalfTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricDilatedConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaVolumetricDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_columns = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_ones = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradWeight, [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor columns, torch.cuda.FloatTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricDilatedConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleVolumetricDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_columns = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_ones = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_columns, arg_ones, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradWeight, [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor columns, torch.cuda.DoubleTensor ones, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullDilatedConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullDilatedConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricFullDilatedConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullDilatedConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullDilatedConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullDilatedConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullDilatedConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricFullDilatedConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 22 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullDilatedConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullDilatedConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfVolumetricFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 23 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 22))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 22));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, [torch.cuda.HalfTensor gradWeight or None], [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullDilatedConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaVolumetricFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 23 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 22))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 22));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, [torch.cuda.FloatTensor gradWeight or None], [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullDilatedConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleVolumetricFullDilatedConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 23 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 19)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 20)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 21)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 22))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 19));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 20));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 21));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 22));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullDilatedConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullDilatedConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, [torch.cuda.DoubleTensor gradWeight or None], [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfVolumetricDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 16) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricDilatedMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaVolumetricDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 16) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleVolumetricDilatedMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 16))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 16) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricDilatedMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricDilatedMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfVolumetricDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 17) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaVolumetricDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 17) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 18 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 17))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_dilationT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_dilationW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_dilationH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 17) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricDilatedMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_dilationT, arg_dilationW, arg_dilationH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricDilatedMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int dilationT, int dilationW, int dilationH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFractionalMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, THCudaLongTensor*, THCudaHalfTensor*);

PyObject * CudaHalfVolumetricFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaHalfTensor* arg_randomSamples = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.HalfTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFractionalMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, THCudaLongTensor*, THCudaTensor*);

PyObject * CudaVolumetricFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaTensor* arg_randomSamples = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.FloatTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFractionalMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, THCudaLongTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleVolumetricFractionalMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THCudaDoubleTensor* arg_randomSamples = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFractionalMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices, arg_randomSamples);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFractionalMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices, torch.cuda.DoubleTensor randomSamples)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFractionalMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, THCudaLongTensor*);

PyObject * CudaHalfVolumetricFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFractionalMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, THCudaLongTensor*);

PyObject * CudaVolumetricFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFractionalMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, THCudaLongTensor*);

PyObject * CudaDoubleVolumetricFractionalMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 10))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_poolSizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_poolSizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_poolSizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFractionalMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_outputT, arg_outputW, arg_outputH, arg_poolSizeT, arg_poolSizeW, arg_poolSizeH, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFractionalMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int outputT, int outputW, int outputH, int poolSizeT, int poolSizeW, int poolSizeH, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullConvolution_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullConvolution_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.HalfTensor weight, [torch.cuda.HalfTensor bias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullConvolution_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullConvolution_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.FloatTensor weight, [torch.cuda.FloatTensor bias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullConvolution_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricFullConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullConvolution_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.DoubleTensor weight, [torch.cuda.DoubleTensor bias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullConvolution_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaHalfTensor* arg_weight = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullConvolution_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.HalfTensor weight, torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullConvolution_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaTensor* arg_weight = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullConvolution_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.FloatTensor weight, torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullConvolution_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricFullConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 19 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaDoubleTensor* arg_weight = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullConvolution_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.DoubleTensor weight, torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricFullConvolution_accGradParameters(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaHalfVolumetricFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaHalfTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaHalfTensor* arg_finput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaHalfTensor* arg_fgradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricFullConvolution_accGradParameters", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, [torch.cuda.HalfTensor gradWeight or None], [torch.cuda.HalfTensor gradBias or None], torch.cuda.HalfTensor finput, torch.cuda.HalfTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricFullConvolution_accGradParameters(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, int, int, int, float);

PyObject * CudaVolumetricFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPFloatUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaTensor* arg_finput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaTensor* arg_fgradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      float arg_scale = THPFloatUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricFullConvolution_accGradParameters", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, [torch.cuda.FloatTensor gradWeight or None], [torch.cuda.FloatTensor gradBias or None], torch.cuda.FloatTensor finput, torch.cuda.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricFullConvolution_accGradParameters(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int, int, double);

PyObject * CudaDoubleVolumetricFullConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 20 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) || PyTuple_GET_ITEM(args, 3) == Py_None) &&
          (THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 16)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 17)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 18)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 19))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradWeight = (PyTuple_GET_ITEM(args, 3) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3)));
      THCudaDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THCudaDoubleTensor* arg_finput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THCudaDoubleTensor* arg_fgradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      int arg_adjT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 16));
      int arg_adjW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 17));
      int arg_adjH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 18));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 19));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricFullConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_adjT, arg_adjW, arg_adjH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricFullConvolution_accGradParameters", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, [torch.cuda.DoubleTensor gradWeight or None], [torch.cuda.DoubleTensor gradBias or None], torch.cuda.DoubleTensor finput, torch.cuda.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, int adjT, int adjW, int adjH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfVolumetricMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaVolumetricMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleVolumetricMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 13) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfVolumetricMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaVolumetricMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleVolumetricMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 15 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 14))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      bool arg_ceilMode = (PyTuple_GET_ITEM(args, 14) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH, arg_ceilMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int kT, int kW, int kH, int dT, int dW, int dH, int padT, int padW, int padH, bool ceilMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricMaxUnpooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricMaxUnpooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricMaxUnpooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricMaxUnpooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricMaxUnpooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaVolumetricMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricMaxUnpooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int arg_outputTime = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricMaxUnpooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices, arg_outputTime, arg_outputWidth, arg_outputHeight, arg_dT, arg_dW, arg_dH, arg_padT, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricMaxUnpooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices, int outputTime, int outputWidth, int outputHeight, int dT, int dW, int dH, int padT, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int);

PyObject * CudaHalfVolumetricAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, torch.cuda.LongTensor indices, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAdaptiveMaxPooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int);

PyObject * CudaVolumetricAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, torch.cuda.LongTensor indices, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int);

PyObject * CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput(arg_state, arg_input, arg_output, arg_indices, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, torch.cuda.LongTensor indices, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*);

PyObject * CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAdaptiveMaxPooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*);

PyObject * CudaVolumetricAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*);

PyObject * CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_CudaLongTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THCudaLongTensor* arg_indices = THNN_CudaLongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_indices);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, torch.cuda.LongTensor indices)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int);

PyObject * CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAdaptiveAveragePooling_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int);

PyObject * CudaVolumetricAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int);

PyObject * CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_osizeT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_osizeH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput(arg_state, arg_input, arg_output, arg_osizeT, arg_osizeW, arg_osizeH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int osizeT, int osizeW, int osizeH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*);

PyObject * CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricAdaptiveAveragePooling_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*);

PyObject * CudaVolumetricAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*);

PyObject * CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricReplicationPadding_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricReplicationPadding_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricReplicationPadding_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaVolumetricReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricReplicationPadding_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricReplicationPadding_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricReplicationPadding_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricReplicationPadding_updateOutput(arg_state, arg_input, arg_output, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricReplicationPadding_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricReplicationPadding_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int);

PyObject * CudaHalfVolumetricReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricReplicationPadding_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int);

PyObject * CudaVolumetricReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricReplicationPadding_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int);

PyObject * CudaDoubleVolumetricReplicationPadding_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_pleft = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_pright = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_ptop = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_pbottom = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_pfront = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_pback = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricReplicationPadding_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_pleft, arg_pright, arg_ptop, arg_pbottom, arg_pfront, arg_pback);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricReplicationPadding_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int pleft, int pright, int ptop, int pbottom, int pfront, int pback)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfVolumetricUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricUpSamplingNearest_updateGradInput(void*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaVolumetricUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricUpSamplingNearest_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleVolumetricUpSamplingNearest_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricUpSamplingNearest_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricUpSamplingNearest_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int);

PyObject * CudaHalfVolumetricUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricUpSamplingNearest_updateOutput(void*, THCudaTensor*, THCudaTensor*, int);

PyObject * CudaVolumetricUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricUpSamplingNearest_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int);

PyObject * CudaDoubleVolumetricUpSamplingNearest_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_scale_factor = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricUpSamplingNearest_updateOutput(arg_state, arg_input, arg_output, arg_scale_factor);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricUpSamplingNearest_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int scale_factor)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, bool);

PyObject * CudaHalfVolumetricUpSamplingTrilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_input = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_output = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricUpSamplingTrilinear_updateOutput", 1, "(int state, torch.cuda.HalfTensor input, torch.cuda.HalfTensor output, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricUpSamplingTrilinear_updateOutput(void*, THCudaTensor*, THCudaTensor*, int, int, int, bool);

PyObject * CudaVolumetricUpSamplingTrilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_input = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_output = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricUpSamplingTrilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricUpSamplingTrilinear_updateOutput", 1, "(int state, torch.cuda.FloatTensor input, torch.cuda.FloatTensor output, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, bool);

PyObject * CudaDoubleVolumetricUpSamplingTrilinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateOutput(arg_state, arg_input, arg_output, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricUpSamplingTrilinear_updateOutput", 1, "(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(void*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaHalfTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaHalfTensor* arg_gradOutput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaHalfTensor* arg_gradInput = THNN_CudaHalfTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputDepth, arg_inputHeight, arg_inputWidth, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaHalfVolumetricUpSamplingTrilinear_updateGradInput", 1, "(int state, torch.cuda.HalfTensor gradOutput, torch.cuda.HalfTensor gradInput, int nbatch, int nchannels, int inputDepth, int inputHeight, int inputWidth, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaVolumetricUpSamplingTrilinear_updateGradInput(void*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaVolumetricUpSamplingTrilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaFloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaTensor* arg_gradOutput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaTensor* arg_gradInput = THNN_CudaFloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaVolumetricUpSamplingTrilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputDepth, arg_inputHeight, arg_inputWidth, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaVolumetricUpSamplingTrilinear_updateGradInput", 1, "(int state, torch.cuda.FloatTensor gradOutput, torch.cuda.FloatTensor gradInput, int nbatch, int nchannels, int inputDepth, int inputHeight, int inputWidth, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, bool);

PyObject * CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 11))) {
      
      AutoGPU auto_gpu(get_device(args));
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THCudaDoubleTensor* arg_gradOutput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THCudaDoubleTensor* arg_gradInput = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_nbatch = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_nchannels = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_inputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_outputDepth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      bool arg_align_corners = (PyTuple_GET_ITEM(args, 11) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_nbatch, arg_nchannels, arg_inputDepth, arg_inputHeight, arg_inputWidth, arg_outputDepth, arg_outputHeight, arg_outputWidth, arg_align_corners);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput", 1, "(int state, torch.cuda.DoubleTensor gradOutput, torch.cuda.DoubleTensor gradInput, int nbatch, int nchannels, int inputDepth, int inputHeight, int inputWidth, int outputDepth, int outputHeight, int outputWidth, bool align_corners)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    



static PyMethodDef module_methods[] = {
  {"CudaHalfAbs_updateOutput", (PyCFunction)CudaHalfAbs_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaAbs_updateOutput", (PyCFunction)CudaAbs_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleAbs_updateOutput", (PyCFunction)CudaDoubleAbs_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfAbs_updateGradInput", (PyCFunction)CudaHalfAbs_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaAbs_updateGradInput", (PyCFunction)CudaAbs_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleAbs_updateGradInput", (PyCFunction)CudaDoubleAbs_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfAbsCriterion_updateOutput", (PyCFunction)CudaHalfAbsCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaAbsCriterion_updateOutput", (PyCFunction)CudaAbsCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleAbsCriterion_updateOutput", (PyCFunction)CudaDoubleAbsCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfAbsCriterion_updateGradInput", (PyCFunction)CudaHalfAbsCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaAbsCriterion_updateGradInput", (PyCFunction)CudaAbsCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleAbsCriterion_updateGradInput", (PyCFunction)CudaDoubleAbsCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfBatchNormalization_updateOutput", (PyCFunction)CudaHalfBatchNormalization_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaBatchNormalization_updateOutput", (PyCFunction)CudaBatchNormalization_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleBatchNormalization_updateOutput", (PyCFunction)CudaDoubleBatchNormalization_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfBatchNormalization_backward", (PyCFunction)CudaHalfBatchNormalization_backward, METH_STATIC | METH_VARARGS, NULL},
  {"CudaBatchNormalization_backward", (PyCFunction)CudaBatchNormalization_backward, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleBatchNormalization_backward", (PyCFunction)CudaDoubleBatchNormalization_backward, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfBCECriterion_updateOutput", (PyCFunction)CudaHalfBCECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaBCECriterion_updateOutput", (PyCFunction)CudaBCECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleBCECriterion_updateOutput", (PyCFunction)CudaDoubleBCECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfBCECriterion_updateGradInput", (PyCFunction)CudaHalfBCECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaBCECriterion_updateGradInput", (PyCFunction)CudaBCECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleBCECriterion_updateGradInput", (PyCFunction)CudaDoubleBCECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfClassNLLCriterion_updateOutput", (PyCFunction)CudaHalfClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaClassNLLCriterion_updateOutput", (PyCFunction)CudaClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleClassNLLCriterion_updateOutput", (PyCFunction)CudaDoubleClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfClassNLLCriterion_updateGradInput", (PyCFunction)CudaHalfClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaClassNLLCriterion_updateGradInput", (PyCFunction)CudaClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleClassNLLCriterion_updateGradInput", (PyCFunction)CudaDoubleClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfDistKLDivCriterion_updateOutput", (PyCFunction)CudaHalfDistKLDivCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDistKLDivCriterion_updateOutput", (PyCFunction)CudaDistKLDivCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleDistKLDivCriterion_updateOutput", (PyCFunction)CudaDoubleDistKLDivCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfDistKLDivCriterion_updateGradInput", (PyCFunction)CudaHalfDistKLDivCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDistKLDivCriterion_updateGradInput", (PyCFunction)CudaDistKLDivCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleDistKLDivCriterion_updateGradInput", (PyCFunction)CudaDoubleDistKLDivCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfELU_updateOutput", (PyCFunction)CudaHalfELU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaELU_updateOutput", (PyCFunction)CudaELU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleELU_updateOutput", (PyCFunction)CudaDoubleELU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfELU_updateGradInput", (PyCFunction)CudaHalfELU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaELU_updateGradInput", (PyCFunction)CudaELU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleELU_updateGradInput", (PyCFunction)CudaDoubleELU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfFeatureLPPooling_updateOutput", (PyCFunction)CudaHalfFeatureLPPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaFeatureLPPooling_updateOutput", (PyCFunction)CudaFeatureLPPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleFeatureLPPooling_updateOutput", (PyCFunction)CudaDoubleFeatureLPPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfFeatureLPPooling_updateGradInput", (PyCFunction)CudaHalfFeatureLPPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaFeatureLPPooling_updateGradInput", (PyCFunction)CudaFeatureLPPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleFeatureLPPooling_updateGradInput", (PyCFunction)CudaDoubleFeatureLPPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfHardTanh_updateOutput", (PyCFunction)CudaHalfHardTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHardTanh_updateOutput", (PyCFunction)CudaHardTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleHardTanh_updateOutput", (PyCFunction)CudaDoubleHardTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfHardTanh_updateGradInput", (PyCFunction)CudaHalfHardTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHardTanh_updateGradInput", (PyCFunction)CudaHardTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleHardTanh_updateGradInput", (PyCFunction)CudaDoubleHardTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfGatedLinear_updateOutput", (PyCFunction)CudaHalfGatedLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaGatedLinear_updateOutput", (PyCFunction)CudaGatedLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleGatedLinear_updateOutput", (PyCFunction)CudaDoubleGatedLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfGatedLinear_updateGradInput", (PyCFunction)CudaHalfGatedLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaGatedLinear_updateGradInput", (PyCFunction)CudaGatedLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleGatedLinear_updateGradInput", (PyCFunction)CudaDoubleGatedLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIm2Col_updateOutput", (PyCFunction)CudaHalfIm2Col_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIm2Col_updateOutput", (PyCFunction)CudaIm2Col_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIm2Col_updateOutput", (PyCFunction)CudaDoubleIm2Col_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIm2Col_updateGradInput", (PyCFunction)CudaHalfIm2Col_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIm2Col_updateGradInput", (PyCFunction)CudaIm2Col_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIm2Col_updateGradInput", (PyCFunction)CudaDoubleIm2Col_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfCol2Im_updateOutput", (PyCFunction)CudaHalfCol2Im_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaCol2Im_updateOutput", (PyCFunction)CudaCol2Im_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleCol2Im_updateOutput", (PyCFunction)CudaDoubleCol2Im_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfCol2Im_updateGradInput", (PyCFunction)CudaHalfCol2Im_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaCol2Im_updateGradInput", (PyCFunction)CudaCol2Im_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleCol2Im_updateGradInput", (PyCFunction)CudaDoubleCol2Im_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLeakyReLU_updateOutput", (PyCFunction)CudaHalfLeakyReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLeakyReLU_updateOutput", (PyCFunction)CudaLeakyReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLeakyReLU_updateOutput", (PyCFunction)CudaDoubleLeakyReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLeakyReLU_updateGradInput", (PyCFunction)CudaHalfLeakyReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLeakyReLU_updateGradInput", (PyCFunction)CudaLeakyReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLeakyReLU_updateGradInput", (PyCFunction)CudaDoubleLeakyReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfGRUFused_updateOutput", (PyCFunction)CudaHalfGRUFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaGRUFused_updateOutput", (PyCFunction)CudaGRUFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleGRUFused_updateOutput", (PyCFunction)CudaDoubleGRUFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfGRUFused_updateGradInput", (PyCFunction)CudaHalfGRUFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaGRUFused_updateGradInput", (PyCFunction)CudaGRUFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleGRUFused_updateGradInput", (PyCFunction)CudaDoubleGRUFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLSTMFused_updateOutput", (PyCFunction)CudaHalfLSTMFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLSTMFused_updateOutput", (PyCFunction)CudaLSTMFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLSTMFused_updateOutput", (PyCFunction)CudaDoubleLSTMFused_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLSTMFused_updateGradInput", (PyCFunction)CudaHalfLSTMFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLSTMFused_updateGradInput", (PyCFunction)CudaLSTMFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLSTMFused_updateGradInput", (PyCFunction)CudaDoubleLSTMFused_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLogSigmoid_updateOutput", (PyCFunction)CudaHalfLogSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLogSigmoid_updateOutput", (PyCFunction)CudaLogSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLogSigmoid_updateOutput", (PyCFunction)CudaDoubleLogSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLogSigmoid_updateGradInput", (PyCFunction)CudaHalfLogSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLogSigmoid_updateGradInput", (PyCFunction)CudaLogSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLogSigmoid_updateGradInput", (PyCFunction)CudaDoubleLogSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLookupTable_accGradParameters", (PyCFunction)CudaHalfLookupTable_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLookupTable_accGradParameters", (PyCFunction)CudaLookupTable_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLookupTable_accGradParameters", (PyCFunction)CudaDoubleLookupTable_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLookupTable_renorm", (PyCFunction)CudaHalfLookupTable_renorm, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLookupTable_renorm", (PyCFunction)CudaLookupTable_renorm, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLookupTable_renorm", (PyCFunction)CudaDoubleLookupTable_renorm, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLookupTableBag_updateOutput", (PyCFunction)CudaHalfLookupTableBag_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLookupTableBag_updateOutput", (PyCFunction)CudaLookupTableBag_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLookupTableBag_updateOutput", (PyCFunction)CudaDoubleLookupTableBag_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfLookupTableBag_accGradParameters", (PyCFunction)CudaHalfLookupTableBag_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaLookupTableBag_accGradParameters", (PyCFunction)CudaLookupTableBag_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleLookupTableBag_accGradParameters", (PyCFunction)CudaDoubleLookupTableBag_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfL1Cost_updateOutput", (PyCFunction)CudaHalfL1Cost_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaL1Cost_updateOutput", (PyCFunction)CudaL1Cost_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleL1Cost_updateOutput", (PyCFunction)CudaDoubleL1Cost_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfL1Cost_updateGradInput", (PyCFunction)CudaHalfL1Cost_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaL1Cost_updateGradInput", (PyCFunction)CudaL1Cost_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleL1Cost_updateGradInput", (PyCFunction)CudaDoubleL1Cost_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMarginCriterion_updateOutput", (PyCFunction)CudaHalfMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMarginCriterion_updateOutput", (PyCFunction)CudaMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMarginCriterion_updateOutput", (PyCFunction)CudaDoubleMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMarginCriterion_updateGradInput", (PyCFunction)CudaHalfMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMarginCriterion_updateGradInput", (PyCFunction)CudaMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMarginCriterion_updateGradInput", (PyCFunction)CudaDoubleMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMSECriterion_updateOutput", (PyCFunction)CudaHalfMSECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMSECriterion_updateOutput", (PyCFunction)CudaMSECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMSECriterion_updateOutput", (PyCFunction)CudaDoubleMSECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMSECriterion_updateGradInput", (PyCFunction)CudaHalfMSECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMSECriterion_updateGradInput", (PyCFunction)CudaMSECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMSECriterion_updateGradInput", (PyCFunction)CudaDoubleMSECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMultiLabelMarginCriterion_updateOutput", (PyCFunction)CudaHalfMultiLabelMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMultiLabelMarginCriterion_updateOutput", (PyCFunction)CudaMultiLabelMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMultiLabelMarginCriterion_updateOutput", (PyCFunction)CudaDoubleMultiLabelMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMultiLabelMarginCriterion_updateGradInput", (PyCFunction)CudaHalfMultiLabelMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMultiLabelMarginCriterion_updateGradInput", (PyCFunction)CudaMultiLabelMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMultiLabelMarginCriterion_updateGradInput", (PyCFunction)CudaDoubleMultiLabelMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMultiMarginCriterion_updateOutput", (PyCFunction)CudaHalfMultiMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMultiMarginCriterion_updateOutput", (PyCFunction)CudaMultiMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMultiMarginCriterion_updateOutput", (PyCFunction)CudaDoubleMultiMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfMultiMarginCriterion_updateGradInput", (PyCFunction)CudaHalfMultiMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaMultiMarginCriterion_updateGradInput", (PyCFunction)CudaMultiMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleMultiMarginCriterion_updateGradInput", (PyCFunction)CudaDoubleMultiMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfPReLU_updateOutput", (PyCFunction)CudaHalfPReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaPReLU_updateOutput", (PyCFunction)CudaPReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoublePReLU_updateOutput", (PyCFunction)CudaDoublePReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfPReLU_updateGradInput", (PyCFunction)CudaHalfPReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaPReLU_updateGradInput", (PyCFunction)CudaPReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoublePReLU_updateGradInput", (PyCFunction)CudaDoublePReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfPReLU_accGradParameters", (PyCFunction)CudaHalfPReLU_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaPReLU_accGradParameters", (PyCFunction)CudaPReLU_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoublePReLU_accGradParameters", (PyCFunction)CudaDoublePReLU_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSmoothL1Criterion_updateOutput", (PyCFunction)CudaHalfSmoothL1Criterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSmoothL1Criterion_updateOutput", (PyCFunction)CudaSmoothL1Criterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSmoothL1Criterion_updateOutput", (PyCFunction)CudaDoubleSmoothL1Criterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSmoothL1Criterion_updateGradInput", (PyCFunction)CudaHalfSmoothL1Criterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSmoothL1Criterion_updateGradInput", (PyCFunction)CudaSmoothL1Criterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSmoothL1Criterion_updateGradInput", (PyCFunction)CudaDoubleSmoothL1Criterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_updateOutput", (PyCFunction)CudaHalfSparseLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_updateOutput", (PyCFunction)CudaSparseLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_updateOutput", (PyCFunction)CudaDoubleSparseLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_accGradParameters", (PyCFunction)CudaHalfSparseLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_accGradParameters", (PyCFunction)CudaSparseLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_accGradParameters", (PyCFunction)CudaDoubleSparseLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_legacyUpdateOutput", (PyCFunction)CudaHalfSparseLinear_legacyUpdateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_legacyUpdateOutput", (PyCFunction)CudaSparseLinear_legacyUpdateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_legacyUpdateOutput", (PyCFunction)CudaDoubleSparseLinear_legacyUpdateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_legacyAccGradParameters", (PyCFunction)CudaHalfSparseLinear_legacyAccGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_legacyAccGradParameters", (PyCFunction)CudaSparseLinear_legacyAccGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_legacyAccGradParameters", (PyCFunction)CudaDoubleSparseLinear_legacyAccGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_zeroGradParameters", (PyCFunction)CudaHalfSparseLinear_zeroGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_zeroGradParameters", (PyCFunction)CudaSparseLinear_zeroGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_zeroGradParameters", (PyCFunction)CudaDoubleSparseLinear_zeroGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSparseLinear_updateParameters", (PyCFunction)CudaHalfSparseLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSparseLinear_updateParameters", (PyCFunction)CudaSparseLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSparseLinear_updateParameters", (PyCFunction)CudaDoubleSparseLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIndexLinear_updateOutput", (PyCFunction)CudaHalfIndexLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIndexLinear_updateOutput", (PyCFunction)CudaIndexLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIndexLinear_updateOutput", (PyCFunction)CudaDoubleIndexLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIndexLinear_accGradParameters", (PyCFunction)CudaHalfIndexLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIndexLinear_accGradParameters", (PyCFunction)CudaIndexLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIndexLinear_accGradParameters", (PyCFunction)CudaDoubleIndexLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIndexLinear_accUpdateGradParameters", (PyCFunction)CudaHalfIndexLinear_accUpdateGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIndexLinear_accUpdateGradParameters", (PyCFunction)CudaIndexLinear_accUpdateGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIndexLinear_accUpdateGradParameters", (PyCFunction)CudaDoubleIndexLinear_accUpdateGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfIndexLinear_updateParameters", (PyCFunction)CudaHalfIndexLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaIndexLinear_updateParameters", (PyCFunction)CudaIndexLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleIndexLinear_updateParameters", (PyCFunction)CudaDoubleIndexLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaHalfSpatialAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaSpatialAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaDoubleSpatialAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaHalfSpatialAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaSpatialAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaDoubleSpatialAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaHalfSpatialAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaSpatialAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaDoubleSpatialAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaHalfSpatialAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaSpatialAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaDoubleSpatialAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAveragePooling_updateOutput", (PyCFunction)CudaHalfSpatialAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAveragePooling_updateOutput", (PyCFunction)CudaSpatialAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAveragePooling_updateOutput", (PyCFunction)CudaDoubleSpatialAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialAveragePooling_updateGradInput", (PyCFunction)CudaHalfSpatialAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialAveragePooling_updateGradInput", (PyCFunction)CudaSpatialAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialAveragePooling_updateGradInput", (PyCFunction)CudaDoubleSpatialAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialClassNLLCriterion_updateOutput", (PyCFunction)CudaHalfSpatialClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialClassNLLCriterion_updateOutput", (PyCFunction)CudaSpatialClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialClassNLLCriterion_updateOutput", (PyCFunction)CudaDoubleSpatialClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialClassNLLCriterion_updateGradInput", (PyCFunction)CudaHalfSpatialClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialClassNLLCriterion_updateGradInput", (PyCFunction)CudaSpatialClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialClassNLLCriterion_updateGradInput", (PyCFunction)CudaDoubleSpatialClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionLocal_updateOutput", (PyCFunction)CudaHalfSpatialConvolutionLocal_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionLocal_updateOutput", (PyCFunction)CudaSpatialConvolutionLocal_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionLocal_updateOutput", (PyCFunction)CudaDoubleSpatialConvolutionLocal_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionLocal_updateGradInput", (PyCFunction)CudaHalfSpatialConvolutionLocal_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionLocal_updateGradInput", (PyCFunction)CudaSpatialConvolutionLocal_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionLocal_updateGradInput", (PyCFunction)CudaDoubleSpatialConvolutionLocal_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionLocal_accGradParameters", (PyCFunction)CudaHalfSpatialConvolutionLocal_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionLocal_accGradParameters", (PyCFunction)CudaSpatialConvolutionLocal_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionLocal_accGradParameters", (PyCFunction)CudaDoubleSpatialConvolutionLocal_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionMM_updateOutput", (PyCFunction)CudaHalfSpatialConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionMM_updateOutput", (PyCFunction)CudaSpatialConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionMM_updateOutput", (PyCFunction)CudaDoubleSpatialConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionMM_updateGradInput", (PyCFunction)CudaHalfSpatialConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionMM_updateGradInput", (PyCFunction)CudaSpatialConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionMM_updateGradInput", (PyCFunction)CudaDoubleSpatialConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialConvolutionMM_accGradParameters", (PyCFunction)CudaHalfSpatialConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialConvolutionMM_accGradParameters", (PyCFunction)CudaSpatialConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialConvolutionMM_accGradParameters", (PyCFunction)CudaDoubleSpatialConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDepthwiseConvolution_updateOutput", (PyCFunction)CudaHalfSpatialDepthwiseConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDepthwiseConvolution_updateOutput", (PyCFunction)CudaSpatialDepthwiseConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDepthwiseConvolution_updateOutput", (PyCFunction)CudaDoubleSpatialDepthwiseConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDepthwiseConvolution_updateGradInput", (PyCFunction)CudaHalfSpatialDepthwiseConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDepthwiseConvolution_updateGradInput", (PyCFunction)CudaSpatialDepthwiseConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDepthwiseConvolution_updateGradInput", (PyCFunction)CudaDoubleSpatialDepthwiseConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDepthwiseConvolution_accGradParameters", (PyCFunction)CudaHalfSpatialDepthwiseConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDepthwiseConvolution_accGradParameters", (PyCFunction)CudaSpatialDepthwiseConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDepthwiseConvolution_accGradParameters", (PyCFunction)CudaDoubleSpatialDepthwiseConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialCrossMapLRN_updateOutput", (PyCFunction)CudaHalfSpatialCrossMapLRN_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialCrossMapLRN_updateOutput", (PyCFunction)CudaSpatialCrossMapLRN_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialCrossMapLRN_updateOutput", (PyCFunction)CudaDoubleSpatialCrossMapLRN_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialCrossMapLRN_updateGradInput", (PyCFunction)CudaHalfSpatialCrossMapLRN_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialCrossMapLRN_updateGradInput", (PyCFunction)CudaSpatialCrossMapLRN_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialCrossMapLRN_updateGradInput", (PyCFunction)CudaDoubleSpatialCrossMapLRN_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDilatedConvolution_updateOutput", (PyCFunction)CudaHalfSpatialDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDilatedConvolution_updateOutput", (PyCFunction)CudaSpatialDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDilatedConvolution_updateOutput", (PyCFunction)CudaDoubleSpatialDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDilatedConvolution_updateGradInput", (PyCFunction)CudaHalfSpatialDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDilatedConvolution_updateGradInput", (PyCFunction)CudaSpatialDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDilatedConvolution_updateGradInput", (PyCFunction)CudaDoubleSpatialDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDilatedConvolution_accGradParameters", (PyCFunction)CudaHalfSpatialDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDilatedConvolution_accGradParameters", (PyCFunction)CudaSpatialDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDilatedConvolution_accGradParameters", (PyCFunction)CudaDoubleSpatialDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullDilatedConvolution_updateOutput", (PyCFunction)CudaHalfSpatialFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullDilatedConvolution_updateOutput", (PyCFunction)CudaSpatialFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullDilatedConvolution_updateOutput", (PyCFunction)CudaDoubleSpatialFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullDilatedConvolution_updateGradInput", (PyCFunction)CudaHalfSpatialFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullDilatedConvolution_updateGradInput", (PyCFunction)CudaSpatialFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullDilatedConvolution_updateGradInput", (PyCFunction)CudaDoubleSpatialFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullDilatedConvolution_accGradParameters", (PyCFunction)CudaHalfSpatialFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullDilatedConvolution_accGradParameters", (PyCFunction)CudaSpatialFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullDilatedConvolution_accGradParameters", (PyCFunction)CudaDoubleSpatialFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDilatedMaxPooling_updateOutput", (PyCFunction)CudaHalfSpatialDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDilatedMaxPooling_updateOutput", (PyCFunction)CudaSpatialDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDilatedMaxPooling_updateOutput", (PyCFunction)CudaDoubleSpatialDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialDilatedMaxPooling_updateGradInput", (PyCFunction)CudaHalfSpatialDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialDilatedMaxPooling_updateGradInput", (PyCFunction)CudaSpatialDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialDilatedMaxPooling_updateGradInput", (PyCFunction)CudaDoubleSpatialDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFractionalMaxPooling_updateOutput", (PyCFunction)CudaHalfSpatialFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFractionalMaxPooling_updateOutput", (PyCFunction)CudaSpatialFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFractionalMaxPooling_updateOutput", (PyCFunction)CudaDoubleSpatialFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFractionalMaxPooling_updateGradInput", (PyCFunction)CudaHalfSpatialFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFractionalMaxPooling_updateGradInput", (PyCFunction)CudaSpatialFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFractionalMaxPooling_updateGradInput", (PyCFunction)CudaDoubleSpatialFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullConvolution_updateOutput", (PyCFunction)CudaHalfSpatialFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullConvolution_updateOutput", (PyCFunction)CudaSpatialFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullConvolution_updateOutput", (PyCFunction)CudaDoubleSpatialFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullConvolution_updateGradInput", (PyCFunction)CudaHalfSpatialFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullConvolution_updateGradInput", (PyCFunction)CudaSpatialFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullConvolution_updateGradInput", (PyCFunction)CudaDoubleSpatialFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialFullConvolution_accGradParameters", (PyCFunction)CudaHalfSpatialFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialFullConvolution_accGradParameters", (PyCFunction)CudaSpatialFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialFullConvolution_accGradParameters", (PyCFunction)CudaDoubleSpatialFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialMaxPooling_updateOutput", (PyCFunction)CudaHalfSpatialMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialMaxPooling_updateOutput", (PyCFunction)CudaSpatialMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialMaxPooling_updateOutput", (PyCFunction)CudaDoubleSpatialMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialMaxPooling_updateGradInput", (PyCFunction)CudaHalfSpatialMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialMaxPooling_updateGradInput", (PyCFunction)CudaSpatialMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialMaxPooling_updateGradInput", (PyCFunction)CudaDoubleSpatialMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialMaxUnpooling_updateOutput", (PyCFunction)CudaHalfSpatialMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialMaxUnpooling_updateOutput", (PyCFunction)CudaSpatialMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialMaxUnpooling_updateOutput", (PyCFunction)CudaDoubleSpatialMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialMaxUnpooling_updateGradInput", (PyCFunction)CudaHalfSpatialMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialMaxUnpooling_updateGradInput", (PyCFunction)CudaSpatialMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialMaxUnpooling_updateGradInput", (PyCFunction)CudaDoubleSpatialMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialReflectionPadding_updateOutput", (PyCFunction)CudaHalfSpatialReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialReflectionPadding_updateOutput", (PyCFunction)CudaSpatialReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialReflectionPadding_updateOutput", (PyCFunction)CudaDoubleSpatialReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialReflectionPadding_updateGradInput", (PyCFunction)CudaHalfSpatialReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialReflectionPadding_updateGradInput", (PyCFunction)CudaSpatialReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialReflectionPadding_updateGradInput", (PyCFunction)CudaDoubleSpatialReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialReplicationPadding_updateOutput", (PyCFunction)CudaHalfSpatialReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialReplicationPadding_updateOutput", (PyCFunction)CudaSpatialReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialReplicationPadding_updateOutput", (PyCFunction)CudaDoubleSpatialReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialReplicationPadding_updateGradInput", (PyCFunction)CudaHalfSpatialReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialReplicationPadding_updateGradInput", (PyCFunction)CudaSpatialReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialReplicationPadding_updateGradInput", (PyCFunction)CudaDoubleSpatialReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialSubSampling_updateOutput", (PyCFunction)CudaHalfSpatialSubSampling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialSubSampling_updateOutput", (PyCFunction)CudaSpatialSubSampling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialSubSampling_updateOutput", (PyCFunction)CudaDoubleSpatialSubSampling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialSubSampling_updateGradInput", (PyCFunction)CudaHalfSpatialSubSampling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialSubSampling_updateGradInput", (PyCFunction)CudaSpatialSubSampling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialSubSampling_updateGradInput", (PyCFunction)CudaDoubleSpatialSubSampling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialSubSampling_accGradParameters", (PyCFunction)CudaHalfSpatialSubSampling_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialSubSampling_accGradParameters", (PyCFunction)CudaSpatialSubSampling_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialSubSampling_accGradParameters", (PyCFunction)CudaDoubleSpatialSubSampling_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialUpSamplingBilinear_updateOutput", (PyCFunction)CudaHalfSpatialUpSamplingBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialUpSamplingBilinear_updateOutput", (PyCFunction)CudaSpatialUpSamplingBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialUpSamplingBilinear_updateOutput", (PyCFunction)CudaDoubleSpatialUpSamplingBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialUpSamplingBilinear_updateGradInput", (PyCFunction)CudaHalfSpatialUpSamplingBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialUpSamplingBilinear_updateGradInput", (PyCFunction)CudaSpatialUpSamplingBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialUpSamplingBilinear_updateGradInput", (PyCFunction)CudaDoubleSpatialUpSamplingBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialUpSamplingNearest_updateGradInput", (PyCFunction)CudaHalfSpatialUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialUpSamplingNearest_updateGradInput", (PyCFunction)CudaSpatialUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialUpSamplingNearest_updateGradInput", (PyCFunction)CudaDoubleSpatialUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialUpSamplingNearest_updateOutput", (PyCFunction)CudaHalfSpatialUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialUpSamplingNearest_updateOutput", (PyCFunction)CudaSpatialUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialUpSamplingNearest_updateOutput", (PyCFunction)CudaDoubleSpatialUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialGridSamplerBilinear_updateOutput", (PyCFunction)CudaHalfSpatialGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialGridSamplerBilinear_updateOutput", (PyCFunction)CudaSpatialGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialGridSamplerBilinear_updateOutput", (PyCFunction)CudaDoubleSpatialGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSpatialGridSamplerBilinear_updateGradInput", (PyCFunction)CudaHalfSpatialGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSpatialGridSamplerBilinear_updateGradInput", (PyCFunction)CudaSpatialGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSpatialGridSamplerBilinear_updateGradInput", (PyCFunction)CudaDoubleSpatialGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricGridSamplerBilinear_updateOutput", (PyCFunction)CudaHalfVolumetricGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricGridSamplerBilinear_updateOutput", (PyCFunction)CudaVolumetricGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricGridSamplerBilinear_updateOutput", (PyCFunction)CudaDoubleVolumetricGridSamplerBilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricGridSamplerBilinear_updateGradInput", (PyCFunction)CudaHalfVolumetricGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricGridSamplerBilinear_updateGradInput", (PyCFunction)CudaVolumetricGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricGridSamplerBilinear_updateGradInput", (PyCFunction)CudaDoubleVolumetricGridSamplerBilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfRReLU_updateOutput", (PyCFunction)CudaHalfRReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaRReLU_updateOutput", (PyCFunction)CudaRReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleRReLU_updateOutput", (PyCFunction)CudaDoubleRReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfRReLU_updateGradInput", (PyCFunction)CudaHalfRReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaRReLU_updateGradInput", (PyCFunction)CudaRReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleRReLU_updateGradInput", (PyCFunction)CudaDoubleRReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSigmoid_updateOutput", (PyCFunction)CudaHalfSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSigmoid_updateOutput", (PyCFunction)CudaSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSigmoid_updateOutput", (PyCFunction)CudaDoubleSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSigmoid_updateGradInput", (PyCFunction)CudaHalfSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSigmoid_updateGradInput", (PyCFunction)CudaSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSigmoid_updateGradInput", (PyCFunction)CudaDoubleSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftMarginCriterion_updateOutput", (PyCFunction)CudaHalfSoftMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftMarginCriterion_updateOutput", (PyCFunction)CudaSoftMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftMarginCriterion_updateOutput", (PyCFunction)CudaDoubleSoftMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftMarginCriterion_updateGradInput", (PyCFunction)CudaHalfSoftMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftMarginCriterion_updateGradInput", (PyCFunction)CudaSoftMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftMarginCriterion_updateGradInput", (PyCFunction)CudaDoubleSoftMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftPlus_updateOutput", (PyCFunction)CudaHalfSoftPlus_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftPlus_updateOutput", (PyCFunction)CudaSoftPlus_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftPlus_updateOutput", (PyCFunction)CudaDoubleSoftPlus_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftPlus_updateGradInput", (PyCFunction)CudaHalfSoftPlus_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftPlus_updateGradInput", (PyCFunction)CudaSoftPlus_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftPlus_updateGradInput", (PyCFunction)CudaDoubleSoftPlus_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftShrink_updateOutput", (PyCFunction)CudaHalfSoftShrink_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftShrink_updateOutput", (PyCFunction)CudaSoftShrink_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftShrink_updateOutput", (PyCFunction)CudaDoubleSoftShrink_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSoftShrink_updateGradInput", (PyCFunction)CudaHalfSoftShrink_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSoftShrink_updateGradInput", (PyCFunction)CudaSoftShrink_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSoftShrink_updateGradInput", (PyCFunction)CudaDoubleSoftShrink_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSquare_updateOutput", (PyCFunction)CudaHalfSquare_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSquare_updateOutput", (PyCFunction)CudaSquare_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSquare_updateOutput", (PyCFunction)CudaDoubleSquare_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSquare_updateGradInput", (PyCFunction)CudaHalfSquare_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSquare_updateGradInput", (PyCFunction)CudaSquare_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSquare_updateGradInput", (PyCFunction)CudaDoubleSquare_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSqrt_updateOutput", (PyCFunction)CudaHalfSqrt_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSqrt_updateOutput", (PyCFunction)CudaSqrt_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSqrt_updateOutput", (PyCFunction)CudaDoubleSqrt_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfSqrt_updateGradInput", (PyCFunction)CudaHalfSqrt_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaSqrt_updateGradInput", (PyCFunction)CudaSqrt_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleSqrt_updateGradInput", (PyCFunction)CudaDoubleSqrt_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTanh_updateOutput", (PyCFunction)CudaHalfTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTanh_updateOutput", (PyCFunction)CudaTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTanh_updateOutput", (PyCFunction)CudaDoubleTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTanh_updateGradInput", (PyCFunction)CudaHalfTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTanh_updateGradInput", (PyCFunction)CudaTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTanh_updateGradInput", (PyCFunction)CudaDoubleTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalConvolution_updateOutput", (PyCFunction)CudaHalfTemporalConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalConvolution_updateOutput", (PyCFunction)CudaTemporalConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalConvolution_updateOutput", (PyCFunction)CudaDoubleTemporalConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalConvolution_updateGradInput", (PyCFunction)CudaHalfTemporalConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalConvolution_updateGradInput", (PyCFunction)CudaTemporalConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalConvolution_updateGradInput", (PyCFunction)CudaDoubleTemporalConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalConvolution_accGradParameters", (PyCFunction)CudaHalfTemporalConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalConvolution_accGradParameters", (PyCFunction)CudaTemporalConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalConvolution_accGradParameters", (PyCFunction)CudaDoubleTemporalConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalMaxPooling_updateOutput", (PyCFunction)CudaHalfTemporalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalMaxPooling_updateOutput", (PyCFunction)CudaTemporalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalMaxPooling_updateOutput", (PyCFunction)CudaDoubleTemporalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalMaxPooling_updateGradInput", (PyCFunction)CudaHalfTemporalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalMaxPooling_updateGradInput", (PyCFunction)CudaTemporalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalMaxPooling_updateGradInput", (PyCFunction)CudaDoubleTemporalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalRowConvolution_updateOutput", (PyCFunction)CudaHalfTemporalRowConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalRowConvolution_updateOutput", (PyCFunction)CudaTemporalRowConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalRowConvolution_updateOutput", (PyCFunction)CudaDoubleTemporalRowConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalRowConvolution_updateGradInput", (PyCFunction)CudaHalfTemporalRowConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalRowConvolution_updateGradInput", (PyCFunction)CudaTemporalRowConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalRowConvolution_updateGradInput", (PyCFunction)CudaDoubleTemporalRowConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalRowConvolution_accGradParameters", (PyCFunction)CudaHalfTemporalRowConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalRowConvolution_accGradParameters", (PyCFunction)CudaTemporalRowConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalRowConvolution_accGradParameters", (PyCFunction)CudaDoubleTemporalRowConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalReflectionPadding_updateOutput", (PyCFunction)CudaHalfTemporalReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalReflectionPadding_updateOutput", (PyCFunction)CudaTemporalReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalReflectionPadding_updateOutput", (PyCFunction)CudaDoubleTemporalReflectionPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalReflectionPadding_updateGradInput", (PyCFunction)CudaHalfTemporalReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalReflectionPadding_updateGradInput", (PyCFunction)CudaTemporalReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalReflectionPadding_updateGradInput", (PyCFunction)CudaDoubleTemporalReflectionPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalReplicationPadding_updateOutput", (PyCFunction)CudaHalfTemporalReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalReplicationPadding_updateOutput", (PyCFunction)CudaTemporalReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalReplicationPadding_updateOutput", (PyCFunction)CudaDoubleTemporalReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalReplicationPadding_updateGradInput", (PyCFunction)CudaHalfTemporalReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalReplicationPadding_updateGradInput", (PyCFunction)CudaTemporalReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalReplicationPadding_updateGradInput", (PyCFunction)CudaDoubleTemporalReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalUpSamplingLinear_updateOutput", (PyCFunction)CudaHalfTemporalUpSamplingLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalUpSamplingLinear_updateOutput", (PyCFunction)CudaTemporalUpSamplingLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalUpSamplingLinear_updateOutput", (PyCFunction)CudaDoubleTemporalUpSamplingLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalUpSamplingLinear_updateGradInput", (PyCFunction)CudaHalfTemporalUpSamplingLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalUpSamplingLinear_updateGradInput", (PyCFunction)CudaTemporalUpSamplingLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalUpSamplingLinear_updateGradInput", (PyCFunction)CudaDoubleTemporalUpSamplingLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalUpSamplingNearest_updateGradInput", (PyCFunction)CudaHalfTemporalUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalUpSamplingNearest_updateGradInput", (PyCFunction)CudaTemporalUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalUpSamplingNearest_updateGradInput", (PyCFunction)CudaDoubleTemporalUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfTemporalUpSamplingNearest_updateOutput", (PyCFunction)CudaHalfTemporalUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaTemporalUpSamplingNearest_updateOutput", (PyCFunction)CudaTemporalUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleTemporalUpSamplingNearest_updateOutput", (PyCFunction)CudaDoubleTemporalUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfThreshold_updateOutput", (PyCFunction)CudaHalfThreshold_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaThreshold_updateOutput", (PyCFunction)CudaThreshold_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleThreshold_updateOutput", (PyCFunction)CudaDoubleThreshold_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfThreshold_updateGradInput", (PyCFunction)CudaHalfThreshold_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaThreshold_updateGradInput", (PyCFunction)CudaThreshold_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleThreshold_updateGradInput", (PyCFunction)CudaDoubleThreshold_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAveragePooling_updateOutput", (PyCFunction)CudaHalfVolumetricAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAveragePooling_updateOutput", (PyCFunction)CudaVolumetricAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAveragePooling_updateOutput", (PyCFunction)CudaDoubleVolumetricAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAveragePooling_updateGradInput", (PyCFunction)CudaHalfVolumetricAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAveragePooling_updateGradInput", (PyCFunction)CudaVolumetricAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAveragePooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricConvolution_updateOutput", (PyCFunction)CudaHalfVolumetricConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricConvolution_updateOutput", (PyCFunction)CudaVolumetricConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricConvolution_updateOutput", (PyCFunction)CudaDoubleVolumetricConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricConvolution_updateGradInput", (PyCFunction)CudaHalfVolumetricConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricConvolution_updateGradInput", (PyCFunction)CudaVolumetricConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricConvolution_updateGradInput", (PyCFunction)CudaDoubleVolumetricConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricConvolution_accGradParameters", (PyCFunction)CudaHalfVolumetricConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricConvolution_accGradParameters", (PyCFunction)CudaVolumetricConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricConvolution_accGradParameters", (PyCFunction)CudaDoubleVolumetricConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricDilatedConvolution_updateOutput", (PyCFunction)CudaHalfVolumetricDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricDilatedConvolution_updateOutput", (PyCFunction)CudaVolumetricDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricDilatedConvolution_updateOutput", (PyCFunction)CudaDoubleVolumetricDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricDilatedConvolution_updateGradInput", (PyCFunction)CudaHalfVolumetricDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricDilatedConvolution_updateGradInput", (PyCFunction)CudaVolumetricDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricDilatedConvolution_updateGradInput", (PyCFunction)CudaDoubleVolumetricDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricDilatedConvolution_accGradParameters", (PyCFunction)CudaHalfVolumetricDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricDilatedConvolution_accGradParameters", (PyCFunction)CudaVolumetricDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricDilatedConvolution_accGradParameters", (PyCFunction)CudaDoubleVolumetricDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullDilatedConvolution_updateOutput", (PyCFunction)CudaHalfVolumetricFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullDilatedConvolution_updateOutput", (PyCFunction)CudaVolumetricFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullDilatedConvolution_updateOutput", (PyCFunction)CudaDoubleVolumetricFullDilatedConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullDilatedConvolution_updateGradInput", (PyCFunction)CudaHalfVolumetricFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullDilatedConvolution_updateGradInput", (PyCFunction)CudaVolumetricFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullDilatedConvolution_updateGradInput", (PyCFunction)CudaDoubleVolumetricFullDilatedConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullDilatedConvolution_accGradParameters", (PyCFunction)CudaHalfVolumetricFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullDilatedConvolution_accGradParameters", (PyCFunction)CudaVolumetricFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullDilatedConvolution_accGradParameters", (PyCFunction)CudaDoubleVolumetricFullDilatedConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricDilatedMaxPooling_updateOutput", (PyCFunction)CudaHalfVolumetricDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricDilatedMaxPooling_updateOutput", (PyCFunction)CudaVolumetricDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricDilatedMaxPooling_updateOutput", (PyCFunction)CudaDoubleVolumetricDilatedMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricDilatedMaxPooling_updateGradInput", (PyCFunction)CudaHalfVolumetricDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricDilatedMaxPooling_updateGradInput", (PyCFunction)CudaVolumetricDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricDilatedMaxPooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricDilatedMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFractionalMaxPooling_updateOutput", (PyCFunction)CudaHalfVolumetricFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFractionalMaxPooling_updateOutput", (PyCFunction)CudaVolumetricFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFractionalMaxPooling_updateOutput", (PyCFunction)CudaDoubleVolumetricFractionalMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFractionalMaxPooling_updateGradInput", (PyCFunction)CudaHalfVolumetricFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFractionalMaxPooling_updateGradInput", (PyCFunction)CudaVolumetricFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFractionalMaxPooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricFractionalMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullConvolution_updateOutput", (PyCFunction)CudaHalfVolumetricFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullConvolution_updateOutput", (PyCFunction)CudaVolumetricFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullConvolution_updateOutput", (PyCFunction)CudaDoubleVolumetricFullConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullConvolution_updateGradInput", (PyCFunction)CudaHalfVolumetricFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullConvolution_updateGradInput", (PyCFunction)CudaVolumetricFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullConvolution_updateGradInput", (PyCFunction)CudaDoubleVolumetricFullConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricFullConvolution_accGradParameters", (PyCFunction)CudaHalfVolumetricFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricFullConvolution_accGradParameters", (PyCFunction)CudaVolumetricFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricFullConvolution_accGradParameters", (PyCFunction)CudaDoubleVolumetricFullConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricMaxPooling_updateOutput", (PyCFunction)CudaHalfVolumetricMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricMaxPooling_updateOutput", (PyCFunction)CudaVolumetricMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricMaxPooling_updateOutput", (PyCFunction)CudaDoubleVolumetricMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricMaxPooling_updateGradInput", (PyCFunction)CudaHalfVolumetricMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricMaxPooling_updateGradInput", (PyCFunction)CudaVolumetricMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricMaxPooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricMaxUnpooling_updateOutput", (PyCFunction)CudaHalfVolumetricMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricMaxUnpooling_updateOutput", (PyCFunction)CudaVolumetricMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricMaxUnpooling_updateOutput", (PyCFunction)CudaDoubleVolumetricMaxUnpooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricMaxUnpooling_updateGradInput", (PyCFunction)CudaHalfVolumetricMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricMaxUnpooling_updateGradInput", (PyCFunction)CudaVolumetricMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricMaxUnpooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricMaxUnpooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaHalfVolumetricAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaVolumetricAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput", (PyCFunction)CudaDoubleVolumetricAdaptiveMaxPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaVolumetricAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricAdaptiveMaxPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaHalfVolumetricAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaVolumetricAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput", (PyCFunction)CudaDoubleVolumetricAdaptiveAveragePooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaVolumetricAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput", (PyCFunction)CudaDoubleVolumetricAdaptiveAveragePooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricReplicationPadding_updateOutput", (PyCFunction)CudaHalfVolumetricReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricReplicationPadding_updateOutput", (PyCFunction)CudaVolumetricReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricReplicationPadding_updateOutput", (PyCFunction)CudaDoubleVolumetricReplicationPadding_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricReplicationPadding_updateGradInput", (PyCFunction)CudaHalfVolumetricReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricReplicationPadding_updateGradInput", (PyCFunction)CudaVolumetricReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricReplicationPadding_updateGradInput", (PyCFunction)CudaDoubleVolumetricReplicationPadding_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricUpSamplingNearest_updateGradInput", (PyCFunction)CudaHalfVolumetricUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricUpSamplingNearest_updateGradInput", (PyCFunction)CudaVolumetricUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricUpSamplingNearest_updateGradInput", (PyCFunction)CudaDoubleVolumetricUpSamplingNearest_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricUpSamplingNearest_updateOutput", (PyCFunction)CudaHalfVolumetricUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricUpSamplingNearest_updateOutput", (PyCFunction)CudaVolumetricUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricUpSamplingNearest_updateOutput", (PyCFunction)CudaDoubleVolumetricUpSamplingNearest_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricUpSamplingTrilinear_updateOutput", (PyCFunction)CudaHalfVolumetricUpSamplingTrilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricUpSamplingTrilinear_updateOutput", (PyCFunction)CudaVolumetricUpSamplingTrilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricUpSamplingTrilinear_updateOutput", (PyCFunction)CudaDoubleVolumetricUpSamplingTrilinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaHalfVolumetricUpSamplingTrilinear_updateGradInput", (PyCFunction)CudaHalfVolumetricUpSamplingTrilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaVolumetricUpSamplingTrilinear_updateGradInput", (PyCFunction)CudaVolumetricUpSamplingTrilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput", (PyCFunction)CudaDoubleVolumetricUpSamplingTrilinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},

  {NULL, NULL, 0, NULL}
};
namespace torch { namespace nn {

static PyTypeObject thnn_type;

void init__THCUNN(PyObject* c_module) {
  ((PyObject*)&thnn_type)->ob_refcnt = 1;
  thnn_type.tp_flags = Py_TPFLAGS_DEFAULT;
  thnn_type.tp_methods = module_methods;
  thnn_type.tp_name = "torch._C._THCUNN";
  if (PyType_Ready(&thnn_type) < 0) {
    throw python_error();
  }

  PyObject* type_obj = (PyObject*)&thnn_type;
  Py_INCREF(type_obj);
  if (PyModule_AddObject(c_module, "_THCUNN", type_obj) < 0) {
    throw python_error();
  }
}

}}  // namespace torch::nn
