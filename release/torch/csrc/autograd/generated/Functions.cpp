#include "Functions.h"
#include <ATen/WrapDimUtils.h>
#include <ATen/WrapDimUtilsMulti.h>

// define constants like M_PI and C keywords for MSVC
#ifdef _MSC_VER
#define _USE_MATH_DEFINES
#include <ciso646>
#endif
#include <math.h>
#include <algorithm>
#include <numeric>

// generated from tools/autograd/templates/Functions.cpp

using at::Tensor;
using at::Scalar;
using at::IntList;
using at::TensorList;

namespace torch { namespace autograd { namespace generated {

namespace {

// Helper functions for autogenerated code

// A simple way to imperatively compute index ranges for slots
// that have been flattened
struct IndexRangeGenerator {
  IndexRange range(size_t range_size) {
    i += range_size;
    return {i - range_size, i};
  }
  size_t size() { return i; }
  private:
    size_t i = 0;
};

void copy_range(variable_list& out, IndexRange range, const Tensor & t) {
  AT_ASSERT(range.second <= out.size());
  AT_ASSERTM(range.second - range.first == 1, "inconsistent range for Tensor output");
  out[range.first] = t;
}

void copy_range(variable_list& out, IndexRange range, at::ArrayRef<Tensor> t) {
  AT_ASSERT(range.second <= out.size());
  AT_ASSERTM(range.second - range.first == t.size(), "inconsistent range for TensorList output");
  std::copy(t.begin(), t.end(), out.begin() + range.first);
}

Tensor not_implemented(const char* name) {
  throw std::runtime_error(
      std::string("the derivative for '") + name + "' is not implemented");
}

Tensor maybe_multiply(const Tensor & t, const Scalar & s) {
  bool is_one = false;
  if (s.isFloatingPoint()) {
    is_one = s.toDouble() == 1;
  } else if(s.isIntegral()) {
    is_one = s.toLong() == 1;
  }

  if (is_one) {
    return t;
  } else {
    return t * s;
  }
}

int64_t _safe_size(IntList sizes, int64_t dim) {
  dim = at::maybe_wrap_dim(dim, sizes.size());
  return sizes.size() != 0 ? sizes[dim] : 1;
}

Tensor norm_backward(const Tensor & grad, const Tensor & self, const Scalar & p_, const Tensor & norm) {
  double p = p_.toDouble();
  Tensor self_scaled;
  Tensor scale_v;
  if (p == 0.0) {
    return zeros_like(self);
  } else if (p == 1.0) {
    return self.sign() * grad;
  } else if (p < 2.0) {
    self_scaled = self.sign() * self.abs().pow(p - 1);
    scale_v = grad / norm.pow(p - 1);
  } else if (p == 2.0) {
    self_scaled = self;
    scale_v = grad / norm;
  } else if (p == INFINITY) {
    self_scaled = self.sign() * (self.abs() == norm).toType(self.type());
    scale_v = grad.clone();
  } else {
    self_scaled = self * self.abs().pow(p - 2);
    scale_v = grad / norm.pow(p - 1);
  }
  // handle case at 0 where we return a subgradient containing 0
  scale_v.masked_fill_(norm == 0, 0);
  return self_scaled * scale_v;
}

Tensor norm_backward(Tensor grad, const Tensor & self, const Scalar & p_, Tensor norm, int64_t dim, bool keepdim) {
  if (!keepdim && self.dim() != 0) {
    grad = grad.unsqueeze(dim);
    norm = norm.unsqueeze(dim);
  }
  return norm_backward(grad, self, p_, norm);
}

Tensor reduce_to(const Tensor & grad, IntList sizes) {
  if (sizes.size() == 0) {
    return grad.sum();
  }
  Tensor result = grad;
  while (result.dim() > (int64_t)sizes.size()) {
    result = result.sum(0, false);
  }
  for (int64_t i = 0; i < result.dim(); ++i) {
    if (sizes[i] == 1 && result.sizes()[i] > 1) {
      result = result.sum(i, true);
    }
  }
  return result;
}

Tensor permute_backwards(const Tensor & grad, IntList fwd_dims) {
  // invert the permutation
  auto ndims = fwd_dims.size();
  std::vector<int64_t> dims(ndims);
  for (size_t i = 0; i < ndims; i++) {
    dims[at::maybe_wrap_dim(fwd_dims[i], ndims)] = i;
  }
  return grad.permute(dims);
}

Tensor sum_backward(const Tensor & grad, IntList sizes, IntList dims, bool keepdim) {
  if (!keepdim && sizes.size() > 0) {
    if (dims.size()==1) {
      return grad.unsqueeze(dims[0]).expand(sizes);
    } else {
      auto dims_to_unsqueeze = dim_list_to_bitset(dims, sizes.size());
      Tensor res = grad;
      for (size_t i = 0; i < sizes.size(); i++){
	if (dims_to_unsqueeze[i])
	  res = res.unsqueeze(i);
      }
      return res.expand(sizes);
    }
  } else {
    return grad.expand(sizes);
  }
}

Tensor reverse_dim(const Tensor& t, int64_t dim) {
  Tensor index = at::arange(t.type().toScalarType(at::ScalarType::Long), t.size(dim) - 1, -1, -1);
  return t.index_select(dim, index);
}

Tensor prod_safe_zeros_backward(const Tensor &grad, const Tensor& inp, int64_t dim) {
  if (inp.size(dim) == 1) {
    return grad;
  }

  std::vector<int64_t> ones_size(inp.sizes());
  ones_size[dim] = 1;
  Tensor ones = at::ones(grad.type(), ones_size);
  Tensor exclusive_normal_nocp = at::cat({ones, inp.narrow(dim, 0, inp.size(dim) - 1)}, dim);
  Tensor exclusive_normal = exclusive_normal_nocp.cumprod(dim);

  Tensor narrow_reverse = reverse_dim(inp.narrow(dim, 1, inp.size(dim) - 1), dim);
  Tensor exclusive_reverse_nocp = at::cat({ones, narrow_reverse}, dim);
  Tensor exclusive_reverse = reverse_dim(exclusive_reverse_nocp.cumprod(dim), dim);

  return grad * (exclusive_normal * exclusive_reverse);
}

// note that the gradient for prod is equivalent to:
// cumprod(exclusive, normal) * cumprod(exclusive, reverse), e.g.:
// input:                        [    a,     b,     c]
// cumprod(exclusive, normal):   [1    ,     a, a * b]
// cumprod(exclusive, reverse):  [b * c,     c,     1]
// product:                      [b * c, a * c, a * b]
// and this is safe under input with 0s.
Tensor prod_backward(const Tensor& grad, const Tensor& input, const Tensor& result) {
  if (input.dim() == 0) {
    return grad;
  }
  Tensor zero_idx = (input == 0).nonzero();
  if (zero_idx.numel() == 0) {
    return (grad * result) / input;
  } else if (zero_idx.size(0) > 1) {
    return zeros_like(input);
  } else {
    return prod_safe_zeros_backward(grad, input.contiguous().view(-1), 0).view_as(input);
  }
}

Tensor prod_backward(Tensor grad, const Tensor& input, Tensor result, int64_t dim, bool keepdim) {
  if (input.dim() == 0) {
    return grad;
  }
  dim = at::maybe_wrap_dim(dim, input.sizes().size());
  if (!keepdim && input.dim() != 1) {
    grad = grad.unsqueeze(dim);
    result = result.unsqueeze(dim);
  }

  Tensor zero_mask = (input == 0);
  Tensor slice_zero_count = zero_mask.sum(dim, true);
  int64_t total_zeros = slice_zero_count.sum().toCLong();
  if (total_zeros == 0) {
    return (grad * result) / input;
  } else {
    return prod_safe_zeros_backward(grad, input, dim);
  }
}

Tensor sum_scan_exclusive(const Tensor& x, int64_t dim) {
  Tensor ret = at::cumsum(-x, dim);

  int64_t end_idx = ret.size(dim) - 1;
  Tensor ret_sum = ret.narrow(dim, end_idx, 1).clone();
  ret -= ret_sum.expand_as(ret);
  ret += x;
  return ret;
}

Tensor cumprod_backward(const Tensor &grad, const Tensor &input, int64_t dim) {
  /*
    There are two algorithms to do this. The first one
    is very efficient, but works only when there are no
    nonzero elements in the input.

    The second one is much more complex, but it doesn't
    assume anything on the input. The main downside is
    that it takes time O(n^2), where n = input.size(self.dim)
    (i.e. the length of the cumulative product). This is in
    contrast to the forward pass and the efficient algorithm,
    which are both O(n).

    The second algorithm is a simple application of the chain
    rule. If x is an n-dimensional vector, and y = cumprod(x),
    and F is the final cost, then

    dF / dx_k = sum_j (dF / dy_j) * (dy_j / dx_k)   (1)

    The term dF / dy_j is just grad_output[j] (assuming again
    everything is one-dimensional).

    The term (dy_j / dx_k) is easilly seen to be

    if j >= k
      dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i
    else:
      dy_j / dx_k = 0

    Note that the indicator (j>=k) can be taken out
    by replacing the sum in (1) with a sum from
    j = k to n.

    Thus,
    df / dx_k = sum_{k <= j <= n} grad_output[j] * (dy_j / dx_k)

    with
    dy_j / dx_k = prod_{1 <= i <= j, i != k} x_i     (2)

    Note that this last term is just the cumulative product
    with k omitted. Thus, if x_k (the input) is nonzero, we can
    just express this as

    dy_j / dx_k = (prod_{1 <= i <= j} x_i) / x_k
                = y_j / x_k

    So therefore,

    df / dx_k = sum_{k <= j <= n} grad_output[j] * y_j / x_k

    so

    grad_output = sum_scan_exclusiv(grad_output * output) / input

    If the input is nonzero, we need to calculate the dy_j / dx_k
    by using the formula (2), called in the code omitted_products.

    The way the code calculates it is simply by noting that

    prod_{1 <= i <= j, i != k} x_i
        = (prod_{1 <= i <= k} x_i) * (prod_{k + 1 <= i <= j} x_i)

    the first term is calculated as prods_until_k, which since
    doesn't depend in j is easy to vectorize.

    The second term (indexed by j) is the cumulative product of
    x_{k+1}, x_{k+2}, ..., x_n, and it's named in the code
    prods_from_k_pkus_1, and it's calculated as a cumprod.

    In order to vectorize this properly, we need to add to
    omitted_products the dimensions where k > j, and therefore
    dy_j / dx_k = 0, which is done right after the assert.
  */

  if (input.dim() == 0) {
    return grad;
  }
  dim = at::maybe_wrap_dim(dim, input.sizes().size());
  int64_t dim_size = input.size(dim);
  if (dim_size == 1) {
    return grad;
  }

  // Simple case with nonzero elements in the input
  if ((input != 0).all().toCByte()) {
    Tensor result = at::cumprod(input, dim);
    return sum_scan_exclusive(result * grad, dim) / input;
  }

  std::vector<int64_t> ones_size(input.sizes());
  ones_size[dim] = 1;
  Tensor ones = at::ones(grad.type(), {1}).expand(ones_size);
  Tensor grad_input = at::zeros(grad.type(), input.sizes());
  Tensor prods_from_k_plus_1;
  Tensor omitted_products;
  for (int k = 0; k < dim_size; ++k) {
    if (k == 0) {
      prods_from_k_plus_1 = at::cumprod(input.slice(dim, k + 1), dim);
      omitted_products = at::cat({ones, prods_from_k_plus_1}, dim);
    } else if (k == dim_size - 1) {
      Tensor prods_until_k = at::prod(input.slice(dim, 0, k), dim, true);
      omitted_products = prods_until_k;
    } else {
      Tensor prods_until_k = at::prod(input.slice(dim, 0, k), dim, true);
      prods_from_k_plus_1 = at::cumprod(input.slice(dim, k+1), dim);
      omitted_products = prods_until_k.expand_as(prods_from_k_plus_1) * prods_from_k_plus_1;
      omitted_products = at::cat({prods_until_k, omitted_products}, dim);
    }

    // At this point omitted_products is the same size
    // as input, except on the dimension dim where it's
    // dim_size - k
    TORCH_ASSERT(omitted_products.size(dim) == dim_size - k);

    grad_input.select(dim, k).copy_(
        at::sum(grad.slice(dim, k) * omitted_products,dim));
  }

  return grad_input;
}

Tensor gesv_backward_self(const Tensor & grad, const Tensor & self, const Tensor & A) {
  return std::get<0>(at::gesv(grad, A.transpose(-2, -1)));
}

Tensor gesv_backward_A(const Tensor & grad, const Tensor & self, const Tensor & A, const Tensor & solution) {
  Tensor grad_self = gesv_backward_self(grad, self, A);
  if (self.ndimension() == 2 && A.ndimension() == 2) {
    return -at::mm(grad_self, solution.transpose(-2, -1));
  }
  return -at::matmul(grad_self, solution.transpose(-2, -1));
}

Tensor cumsum_backward(const Tensor & x, int64_t dim) {
  if (x.dim() == 0) {
    return x;
  }
  auto ret = at::cumsum(-x, dim);
  auto ret_sum = ret.narrow(dim, ret.size(dim) - 1, 1).clone();
  ret -= ret_sum.expand(ret.sizes());
  ret += x;
  return ret;
}

Tensor logsumexp_backward(Tensor grad, const Tensor & self, Tensor result, int64_t dim, bool keepdim) {
  if (! keepdim) {
    grad = grad.unsqueeze(dim);
    result = result.unsqueeze(dim);
  }
  return grad * (self - result).exp();
}

Tensor unsqueeze_to(const Tensor & self, IntList sizes) {
  auto result = self;

  int64_t nDims = sizes.size();
  for (int64_t dim = 0; dim < nDims; dim++) {
    if (sizes[dim] == 1) {
      result = result.unsqueeze(dim);
    }
  }
  return result;
}

Tensor unsqueeze_to(const Tensor & self, int64_t dim, IntList sizes) {
  dim = at::maybe_wrap_dim(dim, sizes.size());
  // in NumPy it's not an error to unsqueeze a scalar, but we still need to avoided
  // unsqueezing in the backward.
  if (sizes.size() > 0 && sizes[dim] == 1) {
    return self.unsqueeze(dim);
  }
  return self;
}

std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<std::vector<int64_t>> &sizes, int64_t dim) {
  dim = at::legacy_cat_wrap_dim(dim, sizes);
  std::vector<Tensor> grad_inputs(sizes.size());
  int64_t accumulate = 0;
  for (size_t i = 0; i < sizes.size(); ++i) {
    auto& shape = sizes[i];
    // If input was empty tensor, gradInput should be empty tensor.
    if (shape == std::vector<int64_t>({0})) {
      grad_inputs[i] = at::zeros(grad.type(), {0});
      continue;
    }
    auto size = shape[dim];
    accumulate += size;
    grad_inputs[i] = grad.narrow(dim, accumulate - size, size);
  }
  return grad_inputs;
}

Tensor mm_mat1_backward(const Tensor & grad, const Tensor & mat2, IntList sizes, IntList strides, const Scalar & alpha) {
  // if input was column-major, return grad as column-order for efficiency
  if (strides[0] == 1 && strides[1] == sizes[0]) {
    return maybe_multiply(mat2.mm(grad.t()).t(), alpha);
  } else {
    return maybe_multiply(grad.mm(mat2.t()), alpha);
  }
}

Tensor mm_mat2_backward(const Tensor & grad, const Tensor & mat1, IntList sizes, IntList strides, const Scalar & alpha) {
  // if input was column-major, return grad as column-order for efficiency
  if (strides[0] == 1 && strides[1] == sizes[0]) {
    return maybe_multiply(grad.t().mm(mat1).t(), alpha);
  } else {
    return maybe_multiply(mat1.t().mm(grad), alpha);
  }
}

Tensor renorm_backward(const Tensor & grad, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
  auto transposed_sizes = std::vector<int64_t>(self.transpose(dim, 0).sizes());
  auto flatten = [&](const Tensor & t) {
    return t.transpose(dim, 0).contiguous().view({t.size(dim), -1});
  };
  auto unflatten = [&](const Tensor & t) {
    return t.contiguous().view(transposed_sizes).transpose(dim, 0);
  };

  // renorm computes the norm over all dimensions except `dim`, which is why
  // we need the flatten and unflatten business. TODO: simplify this when we
  // add support for norm over multiple dimensions.
  auto self_flat = flatten(self);
  auto grad_flat = flatten(grad);
  auto norm_flat = self_flat.norm(p, 1, true);
  auto grad_output = (self_flat * grad_flat).sum(1, true);
  auto nb = norm_backward(grad_output, self_flat, p, norm_flat, 1, true);
  auto invnorm = (norm_flat + 1e-7).reciprocal();
  auto grad_norm = unflatten(maxnorm * invnorm * (grad_flat - invnorm * nb));
  auto norm = unflatten(norm_flat.expand_as(self_flat));

  // TODO: remove the detach once comparison ops no longer require grad
  auto mask = Variable(norm < maxnorm).detach();
  return at::where(mask, grad, grad_norm);
}

Tensor sum_tensorlist(TensorList tl) {
  if (tl.size() == 0) {
    throw std::runtime_error("Can't sum tensorlist of size 0");
  }
  Tensor sum = tl[0];
  for(size_t i = 1; i < tl.size(); ++i) {
    sum = sum + tl[i];
  }
  return sum;
}

Tensor repeat_backward(Tensor grad, int64_t input_dims, IntList repeats) {
  int64_t num_unsqueezed = grad.dim() - input_dims;
  for (int64_t i = 0; i < num_unsqueezed; ++i) {
    grad = grad.sum(0, false);
  }
  for (size_t j = num_unsqueezed; j < repeats.size(); ++j) {
    int64_t repeat = repeats[j];
    if (repeat == 1) {
      continue;
    }
    int64_t dim = j - num_unsqueezed;
    grad = sum_tensorlist(grad.chunk(repeat, dim));
  }
  return grad;
}

Tensor select_backward_scalar(Tensor grad, const Tensor & input, const Tensor & value) {
  auto grad_input = zeros_like(input);
  grad_input.masked_fill_(input == value, grad);
  return grad_input;
}

Tensor select_backward(Tensor grad, int64_t dim, Tensor indices, IntList sizes, bool keepdim) {
  if (!keepdim) {
    grad = grad.unsqueeze(dim);
    indices = indices.unsqueeze(dim);
  }
  return at::zeros(grad.type(), sizes).scatter_(dim, indices, grad);
}

Tensor trace_backward(const Tensor & grad, IntList sizes) {
  if (sizes.size() != 2) {
    throw std::runtime_error("expected matrix input");
  }

  auto& long_type = grad.type().toScalarType(at::kLong);

  auto grad_input = at::zeros(grad.type(), sizes[0] * sizes[1]);
  auto indices = at::arange(long_type, 0, grad_input.numel(), sizes[1] + 1);
  grad_input.index_fill_(0, indices, grad);
  return grad_input.view(sizes);
}

Tensor unfold_backward(const Tensor & grad, IntList input_sizes, int64_t dim, int64_t size, int64_t step) {
  auto& long_type = grad.type().toScalarType(at::kLong);

  int64_t numel = 1;
  for (auto size : input_sizes) {
    numel *= size;
  }

  auto idx = at::arange(long_type, 0, numel).view(input_sizes);
  auto idx_unfolded = idx.unfold(dim, size, step).contiguous().view(-1);
  auto grad_input = at::zeros(grad.type(), {numel});
  grad_input.index_add_(0, idx_unfolded, grad.contiguous().view(-1));
  return grad_input.view(input_sizes);
}

Tensor var_backward(const Tensor & grad, const Tensor & self, bool unbiased) {
  return (2.0 / (self.numel() - unbiased)) * grad * (self - self.mean());
}

Tensor var_backward(Tensor grad, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) {
  if (self.dim() == 0) {
    return var_backward(grad, self, unbiased);
  }
  if (!keepdim && self.dim() > 1) {
    grad = grad.unsqueeze(dim);
  }
  return (2.0 / (self.size(dim) - unbiased)) * grad * (self - self.mean(dim, true));
}

Tensor masked_scatter_backward(const Tensor & grad, const Tensor & mask, IntList sizes) {
  int64_t numel = 1;
  for (auto size : sizes) {
    numel *= size;
  }
  auto mask_selected = grad.masked_select(mask);
  auto diff_nelem = numel - mask_selected.numel();
  if (diff_nelem > 0) {
    // because mask_selected returns a 1-d tensor with size of masked elements that are 1,
    // we need to fill out the rest with zeros then reshape back to tensor2's size.
    auto zeros_fillin = at::zeros(grad.type(), {diff_nelem});
    mask_selected = at::cat({mask_selected, zeros_fillin}, 0);
  }
  return mask_selected.view(sizes);
}

Tensor potrf_backward(Tensor grad, bool upper, Tensor L) {
  // cf. Iain Murray (2016); arXiv 1602.07527
  if (upper) {
    L = L.t();
    grad = grad.t();
  }

  auto phi = [](const Tensor & A) -> Tensor {
    auto B = A.tril();
    B = B - 0.5 * at::diag(at::diag(B));
    return B;
  };

  // make sure not to double-count variation, since
  // only half of output matrix is unique
  auto Lbar = grad.tril();

  auto P = phi(at::mm(L.t(), Lbar));
  Tensor S;
  std::tie(S, std::ignore) = at::gesv(P + P.t(), L.t());
  std::tie(S, std::ignore) = at::gesv(S.t(), L.t());
  S = phi(S);
  if (upper) {
    S = S.t();
  }
  return S;
}

Tensor split_with_sizes_backward(const std::vector<torch::autograd::Variable> &grads,
                                 IntList split_sizes, int64_t dim, IntList sizes, const Type &type) {
  dim = at::maybe_wrap_dim(dim, sizes.size());

  // it's possible some of the grads are not defined (represents tensors of all 0s).
  // Since at::cat can't handle those, let's define them
  std::vector<Tensor> grads_all_defined(grads.size());
  for (size_t j = 0; j < grads.size(); ++j) {
    if (grads[j].defined()) {
      grads_all_defined[j] = grads[j];
    } else {
      auto length = split_sizes[j];
      std::vector<int64_t> grad_size(sizes);
      grad_size[dim] = length;
      grads_all_defined[j] = at::zeros(type, grad_size);
    }
  }

  auto ret =  at::cat(grads_all_defined, dim);
  return ret;
}

Tensor split_backward(const std::vector<torch::autograd::Variable> &grads,
                      int64_t split_size, int64_t dim, IntList sizes, const Type &type) {
  dim = at::maybe_wrap_dim(dim, sizes.size());
  int64_t dim_size = sizes[dim];
  int64_t num_splits = grads.size();
  std::vector<int64_t> split_sizes(num_splits, split_size);
  split_sizes[num_splits - 1] = split_size - (split_size * num_splits - dim_size);
  return split_with_sizes_backward(grads, split_sizes, dim, sizes, type);
}

Tensor max_pool_double_backward(const Tensor & grad, const Tensor & indices, int dim) {
  TORCH_ASSERT(indices.dim() >= dim);
  auto size = std::vector<int64_t>(indices.sizes().slice(0, indices.dim() - dim));
  size.push_back(-1);
  auto indices_view = indices.view(size);
  return grad.contiguous().view(size).gather(-1, indices_view).view(indices.sizes());
}

Tensor glu_double_backward(const Tensor & grad, const Tensor & grad_output, const Tensor & input, int64_t dim) {
  auto& gO = grad_output;
  auto input_size = input.size(dim) / 2;
  auto first_half = input.narrow(dim, 0, input_size);
  auto second_half = input.narrow(dim, input_size, input_size);
  auto sig_second_half = second_half.sigmoid();
  auto one_sub_sig_second_half = 1 - sig_second_half;
  auto sig_one_sub_sig = sig_second_half * one_sub_sig_second_half;

  auto ggI_first_half = grad.narrow(dim, 0, input_size);
  auto ggI_second_half = grad.narrow(dim, input_size, input_size);
  auto ggI_second_half_times_first_half = ggI_second_half * first_half;

  auto gI_first_half = ggI_second_half * gO * sig_one_sub_sig;
  auto second_order_sh = sig_one_sub_sig * one_sub_sig_second_half - sig_second_half * sig_one_sub_sig;
  auto gI_second_half = ggI_second_half_times_first_half * gO * second_order_sh + ggI_first_half * gO * sig_one_sub_sig;
  return at::cat({gI_first_half, gI_second_half}, dim);
}

Tensor glu_double_backward_grad_output(const Tensor & grad, const Tensor & input, int64_t dim) {
  if (dim < 0) dim += input.dim();
  std::vector<int64_t> sizes = input.sizes();
  sizes[dim] /= 2;
  auto tmp = grad * glu_backward(at::ones(input.type(), sizes), input, dim);
  return tmp.narrow(dim, 0, sizes[dim]) + tmp.narrow(dim, sizes[dim], sizes[dim]);
}

Tensor kl_div_double_backward_grad_output(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  auto result = kl_div_backward(grad, input, target, size_average, false);
  if (reduce && size_average) {
    return result.mean();
  } else if (reduce) {
    return result.sum();
  }
  return result;
}

Tensor log_sigmoid_double_backward(const Tensor & grad, const Tensor & input) {
  auto z = input.sigmoid();
  return grad * (z - 1) * z;
}

Tensor softmax_double_backward(const Tensor & grad, const Tensor & grad_output, int dim, const Tensor & output) {
  auto gO = grad_output;
  auto ggI = grad;

  auto ggI_output = ggI * output;
  auto ggI_out_sum = ggI_output.sum(dim, true);
  auto ggI_out_sum_output = ggI_out_sum * output;
  auto gO_out_sum = (gO * output).sum(dim, true);

  // gI calculation
  auto gI_t0 = ggI_output * (gO - gO_out_sum);
  auto gI_t1 = output * ((ggI_output * gO).sum(dim, true).sub_(gO_out_sum * ggI_out_sum));
  auto gI_t2 = ggI_out_sum_output * gO;
  auto gI_t3 = ggI_out_sum_output * gO_out_sum;
  return gI_t0 - gI_t1 - gI_t2 + gI_t3;
}

Tensor log_softmax_double_backward(const Tensor & grad, const Tensor & grad_output, int dim, const Tensor & output) {
  auto z = output.exp();
  return z * grad_output.sum(dim, true) * ((grad * z).sum(dim, true) - grad);
}

Tensor l1_loss_double_backward_grad_output(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  auto output = l1_loss_backward(grad, input, target, size_average, false);
  if (reduce and size_average) {
    return output.mean();
  } else if (reduce) {
    return output.sum();
  }
  return output;
}

Tensor smooth_l1_loss_double_backward(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  auto d = (input - target).abs();
  auto grad_input = grad * (d < 1).toType(grad.type());
  if (size_average && reduce) {
    grad_input /= input.numel();
  }
  return grad_input;
}

Tensor smooth_l1_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  if (!reduce) {
    return smooth_l1_loss_backward(grad, input, target, size_average, reduce);
  }
  auto r = smooth_l1_loss_backward(ones_like(grad_output), input, target, size_average, true);
  return (r * grad).sum();
}

static inline int64_t diag_size(int64_t height, int64_t width, int64_t diagonal) {
  if (width > height) {
    return diag_size(width, height, -diagonal);
  }
  // Assumes height >= width
  auto longest_diag = width;
  if (diagonal >= 0) {
    return longest_diag - diagonal;
  }
  if (longest_diag < height + diagonal) {
    return longest_diag;
  }
  return height + diagonal;
}

Tensor diag_backward(const Tensor & grad, IntList input_sizes, int64_t diagonal) {
  auto ndimension = input_sizes.size();
  TORCH_ASSERT(ndimension == 1 || ndimension == 2);

  if (ndimension == 1 || input_sizes[0] == input_sizes[1]) {
    return grad.diag(diagonal);
  }

  // Input was a matrix but was not square
  auto grad_input = at::zeros(grad.type(), input_sizes);
  auto diagonal_size = diag_size(input_sizes[0], input_sizes[1], diagonal);
  auto storage_offset = diagonal >= 0 ? diagonal : -diagonal * input_sizes[1];
  auto diag = grad_input.as_strided({diagonal_size}, {input_sizes[1] + 1}, storage_offset);
  diag.copy_(grad);
  return grad_input;
}

Tensor diagonal_backward(const Tensor & grad, IntList input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
  auto grad_input = at::zeros(grad.type(), input_sizes);
  auto diag = grad_input.diagonal(offset, dim1, dim2);
  diag.copy_(grad);
  return grad_input;
}

Tensor mse_loss_double_backward(const Tensor & grad, const Tensor & input, bool size_average, bool reduce) {
  auto grad_input = 2 * grad;
  if (size_average && reduce) {
    grad_input /= input.numel();
  }
  return grad_input;
}

Tensor mse_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  if (!reduce) {
    return mse_loss_backward(grad, input, target, size_average, reduce);
  }
  auto r = mse_loss_backward(ones_like(grad_output), input, target, size_average, true);
  return (r * grad).sum();
}

Tensor soft_margin_loss_double_backward(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  auto z = (input * -target).exp();
  auto zplus1 = z + 1;
  auto grad_input = grad * (target * target) * z / (zplus1 * zplus1);
  if (size_average && reduce) {
    grad_input /= input.numel();
  }
  return grad_input;
}

Tensor soft_margin_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce) {
  if (!reduce) {
    return soft_margin_loss_backward(grad, input, target, size_average, reduce);
  }
  auto r = soft_margin_loss_backward(ones_like(grad_output), input, target, size_average, true);
  return (r * grad).sum();
}

Tensor softplus_double_backward(const Tensor & grad, const Tensor & input, Scalar beta, Scalar threshold) {
  auto x = (input * beta);
  return _sigmoid_backward(grad, x.sigmoid()) * (x < threshold).toType(grad.type()) * beta;
}

Tensor as_strided_backward(const Tensor & grad, TensorGeometry base, IntList sizes, IntList strides, int64_t storage_offset) {
  auto src = base.zeros_with_stride(grad.type());
  src.as_strided(sizes, strides, storage_offset - base.storage_offset()).copy_(grad);
  return src;
}

std::tuple<Tensor, Tensor> atan2_backward(const Tensor& grad, const Tensor& self, const Tensor& other, std::array<bool, 2> output_mask) {
  auto recip = (self * self + other * other).reciprocal();
  return std::tuple<Tensor,Tensor>{
            output_mask[0] ? grad * other * recip : Tensor(),
            output_mask[1] ? grad * -self * recip : Tensor() };
}

// TODO: Seriously consider writing the derivative formulas for
// each output separately; there is not all that much sharing
// of computation going on here.
std::tuple<Tensor, Tensor, Tensor> prelu_double_backward(
    const Tensor & mb_ggI,
    const Tensor & mb_ggW,
    const Tensor & mb_gO,
    const Tensor & input,
    const Tensor & weight,
    std::array<bool, 3> output_mask) {

  // Zero-fill undefined grads (TODO: do this more efficiently)
  auto ggI = mb_ggI.defined() ? mb_ggI : input.type().zeros_like(input);
  auto ggW = mb_ggW.defined() ? mb_ggW : weight.type().zeros_like(weight);
  auto gO = mb_gO.defined() ? mb_gO : input.type().zeros_like(input);

  auto positive_mask = (input > 0).type_as(ggI);
  auto nonpositive_mask = (input <= 0).type_as(ggW);

  // Explanation: Let input be i, weight be w, grad_output be gO.
  // f(i, w) = i  if i > 0
  //         = wi if i <= 0
  // df/di * gO  = gO      if i > 0      df/dw * g0 = 0      if i > 0
  //             = g0 * w  if i <= 0                = g0 * i  if i <= 0
  // The rest is taking derivatives of these wrt i, w, gO and summing/expanding properly.

  if (weight.numel() == 1) {
      // from PReLU.forward: num_parameters == 0 is used indicate that a
      // single weight is shared among all input channels.

      // this is a little tricky because PReLU currently doesn't take a shape so the weight may be
      // 1-d when the input is a scalar (and there isn't a good Parameter API for that anyway until Variable
      // and tensor are merged).  So, use weight and ggW as 0-dim in this case.
      bool scalar_input_1d_weight = (positive_mask.dim() == 0 && weight.dim() == 1);
      auto weight_maybe_squeeze = scalar_input_1d_weight ? weight.squeeze() : weight;
      auto ggW_maybe_squeeze = scalar_input_1d_weight ? ggW.squeeze() : ggW;

      auto mask = positive_mask + nonpositive_mask * weight_maybe_squeeze.expand_as(input);
      auto ggO = ggI * mask + ggW_maybe_squeeze.expand_as(gO) * (nonpositive_mask * input);
      return std::tuple<Tensor, Tensor, Tensor>(
                ggO,
                ggW_maybe_squeeze.expand_as(gO) * gO * nonpositive_mask,
                (ggI * gO * nonpositive_mask).sum().expand_as(weight)
          );
  } else {
      // Expand ggW to match size of ggI; a simple expand doesn't work because
      // ggW is the size of the input channel (dim==1 unless there is only 1 dimension).  For example,
      // let ggI be size (3,4,5,6,7) and ggW be size (4).  Then we unsqueeze ggW to be size (4,1,1,1)
      // so the expand succeeds.
      auto dims_to_unsqueeze = std::max<int64_t>(input.dim() - 2, 0);
      auto ggW_expanded = ggW;
      for (int64_t i = 0; i < dims_to_unsqueeze; i++) {
          ggW_expanded = ggW_expanded.unsqueeze(1);
      }
      ggW_expanded = ggW_expanded.expand_as(ggI);

      auto gI = ggW_expanded * gO * nonpositive_mask;

      auto gW = ggI * gO * nonpositive_mask;
      if (input.dim() > 1) {
          gW = gW.sum(0);
      }
      while (gW.dim() > 1) {
          gW = gW.sum(1);
      }

      Tensor ggO;
      if (output_mask[0]) {
          // expand weight as input as in ggW/ggI above
          auto weight_expanded = weight;
          for (int64_t i = 0; i < dims_to_unsqueeze; i++) {
              weight_expanded = weight_expanded.unsqueeze(1);
          }
          weight_expanded = weight_expanded.expand_as(input);

          auto mask = positive_mask + nonpositive_mask * weight_expanded;
          ggO = ggI * mask + ggW_expanded * nonpositive_mask * input;
      }
      return std::tuple<Tensor,Tensor,Tensor>{ggO, gI, gW};
  }
}

// https://j-towns.github.io/papers/svd-derivative.pdf
//
// This makes no assumption on the signs of sigma.
Tensor svd_backward(const std::vector<torch::autograd::Variable> &grads, const Tensor& self,
          bool some, const Tensor& raw_u, const Tensor& sigma, const Tensor& raw_v) {
  auto m = self.size(0);
  auto n = self.size(1);
  auto k = sigma.size(0);
  auto gsigma = grads[1];

  auto u = raw_u;
  auto v = raw_v;
  auto gu = grads[0];
  auto gv = grads[2];

  if (!some) {
    // We ignore the free subspace here because possible base vectors cancel
    // each other, e.g., both -v and +v are valid base for a dimension.
    // Don't assume behavior of any particular implementation of svd.
    u = raw_u.narrow(1, 0, k);
    v = raw_v.narrow(1, 0, k);
    if (gu.defined()) {
      gu = gu.narrow(1, 0, k);
    }
    if (gv.defined()) {
      gv = gv.narrow(1, 0, k);
    }
  }
  auto vt = v.t();

  Tensor sigma_term;
  if (gsigma.defined()) {
    sigma_term = u.mm(gsigma.diag()).mm(vt);
  } else {
    sigma_term = at::zeros(self.type(), {1}).expand_as(self);
  }
  // in case that there are no gu and gv, we can avoid the series of kernel
  // calls below
  if (!gv.defined() && !gu.defined()) {
    return sigma_term;
  }

  auto ut = u.t();
  auto im = self.type().eye(m);
  auto in = self.type().eye(n);
  auto sigma_mat = sigma.diag();
  auto sigma_mat_inv = sigma.pow(-1).diag();
  auto sigma_expanded_sq = sigma.pow(2).expand_as(sigma_mat);
  auto F = sigma_expanded_sq - sigma_expanded_sq.t();
  // The following two lines invert values of F, and fills the diagonal with 0s.
  // Notice that F currently has 0s on diagonal. So we fill diagonal with +inf
  // first to prevent nan from appearing in backward of this function.
  F.as_strided({k}, {k + 1}).fill_(INFINITY);
  F = F.pow(-1);

  Tensor u_term, v_term;

  if (gu.defined()) {
    u_term = u.mm(F.mul(ut.mm(gu) - gu.t().mm(u))).mm(sigma_mat);
    if (m > k) {
      u_term = u_term + (im - u.mm(ut)).mm(gu).mm(sigma_mat_inv);
    }
    u_term = u_term.mm(vt);
  } else {
    u_term = at::zeros(self.type(), {1}).expand_as(self);
  }

  if (gv.defined()) {
    auto gvt = gv.t();
    v_term = sigma_mat.mm(F.mul(vt.mm(gv) - gvt.mm(v))).mm(vt);
    if (n > k) {
      v_term = v_term + sigma_mat_inv.mm(gvt.mm(in - v.mm(vt)));
    }
    v_term = u.mm(v_term);
  } else {
    v_term = at::zeros(self.type(), {1}).expand_as(self);
  }

  return u_term + sigma_term + v_term;
}

// Invertible case is derived from Jacobi's formula, and also can be found at:
// http://eprints.maths.ox.ac.uk/1079/1/NA-08-01.pdf
Tensor det_backward(const Tensor & grad, const Tensor& self, const Tensor& det) {
  auto det_val = det.toCDouble();
  if (det_val != 0 /* invertible */) {
    return grad * det * self.inverse().t();
  } else /* otherwise det = \prod(sigma) = 0, use svd */ {
    Tensor u, sigma, v;
    std::tie(u, sigma, v) = self.svd();
    auto gsigma = prod_backward(grad, sigma, det);
    return svd_backward({{}, gsigma, {}}, self, true, u, sigma, v);
  }
}

Tensor logdet_backward(const Tensor & grad, const Tensor& self, const Tensor& logdet) {
  auto logdet_val = logdet.toCDouble();
  if (logdet_val != -INFINITY /* det != 0, invertible */) {
    return grad * self.inverse().t();
  } else /* otherwise det = \prod(sigma) = 0, use svd */ {
    Tensor u, sigma, v;
    std::tie(u, sigma, v) = self.svd();
    // backward det = \sum log(sigma)
    auto gsigma = grad.div(sigma);
    return svd_backward({{}, gsigma, {}}, self, true, u, sigma, v);
  }
}

Tensor slogdet_backward(const std::vector<torch::autograd::Variable> &grads,
                        const Tensor& self,
                        const Tensor& signdet, const Tensor& logabsdet) {
  AT_ASSERTM(!grads[0].defined(), "slogdet's sign output should never have gradient");
  auto signdet_val = signdet.toCDouble();
  if (signdet_val != 0 /* det != 0, invertible */) {
    return grads[1] * self.inverse().t();
  } else /* otherwise det = \prod(sigma) = 0, use svd */ {
    Tensor u, sigma, v;
    std::tie(u, sigma, v) = self.svd();
    // sigma has all non-negative entries (also with at least one zero entry)
    // so logabsdet = \sum log(abs(sigma))
    // but det = 0, so backward logabsdet = \sum log(sigma)
    auto gsigma = grads[1].div(sigma);
    return svd_backward({{}, gsigma, {}}, self, true, u, sigma, v);
  }
}

// Reference:
// https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf
// Sec. 2.3.1 Matrix inverse product
std::tuple<Tensor, Tensor> trtrs_backward(
    const Tensor & grad_x, const Tensor & grad_m,
    const Tensor & b, const Tensor & a, const Tensor & x,
    const bool upper, const bool transpose, const bool unitriangular,
    std::array<bool, 2> output_mask) {
  Tensor grad_b, grad_a;
  if (grad_x.defined()) {
    grad_b = std::get<0>(grad_x.trtrs(a, upper, !transpose, unitriangular));
    if (output_mask[1]) {
      grad_a = transpose ? -x.mm(grad_b.t()) : -grad_b.mm(x.t());
      if (upper) {
        grad_a = grad_a.triu((int) unitriangular);
      } else {
        grad_a = grad_a.tril(-((int) unitriangular));
      }
    }
  }
  if (!grad_a.defined()) {
    grad_a = at::zeros(a.type(), {1}).expand_as(a);
  }
  if (!grad_b.defined()) {
    grad_b = at::zeros(b.type(), {1}).expand_as(b);
  }
  if (output_mask[1] && grad_m.defined()) {
    grad_a = grad_a.add(grad_m);
  }
  return std::tuple<Tensor, Tensor>{grad_b, grad_a};
}

// Generally speaking, fft's backward is ifft.
Tensor fft_backward(const Tensor& self, const Tensor& grad, int64_t signal_ndim,
                    bool complex_input, bool complex_output,
                    bool inverse, IntList checked_signal_sizes,
                    bool normalized, bool onesided,
                    IntList output_sizes) {
  Tensor gI;
  if (!complex_input && complex_output) {
    // Forward is R2C
    // Do inverse C2C and project onto real plane because grad can be
    // asymmetrical so C2R can't be used.
    if (onesided) {
      // Forward is R2C (onesided)
      // Think of onesided R2C rfft as
      //     1. view as complex numbers (fill complex dim with zeros)
      //     2. C2C fft
      //     3. discard half of results
      // So backward is
      //     1. fill the other half with zeros (with `zero_grad_shape` below)
      //        (C2C ifft only take twosided inputs so we need to fill here)
      //     2. inverse C2C ifft
      //     3. discard the complex dim
      int64_t zero_length = checked_signal_sizes[signal_ndim - 1] - grad.size(signal_ndim);
      auto complex_full_grad = grad;
      if (zero_length > 0) {
        std::vector<int64_t> zero_grad_shape(signal_ndim + 2);
        zero_grad_shape[0] = self.size(0);
        for (int64_t i = 1; i < signal_ndim; i++) {
          zero_grad_shape[i] = checked_signal_sizes[i - 1];
        }
        zero_grad_shape[signal_ndim] = zero_length;
        zero_grad_shape[signal_ndim + 1] = 2;
        complex_full_grad =  at::cat({ grad, at::zeros(grad.type(), zero_grad_shape) }, signal_ndim);
      }
      gI = _fft_with_size(complex_full_grad, signal_ndim,
                          /* complex_input */ true, /* complex_output */ true,
                          !inverse, checked_signal_sizes, normalized,
                          /* onesided */ false, complex_full_grad.sizes()).select(-1, 0);
    } else {
      gI = _fft_with_size(grad, signal_ndim, /* complex_input */ true,
                          /* complex_output */ true, !inverse,
                          checked_signal_sizes, normalized,
                          /* onesided */ false, grad.sizes()).select(-1, 0);
    }
  } else if (complex_input && !complex_output && onesided) {
    // Forward is C2R (onesided)
    // Think of onesided C2R irfft as
    //    1. fill the other half by conjugate symmetry
    //    2. inverse C2C ifft
    //    3. discard the complex dimension
    // So backward is
    //    1. R2C rfft (essentially add dummy complex dimension, and dft)
    //    2. accumulate gradient by conjugate symmetry
    //       since rfft results follow conjugate symmetry, we only need to
    //       double some entries from onesided rfft results, i.e., the ones with
    //       their reflected indices also landing out of the onesided range. So
    //       consider the index of last dim:
    //           i.   idx = 0.
    //                Reflected to (N - 0) % N = 0. Not doubled.
    //           ii   0 < idx < floor(N/2) (last).
    //                N > N - idx > ceil(N/2)
    //                Reflected to ()
    //           iii. idx = floor(N/2) = N/2 (last) when N even.
    //                Reflected to (N - N/2) % N = N/2. Not doubled.
    //           iv.  idx = floor(N/2) = (N-1)/2 (last) when N odd.
    //                Reflected to (N - (N-1)/2) % N = (N+1)/2. Doubled.
    //       Therefore, needs to double
    //           idx = 1, 2, ..., N/2 - 1     when N even
    //           idx = 1, 2, ..., (N-1)/2     when N odd
    //       that is
    //           idx = 1, 2, ..., N - (floor(N/2) + 1)
    //               = 1, 2, ..., N - onesided_length
    gI = _fft_with_size(grad, signal_ndim, /* complex_input */ false,
                        /* complex_output */ true, /* inverse */ false,
                        checked_signal_sizes, normalized, /* onesided */ true,
                        self.sizes());
    int64_t double_length = checked_signal_sizes[signal_ndim - 1] - self.size(signal_ndim);
    if (double_length > 0) {  // also covers case when signal size is zero
      gI.narrow(signal_ndim, 1, double_length).mul_(2);
    }
  } else {
    gI = _fft_with_size(grad, signal_ndim, complex_output, complex_input,
                        !inverse, checked_signal_sizes, normalized, onesided,
                        self.sizes());
  }
  if (normalized) {
    // If normalized, backward is exactly calling fft with inversed argument as
    // the forward because both are unitary.
    return gI;
  } else {
    // If not normalized, in backward, we need to upscale or downscale gI basing
    // on whether the forward is an inverse fft.
    auto signal_numel = std::accumulate(checked_signal_sizes.begin(),
                    checked_signal_sizes.end(), 1, std::multiplies<int64_t>());
    if (!inverse) {
      return gI.mul_(static_cast<double>(signal_numel));
    } else {
      return gI.div_(static_cast<double>(signal_numel));
    }
  }
}

// Helper for batchnorm_double_backward
Tensor sum_exclude_dim1(const Tensor& to_sum, bool keepdim=true) {
  auto r = to_sum.sum(0, keepdim);
  int64_t start_point_exclusive = keepdim ? 1 : 0;
  for (int64_t dim = r.dim() - 1; dim > start_point_exclusive; dim--) {
    r = r.sum(dim, keepdim);
  }
  return r;
}

// Helper for batchnorm_double_backward
// similar to expand_as below, but doesn't do the expand_as; operates as if
// reductions were done with keepdim=True
Tensor unsqueeze_dim1(const Tensor& src, const Tensor& target) {
  auto src_expanded = src;
  while (src_expanded.sizes().size() < target.sizes().size() - 1) {
    src_expanded = src_expanded.unsqueeze(1);
  }
  if (src_expanded.sizes().size() == target.sizes().size() - 1) {
    src_expanded = src_expanded.unsqueeze(0);
  }
  return src_expanded;
}

// Helper for batchnorm_double_backward
// because gamma/ggG/ggB are 1-dimensional and represent dim==1, we can't
// do a straight expansion because it won't follow the broadcasting rules.
Tensor expand_as_dim1(const Tensor& src, const Tensor& target) {
  auto src_expanded = src;
  while (src_expanded.sizes().size() < target.sizes().size() - 1) {
    src_expanded = src_expanded.unsqueeze(1);
  }
  return src_expanded.expand_as(target);
}

// NB: This currently is PURPOSELY outside of the anonymous namespace, because
// we are manually calling it from some legacy batchnorm invocation code.  Once
// that code moves into derivatives.yaml, this can be moved into the anonymous
// namespace, BUT NOT BEFORE THEN.
std::tuple<Tensor, Tensor, Tensor> batchnorm_double_backward(
    const Tensor & input,
    const Tensor & gamma,
    const Tensor & ggI,
    const Tensor & ggG,
    const Tensor & ggB,
    const Tensor & gO,
    const Tensor & running_mean,
    const Tensor & running_var,
    bool training,
    double eps,
    const Tensor & save_mean,
    const Tensor & save_std,
    std::array<bool,3> output_mask) {

  bool affine = gamma.defined();
  // TODO: Do we have a ScalarOrTensor type?  Would such a thing exist?
  Tensor gamma_expanded;
  Tensor ggG_expanded, ggB_expanded;
  if (affine) {
    gamma_expanded = expand_as_dim1(gamma, input);
    if (ggG.defined()) {
      ggG_expanded = expand_as_dim1(ggG, input);
    }
    if (ggB.defined()) {
      ggB_expanded = expand_as_dim1(ggB, input);
    }
  } else {
    gamma_expanded = input.type().tensor({}).fill_(1);
  }

  // define some terms we will reuse
  auto M = input.size(0);
  for (auto s : input.sizes().slice(2)) {
    M *= s;
  }
  auto mu = unsqueeze_dim1(training ? save_mean : running_mean, input);
  auto input_sub_mu = input - mu;
  auto sigma2_eps_neg_1_2 = unsqueeze_dim1(training ? save_std : running_var.add(Scalar(eps)).pow(-0.5), input);
  auto sigma2_eps_neg_1 = sigma2_eps_neg_1_2.pow(2);
  auto sigma2_eps_neg_3_2 = sigma2_eps_neg_1_2.pow(3);

  // calculate gI
  auto input_mu_sigma2_neg_3_2 = input_sub_mu * sigma2_eps_neg_3_2;
  auto gOinmu_sum = sum_exclude_dim1(gO * input_sub_mu);
  auto gO_sum = sum_exclude_dim1(gO);

  Tensor gI;
  if (ggI.defined() && training) {
    auto ggI_sum = sum_exclude_dim1(ggI);
    auto ggIinmu_sum = sum_exclude_dim1(ggI * input_sub_mu);
    auto all_sub = ((ggI_sum * gO_sum).div_(M)).sub_(sum_exclude_dim1(gO * ggI)).add_(
                    (sigma2_eps_neg_1 * gOinmu_sum * ggIinmu_sum).mul_(3. / M));
    auto gI_0t = (input_mu_sigma2_neg_3_2 * all_sub).div_(M);
    auto gI_1t = (ggIinmu_sum * sigma2_eps_neg_3_2).div_(M) * (gO_sum.div(M) - gO);
    auto gI_2t = (gOinmu_sum * sigma2_eps_neg_3_2).div_(M) * (ggI_sum.div(M) - ggI);
    gI = gamma_expanded * (gI_0t.add_(gI_1t).add_(gI_2t));
  }

  // add contribution of gamma term to gI
  Tensor gI_G_term;
  if (affine && ggG.defined()) {
    if (training) {
      auto t0 = gO * sigma2_eps_neg_1_2;
      auto t1 = (sigma2_eps_neg_1_2 * gO_sum).div_(-M);
      auto t2 = (input_mu_sigma2_neg_3_2 * sum_exclude_dim1(gO * input_sub_mu)).div_(-M);
      gI_G_term = ggG_expanded * (t0.add_(t1).add_(t2));
      gI = gI.defined() ? gI.add_(gI_G_term) : gI_G_term;
    } else {
      gI_G_term = ggG_expanded * sigma2_eps_neg_1_2 * gO;
      gI = gI.defined() ? gI.add_(gI_G_term) : gI_G_term;
    }
  }

  // this is the first backward's grad_input
  auto first_back_grad_input = [&](const Tensor& gO, const Tensor& gamma) -> Tensor {
    auto h0 = (gamma * sigma2_eps_neg_1_2).div_(M);
    auto h1 = (M * gO).sub_(sum_exclude_dim1(gO)).sub_(
                input_sub_mu.mul(sigma2_eps_neg_1) * sum_exclude_dim1(gO * input_sub_mu));
    return h0 * h1;
  };

  // calculate gG
  Tensor gG;
  if (affine && ggI.defined()) {
    if (training) {
      // gG is just the first backwards with the gamma term removed (then shaped properly)
      gG = ggI * first_back_grad_input(gO, sigma2_eps_neg_1_2.type().tensor({}).fill_(1));
      gG = sum_exclude_dim1(gG, false);
    } else {
      gG = sum_exclude_dim1(ggI * gO * sigma2_eps_neg_1_2, false);
    }
  }

  // calculate ggO
  Tensor ggO;
  // contribution of input term
  if (ggI.defined()) {
    if (training) {
      ggO = first_back_grad_input(ggI, gamma_expanded);
    } else {
      ggO = ggI * sigma2_eps_neg_1_2 * gamma_expanded;
    }
  }
  if (ggG.defined()) {
    auto ggO_G_term = ggG_expanded * input_sub_mu * sigma2_eps_neg_1_2;
    ggO = ggO.defined() ? ggO.add_(ggO_G_term) : ggO_G_term;
  }
  if (ggB.defined()) {
    auto ggO_B_term = ggB_expanded;
    ggO = ggO.defined() ? ggO.add_(ggO_B_term) : ggO_B_term;
  }

  if (output_mask[0] && !ggO.defined()) ggO = at::zeros_like(gO);
  if (output_mask[1] && !gG.defined()) {
    AT_ASSERTM(affine, "gamma should always be defined when it requires grad");
    gG = at::zeros_like(gamma);
  }
  if (output_mask[2] && !gI.defined()) gI = at::zeros_like(input);

  return std::tuple<Tensor, Tensor, Tensor>{gI, gG, ggO};

}

std::tuple<Tensor, Tensor, Tensor> _trilinear_backward(const Tensor& grad_out, const Tensor& i1, const Tensor& i2, const Tensor& i3,
						       IntList expand1, IntList expand2, IntList expand3,
						       IntList sumdim, int64_t unroll_dim, std::array<bool, 3> grad_mask) {
  Tensor grad_i1, grad_i2, grad_i3;
  if (grad_mask[0])
    grad_i1 = at::_trilinear(grad_out, i2, i3, sumdim, expand2, expand3, expand1);
  if (grad_mask[1])
    grad_i2 = at::_trilinear(i1, grad_out, i3, expand1, sumdim, expand3, expand2);
  if (grad_mask[2])
    grad_i3 = at::_trilinear(i1, i2, grad_out, expand1, expand2, sumdim, expand3);
  return std::tuple<Tensor, Tensor, Tensor>(grad_i1, grad_i2, grad_i3);
}

} // anonymous namespace

variable_list AbsBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * self.sign();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AcosBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * -((-self * self + 1).rsqrt());
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ other_ix })) {
    auto grad_result = maybe_multiply(grad, alpha);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddbmmBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto batch1_ix = gen.range(1);
  auto batch2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto batch2 = batch2_.unpack();
  auto batch1 = batch1_.unpack();
  if (should_compute_output({ batch1_ix })) {
    auto grad_result = grad.unsqueeze(0).expand({ batch1_argsize_0, batch1_argsize_1, batch2_argsize_2 }).bmm(batch2.transpose(1, 2)) * alpha;
    copy_range(grad_inputs, batch1_ix, grad_result);
  }
  if (should_compute_output({ batch2_ix })) {
    auto grad_result = batch1.transpose(1, 2).bmm(grad.unsqueeze(0).expand({ batch1_argsize_0, batch1_argsize_1, batch2_argsize_2 })) * alpha;
    copy_range(grad_inputs, batch2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = maybe_multiply(grad, beta);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddcdivBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto tensor1_ix = gen.range(1);
  auto tensor2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto tensor2 = tensor2_.unpack();
  auto tensor1 = tensor1_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ tensor1_ix })) {
    auto grad_result = grad * value / tensor2;
    copy_range(grad_inputs, tensor1_ix, grad_result);
  }
  if (should_compute_output({ tensor2_ix })) {
    auto grad_result = -grad * value * tensor1 / (tensor2 * tensor2);
    copy_range(grad_inputs, tensor2_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddcmulBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto tensor1_ix = gen.range(1);
  auto tensor2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto tensor2 = tensor2_.unpack();
  auto tensor1 = tensor1_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ tensor1_ix })) {
    auto grad_result = grad * tensor2 * value;
    copy_range(grad_inputs, tensor1_ix, grad_result);
  }
  if (should_compute_output({ tensor2_ix })) {
    auto grad_result = grad * tensor1 * value;
    copy_range(grad_inputs, tensor2_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddmmBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto mat1_ix = gen.range(1);
  auto mat2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto mat1 = mat1_.unpack();
  auto mat2 = mat2_.unpack();
  if (should_compute_output({ mat1_ix })) {
    auto grad_result = mm_mat1_backward(grad, mat2, mat1_sizes, mat1.strides(), alpha);
    copy_range(grad_inputs, mat1_ix, grad_result);
  }
  if (should_compute_output({ mat2_ix })) {
    auto grad_result = mm_mat2_backward(grad, mat1, mat2_sizes, mat2.strides(), alpha);
    copy_range(grad_inputs, mat2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = maybe_multiply(grad, beta);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddmvBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto mat_ix = gen.range(1);
  auto vec_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto vec = vec_.unpack();
  auto mat = mat_.unpack();
  if (should_compute_output({ mat_ix })) {
    auto grad_result = grad.ger(vec) * alpha;
    copy_range(grad_inputs, mat_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = maybe_multiply(grad, beta);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ vec_ix })) {
    auto grad_result = mat.t().mv(grad) * alpha;
    copy_range(grad_inputs, vec_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AddrBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto vec1_ix = gen.range(1);
  auto vec2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto vec2 = vec2_.unpack();
  auto vec1 = vec1_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = maybe_multiply(grad, beta);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ vec1_ix })) {
    auto grad_result = grad.mv(vec2) * alpha;
    copy_range(grad_inputs, vec1_ix, grad_result);
  }
  if (should_compute_output({ vec2_ix })) {
    auto grad_result = grad.t().mv(vec1) * alpha;
    copy_range(grad_inputs, vec2_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AliasBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AsStridedBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = as_strided_backward(grad, self_geometry, size, stride, storage_offset);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AsinBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (-self * self + 1).rsqrt();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AtanBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / (self * self + 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Atan2Backward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ self_ix, other_ix })) {
      auto grad_input_mask = std::array<bool, 2>{
      should_compute_output({ self_ix }),
      should_compute_output({ other_ix }),
    };
    auto grad_result = atan2_backward(grad, self, other, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, other_ix, std::get<1>(grad_result));
  }
  return grad_inputs;
}
variable_list BaddbmmBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto batch1_ix = gen.range(1);
  auto batch2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto batch2 = batch2_.unpack();
  auto batch1 = batch1_.unpack();
  if (should_compute_output({ batch1_ix })) {
    auto grad_result = grad.bmm(batch2.transpose(1, 2)) * alpha;
    copy_range(grad_inputs, batch1_ix, grad_result);
  }
  if (should_compute_output({ batch2_ix })) {
    auto grad_result = batch1.transpose(1, 2).bmm(grad) * alpha;
    copy_range(grad_inputs, batch2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = maybe_multiply(grad, beta);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list BernoulliBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list BmmBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto mat2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto mat2 = mat2_.unpack();
  if (should_compute_output({ mat2_ix })) {
    auto grad_result = self.transpose(1, 2).bmm(grad);
    copy_range(grad_inputs, mat2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.bmm(mat2.transpose(1, 2));
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list BtrifactBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("btrifact");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list BtrifactWithInfoBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("btrifact_with_info");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list BtrisolveBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("btrisolve");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CatBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto tensors_ix = gen.range(tensors_size_);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ tensors_ix })) {
    auto grad_result = cat_tensors_backward(grad, tensors_args_sizes, dim);
    copy_range(grad_inputs, tensors_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CauchyBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CeilBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ClampBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * ((self >= min) * (self <= max)).type_as(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ClampMinBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (self >= min).type_as(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ClampMaxBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (self <= max).type_as(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CloneBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CosBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * -self.sin();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CoshBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * self.sinh();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CrossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = grad.cross(self, dim);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = other.cross(grad, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CumprodBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = cumprod_backward(grad, self, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CumsumBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = cumsum_backward(grad, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ConvTbcBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto bias = bias_.unpack();
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
    
    auto grad_result = conv_tbc_backward(grad, self, weight, bias, pad);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list DetBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = det_backward(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DiagBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = diag_backward(grad, self_sizes, diagonal);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DiagonalBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = diagonal_backward(grad, self_sizes, offset, dim1, dim2);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DistBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ other_ix })) {
    auto grad_result = -norm_backward(grad, self - other, p, result);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = norm_backward(grad, self - other, p, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DivBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / other;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DivBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = -grad * self / (other * other);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / other;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DotBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto tensor_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto tensor = tensor_.unpack();
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * tensor;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ tensor_ix })) {
    auto grad_result = grad * self;
    copy_range(grad_inputs, tensor_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EigBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("eig");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EqBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EqBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ErfBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = 2.0 / sqrt(M_PI) * exp(-(self.pow(2))) * grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ErfinvBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = 0.5 * sqrt(M_PI) * exp(self.erfinv().pow(2)) * grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ExpBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * result;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Expm1Backward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (result + 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ExpandBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = reduce_to(grad, self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ExponentialBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FillBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FillBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto value_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ value_ix })) {
    auto grad_result = grad.sum();
    copy_range(grad_inputs, value_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FloorBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FmodBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FmodBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = not_implemented("fmod: other");
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FracBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GatherBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = at::zeros(grad.type(), self_sizes).scatter_add_(dim, index, grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GeBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GeBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GelsBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto A_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ A_ix })) {
    auto grad_result = not_implemented("gels");
    copy_range(grad_inputs, A_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("gels");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GeometricBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GeqrfBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("geqrf");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GerBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto vec2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto vec2 = vec2_.unpack();
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.mv(vec2);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ vec2_ix })) {
    auto grad_result = grad.t().mv(self);
    copy_range(grad_inputs, vec2_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GesvSingleBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto A_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto A = A_.unpack();
  auto solution = solution_.unpack(shared_from_this());
  if (should_compute_output({ A_ix })) {
    auto grad_result = gesv_backward_A(grad, self, A, solution);
    copy_range(grad_inputs, A_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = gesv_backward_self(grad, self, A);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GesvHelperBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto A_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto A = A_.unpack();
  auto result0 = result0_.unpack(shared_from_this());
  if (should_compute_output({ A_ix })) {
    auto grad_result = gesv_backward_A(grad, self, A, result0);
    copy_range(grad_inputs, A_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = gesv_backward_self(grad, self, A);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GtBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GtBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HistcBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("histc");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list IndexAddBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto source_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ source_ix })) {
    auto grad_result = grad.index_select(dim, index);
    copy_range(grad_inputs, source_ix, grad_result);
  }
  return grad_inputs;
}
variable_list IndexCopyBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto source_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().index_fill_(dim, index, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ source_ix })) {
    auto grad_result = grad.index_select(dim, index);
    copy_range(grad_inputs, source_ix, grad_result);
  }
  return grad_inputs;
}
variable_list IndexFillBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().index_fill_(dim, index, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list IndexFillBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto value_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().index_fill_(dim, index, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ value_ix })) {
    auto grad_result = grad.index_select(dim, index).sum();
    copy_range(grad_inputs, value_ix, grad_result);
  }
  return grad_inputs;
}
variable_list IndexSelectBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = at::zeros(grad.type(), self_sizes).index_add_(dim, index, grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list InverseBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = -at::mm(output.t(), at::mm(grad, output.t()));
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list KthvalueBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, indices, self_sizes, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LeBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LeBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LerpBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto end_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ end_ix })) {
    auto grad_result = grad * weight;
    copy_range(grad_inputs, end_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (1 - weight.toDouble());
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LgammaBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * digamma(self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list DigammaBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * polygamma(1, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PolygammaBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * polygamma(n + 1, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.div(self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Log10Backward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / (self * 2.3025850929940456);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Log1PBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / (self + 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Log2Backward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / (self * 0.6931471805599453);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogdetBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = logdet_backward(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogNormalBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogsumexpBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = logsumexp_backward(grad, self, result, dim, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LtBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LtBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaskedFillBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto mask = mask_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().masked_fill_(mask, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaskedFillBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto value_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto mask = mask_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().masked_fill_(mask, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ value_ix })) {
    auto grad_result = at::where(mask, grad, zeros_like(grad)).sum();
    copy_range(grad_inputs, value_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaskedScatterBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto source_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto mask = mask_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().masked_fill_(mask, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ source_ix })) {
    auto grad_result = masked_scatter_backward(grad, mask, source_sizes);
    copy_range(grad_inputs, source_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaskedSelectBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto mask = mask_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros().masked_scatter_(mask, grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto max_indices = max_indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, max_indices, self_sizes, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward_scalar(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxBackward2::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = grad.clone().masked_fill_(self > other, 0);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().masked_fill_(self <= other, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MeanBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = sum_backward(grad, self_sizes, dim, keepdim) / _safe_size(self_sizes, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MeanBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.expand(self_sizes) / self_numel;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MedianBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward_scalar(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MedianBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, indices, self_sizes, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MinBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto min_indices = min_indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, min_indices, self_sizes, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MinBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward_scalar(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MinBackward2::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = grad.clone().masked_fill_(self < other, 0);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().masked_fill_(self >= other, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MmBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto mat2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto mat2 = mat2_.unpack();
  if (should_compute_output({ mat2_ix })) {
    auto grad_result = mm_mat2_backward(grad, self, mat2_sizes, mat2.strides(), 1);
    copy_range(grad_inputs, mat2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = mm_mat1_backward(grad, mat2, self_sizes, self.strides(), 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ModeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, indices, self_sizes, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MulBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * other;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MulBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto other = other_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = grad * self;
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * other;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MvBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto vec_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto vec = vec_.unpack();
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.ger(vec);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ vec_ix })) {
    auto grad_result = self.t().mv(grad);
    copy_range(grad_inputs, vec_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NeBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NeBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ other_ix })) {
    auto grad_result = other_info.zeros();
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NegBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.neg();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = norm_backward(grad, self, p, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = norm_backward(grad, self, p, result, dim, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormalBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormalBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto mean_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ mean_ix })) {
    auto grad_result = at::zeros(grad.type(), mean_sizes);
    copy_range(grad_inputs, mean_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormalBackward2::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto std_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ std_ix })) {
    auto grad_result = at::zeros(grad.type(), std_sizes);
    copy_range(grad_inputs, std_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NormalBackward3::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto mean_ix = gen.range(1);
  auto std_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ mean_ix })) {
    auto grad_result = at::zeros(grad.type(), mean_sizes);
    copy_range(grad_inputs, mean_ix, grad_result);
  }
  if (should_compute_output({ std_ix })) {
    auto grad_result = at::zeros(grad.type(), std_sizes);
    copy_range(grad_inputs, std_ix, grad_result);
  }
  return grad_inputs;
}
variable_list OrgqrBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto input2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ input2_ix })) {
    auto grad_result = not_implemented("orgqr");
    copy_range(grad_inputs, input2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("orgqr");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list OrmqrBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto input2_ix = gen.range(1);
  auto input3_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ input2_ix })) {
    auto grad_result = not_implemented("ormqr");
    copy_range(grad_inputs, input2_ix, grad_result);
  }
  if (should_compute_output({ input3_ix })) {
    auto grad_result = not_implemented("ormqr");
    copy_range(grad_inputs, input3_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("ormqr");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PermuteBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = permute_backwards(grad, dims);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PoissonBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PotrfBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = potrf_backward(grad, upper, output);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PotriBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("potri");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PotrsBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto input2_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ input2_ix })) {
    auto grad_result = not_implemented("potri");
    copy_range(grad_inputs, input2_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("potri");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PowBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * exponent * self.pow(exponent.toDouble() - 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PowBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto exponent_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto exponent = exponent_.unpack();
  if (should_compute_output({ exponent_ix })) {
    auto grad_result = grad * self.pow(exponent) * self.log();
    copy_range(grad_inputs, exponent_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * exponent * self.pow(exponent - 1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ProdBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = prod_backward(grad, self, result, dim, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ProdBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = prod_backward(grad, self, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PstrfBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("pstrf");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PutBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto source_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().put_(index, source_info.zeros(), accumulate);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ source_ix })) {
    auto grad_result = grad.take(index);
    copy_range(grad_inputs, source_ix, grad_result);
  }
  return grad_inputs;
}
variable_list QrBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("qr");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RandomBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RandomBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RandomBackward2::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReciprocalBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = -grad * result * result;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RemainderBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RemainderBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RenormBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = renorm_backward(grad, self, p, dim, maxnorm);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RepeatBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = repeat_backward(grad, self.dim(), repeats);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list Roipooling2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto input_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto input = input_.unpack();
  auto rois = rois_.unpack();
  auto result1 = result1_.unpack(shared_from_this());
  if (should_compute_output({ input_ix })) {
    auto grad_result = RoiPooling2d_backward(input, rois, pooledHeight, pooledWidth, spatialScale, grad, result1);
    copy_range(grad_inputs, input_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RoundBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RsqrtBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = -0.5 * grad * result.pow(3);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ScatterBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto src_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().scatter_(dim, index, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ src_ix })) {
    auto grad_result = grad.gather(dim, index);
    copy_range(grad_inputs, src_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ScatterBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.clone().scatter_(dim, index, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ScatterAddBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto src_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  if (should_compute_output({ src_ix })) {
    auto grad_result = grad.gather(dim, index);
    copy_range(grad_inputs, src_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SigmoidBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = _sigmoid_backward(grad, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SignBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SinBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * self.cos();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SinhBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * self.cosh();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SlogdetBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  auto result0 = result0_.unpack(shared_from_this());
  auto result1 = result1_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = slogdet_backward(grads, self, result0, result1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SortBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, indices, self_sizes, true);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SplitBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = split_backward(grads, split_size, dim, self_sizes, self.type());
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SplitWithSizesBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = split_with_sizes_backward(grads, split_sizes, dim, self_sizes, self.type());
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SqrtBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad / (2 * result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SqueezeBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = unsqueeze_to(grad, self_sizes);;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SqueezeBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = unsqueeze_to(grad, dim, self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list StdBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = var_backward(grad / (result * 2), self, unbiased);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list StdBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = var_backward(grad / (result * 2), self, dim, unbiased, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SubBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SubBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ other_ix })) {
    auto grad_result = -grad * alpha;
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad;
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SumBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.expand(self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SumBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = sum_backward(grad, self_sizes, dim, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SvdBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  auto res1 = res1_.unpack(shared_from_this());
  auto res2 = res2_.unpack(shared_from_this());
  auto res3 = res3_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = svd_backward(grads, self, some, res1, res2, res3);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SymeigBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("symeig");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.t();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TakeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto index = index_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros().put_(index, grad, true);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TanBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * (1 + result.pow(2));
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TanhBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = _tanh_backward(grad, result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TopkBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = select_backward(grad, dim, indices, self_sizes, true);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TraceBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = trace_backward(grad, self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TransposeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.transpose(dim0, dim1);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TrilBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.tril(diagonal);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TriuBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.triu(diagonal);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TrtrsBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto A_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  auto A = A_.unpack();
  auto res1 = res1_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, A_ix })) {
      auto grad_input_mask = std::array<bool, 2>{
      should_compute_output({ self_ix }),
      should_compute_output({ A_ix }),
    };
    auto grad_result = trtrs_backward(grads[0], grads[1], self, A, res1, upper, transpose, unitriangular, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, A_ix, std::get<1>(grad_result));
  }
  return grad_inputs;
}
variable_list TruncBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UnfoldBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = unfold_backward(grad, self_sizes, dimension, size, step);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UniformBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UniqueBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("_unique");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UnsafeViewBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.contiguous().view(self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UnsqueezeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.squeeze(dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list VarBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = var_backward(grad, self, unbiased);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list VarBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = var_backward(grad, self, dim, unbiased, keepdim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ViewBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad.contiguous().view(self_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SWhereBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto other_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto condition = condition_.unpack();
  if (should_compute_output({ other_ix })) {
    auto grad_result = where(condition, zeros_like(grad), grad);
    copy_range(grad_inputs, other_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = where(condition, grad, zeros_like(grad));
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ZeroBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SparseMaskBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto mask_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ mask_ix })) {
    auto grad_result = not_implemented("_sparse_mask");
    copy_range(grad_inputs, mask_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("_sparse_mask");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list StandardGammaBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = grad * self._standard_gamma_grad(result);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list StandardGammaGradBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("_standard_gamma_grad");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TrilinearBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto i1_ix = gen.range(1);
  auto i2_ix = gen.range(1);
  auto i3_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto i1 = i1_.unpack();
  auto i2 = i2_.unpack();
  auto i3 = i3_.unpack();
  if (should_compute_output({ i1_ix, i2_ix, i3_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ i1_ix }),
      should_compute_output({ i2_ix }),
      should_compute_output({ i3_ix }),
    };
    auto grad_result = _trilinear_backward(grad, i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim, grad_input_mask);
    copy_range(grad_inputs, i1_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, i2_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, i3_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list BinaryCrossEntropyBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = binary_cross_entropy_backward(grad, self, target, weight, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EmbeddingBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ weight_ix })) {
    auto grad_result = embedding_backward(grad, indices, weight_argsize_0, padding_idx, scale_grad_by_freq, sparse);
    copy_range(grad_inputs, weight_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EmbeddingBagBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  auto offsets = offsets_.unpack();
  auto result1 = result1_.unpack(shared_from_this());
  auto result2 = result2_.unpack(shared_from_this());
  auto result3 = result3_.unpack(shared_from_this());
  if (should_compute_output({ weight_ix })) {
    auto grad_result = embedding_bag_backward(grad, indices, offsets, result1, result2, result3, weight_argsize_0, scale_grad_by_freq, mode, sparse);
    copy_range(grad_inputs, weight_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EmbeddingRenormBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  if (should_compute_output({ self_ix })) {
    auto grad_result = not_implemented("embedding_renorm");
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list KlDivBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = kl_div_backward(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list L1LossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = l1_loss_backward(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MseLossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = mse_loss_backward(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MultiMarginLossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = multi_margin_loss_backward(grad, self, target, p, margin, weight, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MultilabelMarginLossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  auto is_target = is_target_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = multilabel_margin_loss_backward(grad, self, target, size_average, reduce, is_target);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NllLossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  auto total_weight = total_weight_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = nll_loss_backward(grad, self, target, weight, size_average, ignore_index, reduce, total_weight);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NllLoss2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  auto total_weight = total_weight_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = nll_loss2d_backward(grad, self, target, weight, size_average, ignore_index, reduce, total_weight);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SmoothL1LossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = smooth_l1_loss_backward(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftMarginLossBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = soft_margin_loss_backward(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReluBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = threshold_backward(grad, self, 0, 0);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EluBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = elu_backward(grad, alpha, scale, output);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GluBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = glu_backward(grad, self, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HardshrinkBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = hardshrink_backward(grad, self, lambd);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HardtanhBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = hardtanh_backward(grad, self, min_val, max_val);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HardtanhBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = hardtanh_backward(grad, output, min_val, max_val);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LeakyReluBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = leaky_relu_backward(grad, self, negative_slope);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LeakyReluBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = leaky_relu_backward(grad, output, negative_slope);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogSigmoidBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto buffer = buffer_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = log_sigmoid_backward(grad, self, buffer);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogSoftmaxBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = log_softmax_backward_data(grad, result, dim, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PreluBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 2>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = prelu_backward(grad, self, weight, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
  }
  return grad_inputs;
}
variable_list RreluWithNoiseBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto noise = noise_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = rrelu_with_noise_backward(grad, self, noise, lower, upper, training);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list RreluWithNoiseBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto noise = noise_.unpack();
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = rrelu_with_noise_backward(grad, output, noise, lower, upper, training);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftmaxBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto result = result_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = softmax_backward_data(grad, result, dim, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftplusBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = softplus_backward(grad, self, beta, threshold, output);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftshrinkBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = softshrink_backward(grad, self, lambd);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ThresholdBackward0::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = threshold_backward(grad, self, threshold, value);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ThresholdBackward1::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = threshold_backward(grad, output, threshold, value);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReflectionPad1DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = reflection_pad1d_backward(grad, self, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReflectionPad2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = reflection_pad2d_backward(grad, self, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad1DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = replication_pad1d_backward(grad, self, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = replication_pad2d_backward(grad, self, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = replication_pad3d_backward(grad, self, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleLinear1DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_linear1d_backward(grad, output_size, self_sizes, align_corners);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleBilinear2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_bilinear2d_backward(grad, output_size, self_sizes, align_corners);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleTrilinear3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_trilinear3d_backward(grad, output_size, self_sizes, align_corners);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest1DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_nearest1d_backward(grad, self, scale_factor);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_nearest2d_backward(grad, self, scale_factor);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = upsample_nearest3d_backward(grad, self, scale_factor);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveAvgPool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = adaptive_avg_pool2d_backward(grad, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveAvgPool3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = adaptive_avg_pool3d_backward(grad, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveMaxPool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = adaptive_max_pool2d_backward(grad, self, indices);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveMaxPool3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = adaptive_max_pool3d_backward(grad, self, indices);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AvgPool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = avg_pool2d_backward(grad, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AvgPool3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = avg_pool3d_backward(grad, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FractionalMaxPool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = fractional_max_pool2d_backward(grad, self, kernel_size, output_size, indices);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxPool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = max_pool2d_backward(grad, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxPool3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack(shared_from_this());
  if (should_compute_output({ self_ix })) {
    auto grad_result = max_pool3d_backward(grad, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxUnpool2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = max_unpool2d_backward(grad, self, indices, output_size);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxUnpool3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto indices = indices_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = max_unpool3d_backward(grad, self, indices, output_size, stride, padding);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ThnnBatchNormBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto running_mean = running_mean_.unpack();
  auto running_var = running_var_.unpack();
  auto save_mean = save_mean_.unpack(shared_from_this());
  auto save_std = save_std_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_batch_norm_backward(grad.contiguous(), self, weight, running_mean, running_var, training, eps, save_mean, save_std, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnBatchNormBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto save_mean_ix = gen.range(1);
  auto save_std_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto save_mean = save_mean_.unpack();
  auto save_std = save_std_.unpack();
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto running_mean = running_mean_.unpack();
  auto running_var = running_var_.unpack();
  if (should_compute_output({ save_mean_ix })) {
    auto grad_result = not_implemented("thnn_batch_norm_backward save_mean");
    copy_range(grad_inputs, save_mean_ix, grad_result);
  }
  if (should_compute_output({ save_std_ix })) {
    auto grad_result = not_implemented("thnn_batch_norm_backward save_std");
    copy_range(grad_inputs, save_std_ix, grad_result);
  }
  if (should_compute_output({ self_ix, weight_ix, grad_output_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ grad_output_ix }),
    };
    auto grad_result = batchnorm_double_backward(self, weight, grads[0], grads[1], grads[2], grad_output, running_mean, running_var, training, eps, save_mean, save_std, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, grad_output_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvTranspose2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto columns = columns_.unpack(shared_from_this());
  auto ones = ones_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv_transpose2d_backward(grad, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvTranspose2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, true, output_padding, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvTranspose3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto finput = finput_.unpack(shared_from_this());
  auto fgrad_input = fgrad_input_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv_transpose3d_backward(grad, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvTranspose3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, true, output_padding, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConv2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto finput = finput_.unpack(shared_from_this());
  auto fgrad_input = fgrad_input_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv2d_backward(grad, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConv2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, {{1, 1}}, false, {{0, 0}}, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDepthwise2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ bias_ix })) {
    auto grad_result = grad.contiguous().view({grad.size(0), grad.size(1), -1}).sum(0).sum(1);
    copy_range(grad_inputs, bias_ix, grad_result);
  }
  if (should_compute_output({ self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 2>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = thnn_conv_depthwise2d_backward(grad.contiguous(), self, weight, kernel_size, stride, padding, dilation, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDepthwise2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], {}, grad_output, weight, self, stride, padding, dilation, false, {{0, 0}}, self_argsize_1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConv3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto finput = finput_.unpack(shared_from_this());
  auto fgrad_input = fgrad_input_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv3d_backward(grad, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConv3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, {{1, 1, 1}}, false, {{0, 0, 0}}, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDilated2DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto columns = columns_.unpack(shared_from_this());
  auto ones = ones_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv_dilated2d_backward(grad, self, weight, kernel_size, stride, padding, dilation, columns, ones, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDilated2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, false, {{0, 0}}, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDilated3DBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  auto columns = columns_.unpack(shared_from_this());
  auto ones = ones_.unpack(shared_from_this());
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = thnn_conv_dilated3d_backward(grad, self, weight, kernel_size, stride, padding, dilation, columns, ones, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list ThnnConvDilated3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, false, {{0, 0, 0}}, 1, false, false, false, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list AdaptiveAvgPool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = adaptive_avg_pool2d(grad, { grad_output.size(-2), grad_output.size(-1) });
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveAvgPool3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = adaptive_avg_pool3d(grad, { grad_output.size(-3), grad_output.size(-2), grad_output.size(-1) });
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveMaxPool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_pool_double_backward(grad, indices, 2);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AdaptiveMaxPool3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_pool_double_backward(grad, indices, 3);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AvgPool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = avg_pool2d(grad, kernel_size, stride, padding, ceil_mode, count_include_pad);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list AvgPool3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = avg_pool3d(grad, kernel_size, stride, padding, ceil_mode, count_include_pad);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list EluBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = elu_backward(grad, alpha, scale, output);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ output_ix })) {
    auto grad_result = grad * grad_output * (output < 0).toType(grad.type());
    copy_range(grad_inputs, output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list FractionalMaxPool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_pool_double_backward(grad, indices, 2);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list GluBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = glu_double_backward_grad_output(grad, self, dim);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = glu_double_backward(grad, grad_output, self, dim);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HardshrinkBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = hardshrink_backward(grad, self, lambd);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list HardtanhBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = hardtanh_backward(grad, self, min_val, max_val);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list KlDivBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = kl_div_double_backward_grad_output(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list L1LossBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = l1_loss_double_backward_grad_output(grad, self, target, size_average, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogSigmoidBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto buffer = buffer_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = log_sigmoid_backward(grad, self, buffer);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = log_sigmoid_double_backward(grad * grad_output, self);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LogSoftmaxBackwardDataBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = grad - (grad * output.exp()).sum(dim, true);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = log_softmax_double_backward(grad, grad_output, dim, output);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list LeakyReluBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = leaky_relu_backward(grad, self, negative_slope);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxPool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_pool_double_backward(grad, indices, 2);;
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxPool3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_pool_double_backward(grad, indices, 3);;
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MaxUnpool2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto indices = indices_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = max_unpool2d(grad, indices, output_size);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list MseLossBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = mse_loss_double_backward_grad_output(grad, grad_output, self, target, size_average, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = mse_loss_double_backward(grad * grad_output, self, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NllLossBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = nll_loss(grad, target, weight, size_average, ignore_index, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list NllLoss2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto target = target_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = nll_loss2d(grad, target, weight, size_average, ignore_index, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list PreluBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = prelu_double_backward(grads[0], grads[1], grad_output, self, weight, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list RreluWithNoiseBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto noise = noise_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = rrelu_with_noise_backward(grad, self, noise, lower, upper, training);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReflectionPad1DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = reflection_pad1d(grad, padding);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReflectionPad2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = reflection_pad2d(grad, padding);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad1DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = replication_pad1d(grad, padding);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = replication_pad2d(grad, padding);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ReplicationPad3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = replication_pad3d(grad, padding);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = self_info.zeros();
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SmoothL1LossBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = smooth_l1_loss_double_backward_grad_output(grad, grad_output, self, target, size_average, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = smooth_l1_loss_double_backward(grad * grad_output, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftplusBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto output = output_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = softplus_backward(grad, self, beta, threshold, output);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = softplus_double_backward(grad * grad_output, self, beta, threshold);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftmaxBackwardDataBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack();
  auto self = self_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = softmax_backward_data(grad, output, dim, self);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = softmax_double_backward(grad, grad_output, dim, output);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftMarginLossBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto grad_output = grad_output_.unpack();
  auto self = self_.unpack();
  auto target = target_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = soft_margin_loss_double_backward_grad_output(grad, grad_output, self, target, size_average, reduce);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = soft_margin_loss_double_backward(grad * grad_output, self, target, size_average, reduce);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SoftshrinkBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = softshrink_backward(grad, self, lambd);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list ThresholdBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = threshold_backward(grad, self, threshold, value);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleLinear1DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_linear1d(grad, output_size, align_corners);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleBilinear2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_bilinear2d(grad, output_size, align_corners);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleTrilinear3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_trilinear3d(grad, output_size, align_corners);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest1DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_nearest1d(grad, scale_factor);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest2DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_nearest2d(grad, scale_factor);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list UpsampleNearest3DBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = upsample_nearest3d(grad, scale_factor);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ self_ix })) {
    auto grad_result = zeros_like(grad);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}
variable_list SigmoidBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = _sigmoid_backward(grad, output);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ output_ix })) {
    auto grad_result = grad * grad_output * (-2 * output + 1);
    copy_range(grad_inputs, output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list TanhBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto grad_output_ix = gen.range(1);
  auto output_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto output = output_.unpack();
  auto grad_output = grad_output_.unpack();
  if (should_compute_output({ grad_output_ix })) {
    auto grad_result = _tanh_backward(grad, output);
    copy_range(grad_inputs, grad_output_ix, grad_result);
  }
  if (should_compute_output({ output_ix })) {
    auto grad_result = -2 * output * grad * grad_output;
    copy_range(grad_inputs, output_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CudnnConvolutionTransposeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = cudnn_convolution_transpose_backward(self, grad, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnConvolutionTransposeBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto grad_output_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  auto grad_output = grad_output_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, true, output_padding, groups, benchmark, deterministic, true, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnConvolutionBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = cudnn_convolution_backward(self, grad, weight, padding, stride, dilation, groups, benchmark, deterministic, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnConvolutionBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto grad_output_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto self = self_.unpack();
  auto grad_output = grad_output_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ grad_output_ix, self_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ grad_output_ix }),
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, false, std::vector<int64_t>(padding.size(), 0), groups, benchmark, deterministic, true, grad_input_mask);
    copy_range(grad_inputs, grad_output_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, self_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnGridSamplerBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto grid_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto grid = grid_.unpack();
  if (should_compute_output({ self_ix, grid_ix })) {
    
    auto grad_result = cudnn_grid_sampler_backward(self, grid, grad);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, grid_ix, std::get<1>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnAffineGridGeneratorBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto theta_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  if (should_compute_output({ theta_ix })) {
    auto grad_result = cudnn_affine_grid_generator_backward(grad, N, C, H, W);
    copy_range(grad_inputs, theta_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CudnnBatchNormBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto input_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto input = input_.unpack();
  auto weight = weight_.unpack();
  auto running_mean = running_mean_.unpack();
  auto running_var = running_var_.unpack();
  auto result1 = result1_.unpack(shared_from_this());
  auto result2 = result2_.unpack(shared_from_this());
  if (should_compute_output({ input_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ input_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = training ? cudnn_batch_norm_backward(input, grad.contiguous(), weight, running_mean, running_var, result1, result2, epsilon) : thnn_batch_norm_backward(grad.contiguous(), input, weight, running_mean, running_var, training, epsilon, result1, result2, grad_input_mask);
    copy_range(grad_inputs, input_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list CudnnBatchNormBackwardBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto input_ix = gen.range(1);
  auto grad_output_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto save_mean_ix = gen.range(1);
  auto save_var_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto input = input_.unpack();
  auto grad_output = grad_output_.unpack();
  auto weight = weight_.unpack();
  auto running_mean = running_mean_.unpack();
  auto running_var = running_var_.unpack();
  auto save_mean = save_mean_.unpack();
  auto save_var = save_var_.unpack();
  if (should_compute_output({ input_ix, weight_ix, grad_output_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ input_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ grad_output_ix }),
    };
    auto grad_result = batchnorm_double_backward(input, weight, grads[0], grads[1], grads[2], grad_output, running_mean, running_var, true, epsilon, save_mean, save_var, grad_input_mask);
    copy_range(grad_inputs, input_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, grad_output_ix, std::get<2>(grad_result));
  }
  if (should_compute_output({ save_mean_ix })) {
    auto grad_result = not_implemented("cudnn_batch_norm_backward save_mean");
    copy_range(grad_inputs, save_mean_ix, grad_result);
  }
  if (should_compute_output({ save_var_ix })) {
    auto grad_result = not_implemented("cudnn_batch_norm_backward save_var");
    copy_range(grad_inputs, save_var_ix, grad_result);
  }
  return grad_inputs;
}
variable_list CudnnRnnBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto input_ix = gen.range(1);
  auto weight_ix = gen.range(weight_size_);
  auto hx_ix = gen.range(1);
  auto cx_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto input = input_.unpack();
  auto weight = unpack_list(weight_);
  auto hx = hx_.unpack();
  auto cx = cx_.unpack();
  auto dropout_state = dropout_state_.unpack();
  auto result0 = result0_.unpack(shared_from_this());
  auto result3 = result3_.unpack(shared_from_this());
  auto result4 = result4_.unpack(shared_from_this());
  if (should_compute_output({ input_ix, hx_ix, cx_ix, weight_ix })) {
      auto grad_input_mask = std::array<bool, 4>{
      should_compute_output({ input_ix }),
      should_compute_output({ hx_ix }),
      should_compute_output({ cx_ix }),
      should_compute_output({ weight_ix }),
    };
    auto grad_result = _cudnn_rnn_backward(input, weight, weight_stride0, result4, hx, cx, result0, grads[0], grads[1], grads[2], mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, retain_variables ? result3.clone() : result3, grad_input_mask);
    copy_range(grad_inputs, input_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, hx_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, cx_ix, std::get<2>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<3>(grad_result));
  }
  return grad_inputs;
}
variable_list MkldnnConvolutionBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  auto weight_ix = gen.range(1);
  auto bias_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  auto weight = weight_.unpack();
  if (should_compute_output({ self_ix, weight_ix, bias_ix })) {
      auto grad_input_mask = std::array<bool, 3>{
      should_compute_output({ self_ix }),
      should_compute_output({ weight_ix }),
      should_compute_output({ bias_ix }),
    };
    auto grad_result = mkldnn_convolution_backward(self, grad, weight, padding, stride, dilation, grad_input_mask);
    copy_range(grad_inputs, self_ix, std::get<0>(grad_result));
    copy_range(grad_inputs, weight_ix, std::get<1>(grad_result));
    copy_range(grad_inputs, bias_ix, std::get<2>(grad_result));
  }
  return grad_inputs;
}
variable_list FftWithSizeBackward::apply(const variable_list& grads) {
  IndexRangeGenerator gen;
  auto self_ix = gen.range(1);
  variable_list grad_inputs(gen.size());
  auto& grad = grads[0];
  auto self = self_.unpack();
  if (should_compute_output({ self_ix })) {
    auto grad_result = fft_backward(self, grad, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalized, onesided, output_sizes);
    copy_range(grad_inputs, self_ix, grad_result);
  }
  return grad_inputs;
}

}}} // namespace torch::autograd::generated
